{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec253d4",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80125a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import gzip, pickle\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc12187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "ITEM_ATTR_CSV = os.path.join(\"data/books/books_llmrec_format\", \"item_attribute.csv\")\n",
    "DATA_BASE_DIR = \"A-LLMRec/data/books\"\n",
    "U_ITEM_PATH   = f\"data/books/5core/item_meta_2017_kcore5_user_item.json\"\n",
    "META_GZ_PATH  = f\"{DATA_BASE_DIR}/books_text_name_dict.json.gz\"\n",
    "PARTS = 5\n",
    "\n",
    "test_path = os.path.join(DATA_BASE_DIR, \"books_label.txt\")\n",
    "train_path = os.path.join(DATA_BASE_DIR, \"books_train_raw.txt\")\n",
    "\n",
    "\n",
    "# ######################### A-LLMRec #########################\n",
    "BASE_DIR = f\"A-LLMRec/books_results\"\n",
    "PRED_P1 = f\"{BASE_DIR}/predict_label_part1.json\"\n",
    "PRED_P5 = f\"{BASE_DIR}/predict_label_part5.json\"\n",
    "\n",
    "\n",
    "######################## LLMRec #########################\n",
    "BASE_DIR = \"data/books/books_llmrec_format\"\n",
    "PRED_P1    = f\"{BASE_DIR}/predict_label_part1.json\"\n",
    "PRED_P5    = f\"{BASE_DIR}/predict_label_part5.json\"\n",
    "\n",
    "\n",
    "# ###################### Augmentation #######################\n",
    "# BASE_DIR = f\"Augmentation/data/books\"\n",
    "# PRED_P1    = f\"{BASE_DIR}/predict_label_part1.json\"\n",
    "# PRED_P5    = f\"{BASE_DIR}/predict_label_part5.json\"\n",
    "\n",
    "\n",
    "# ###################### Traditional CF ######################\n",
    "# model = \"LightGCN\"  # \"MF-BPR\" or \"LightGCN\"\n",
    "# BASE_DIR = f\"data/books/traditionalCF\"\n",
    "# PRED_P1    = f\"{BASE_DIR}/predict_label_part1.json\"\n",
    "# PRED_P5    = f\"{BASE_DIR}/predict_label_part5.json\"\n",
    "\n",
    "\n",
    "with open(PRED_P5) as f:\n",
    "    pred_dict = json.load(f)\n",
    "print(pred_dict[\"0\"][:30])  # user 0ì˜ ìƒìœ„ 10ê°œ ì˜ˆì¸¡ ì•„ì´í…œ ID\n",
    "\n",
    "\n",
    "actual = sum(len(v) for v in pred_dict.values())\n",
    "print(\"actual predict_label interactions:\", actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971101cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data = {}\n",
    "with open(train_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        u_str, i_str = line.split()\n",
    "        u = str(u_str)         # í‚¤ëŠ” ë¬¸ìì—´ë¡œ\n",
    "        i = int(i_str)         # ì•„ì´í…œì€ ì •ìˆ˜ë¡œ (í˜¹ì€ ë¬¸ìì—´ë¡œ ë‘˜ ìˆ˜ë„ ìˆìŒ)\n",
    "\n",
    "        if u not in train_data:\n",
    "            train_data[u] = []\n",
    "        train_data[u].append(i)\n",
    "\n",
    "ground_truth = {}\n",
    "with open(test_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        u_str, i_str, _ = line.split()\n",
    "        u = str(u_str)\n",
    "        i = int(i_str)\n",
    "        # ground truthê°€ ì—¬ëŸ¬ ê°œë©´ ë¦¬ìŠ¤íŠ¸ì— append, í•œ ê°œë§Œ ìˆìœ¼ë©´ ë®ì–´ì“°ê±°ë‚˜ ì²« ê°œë§Œ ì“°ë©´ ë¨\n",
    "        if u not in ground_truth:\n",
    "            ground_truth[u] = []\n",
    "        ground_truth[u].append(i)\n",
    "\n",
    "expected = sum(len(v) for v in ground_truth.values())\n",
    "print(\"expected ground_truth interactions:\", expected)\n",
    "\n",
    "# ground truthì—ì„œ ê°€ì¥ ê¸´ ê¸°ë¡ì„ ê°€ì§„ ì‚¬ìš©ì ê²€ìƒ‰, í•´ë‹¹ ì‚¬ìš©ìì˜ ê¸°ë¡ ê¸¸ì´ í™•ì¸\n",
    "max_len_user = max(ground_truth.items(), key=lambda x: len(x[1]))\n",
    "print(f\"User with the longest ground truth interaction: {max_len_user[0]}, Length: {len(max_len_user[1])}\")\n",
    "\n",
    "# ê³µí†µ ì‚¬ìš©ì\n",
    "common_users = set(train_data.keys()) & set(ground_truth.keys())\n",
    "print(len(train_data.keys()))\n",
    "print(len(ground_truth.keys()))\n",
    "print(len(common_users))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5903188a",
   "metadata": {},
   "source": [
    "### Multi-hot-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# 1) train_data -> multi-hot (sparse CSR)\n",
    "# -----------------------------\n",
    "# train_data: { \"user_id(str)\": [item_id(int), ...], ... }\n",
    "\n",
    "users = list(train_data.keys())\n",
    "\n",
    "# item universe(ì „ì²´ ì•„ì´í…œ id) ìˆ˜ì§‘\n",
    "all_items = set()\n",
    "for u in users:\n",
    "    all_items.update(train_data[u])#[-10:])\n",
    "\n",
    "# item_idê°€ 0..N-1ë¡œ ì—°ì†ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë‹ˆ re-index\n",
    "item_list = sorted(all_items)\n",
    "item2col = {item: idx for idx, item in enumerate(item_list)}\n",
    "\n",
    "n_users = len(users)\n",
    "n_items = len(item_list)\n",
    "\n",
    "# CSR ë§Œë“¤ê¸°: (row=user, col=item) ìœ„ì¹˜ì— 1\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "for r, u in enumerate(users):\n",
    "    # ì¤‘ë³µ interactionì€ 1ë¡œ ì²˜ë¦¬(ë©€í‹°í•«)\n",
    "    seen_list = train_data[u]#[-10:]\n",
    "    seen = set(seen_list)\n",
    "    for item in seen:\n",
    "        rows.append(r)\n",
    "        cols.append(item2col[item])\n",
    "        data.append(1)\n",
    "\n",
    "X = csr_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32)\n",
    "print(\"X shape:\", X.shape, \"nnz:\", X.nnz)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) ê³ ì°¨ì› í¬ì†Œë²¡í„° -> ì €ì°¨ì› ì„ë² ë”© (SVD)\n",
    "# -----------------------------\n",
    "# KMeansëŠ” ê³ ì°¨ì› í¬ì†Œì—ì„œ ë°”ë¡œ ëŒë ¤ë„ ë˜ì§€ë§Œ,\n",
    "# ë³´í†µ SVDë¡œ 50~200 ì°¨ì› ì •ë„ë¡œ ì¤„ì´ë©´ í›¨ì”¬ ì•ˆì •ì ì„.\n",
    "svd_dim = min(100, n_items - 1) if n_items > 1 else 1\n",
    "svd = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
    "X_svd = svd.fit_transform(X)\n",
    "print(\"X_svd shape:\", X_svd.shape, \"explained_var_ratio_sum:\", svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# -----------------------------\n",
    "# 3) K-means\n",
    "# -----------------------------\n",
    "k = 2  # ì›í•˜ëŠ” í´ëŸ¬ìŠ¤í„° ìˆ˜ë¡œ ë°”ê¿”\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
    "labels = kmeans.fit_predict(X_svd)\n",
    "\n",
    "# -----------------------------\n",
    "# clusterë³„ ì‚¬ìš©ì ìˆ˜ ì¶œë ¥\n",
    "# -----------------------------\n",
    "cluster_cnt = Counter(labels)\n",
    "\n",
    "print(\"\\nUsers per cluster (%)\")\n",
    "for c in sorted(cluster_cnt.keys()):\n",
    "    ratio = cluster_cnt[c] / len(labels) * 100\n",
    "    print(f\"Cluster {c}: {cluster_cnt[c]} users ({ratio:.2f}%)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) 2D ì‹œê°í™” ì¢Œí‘œ ë§Œë“¤ê¸° (t-SNE)\n",
    "# -----------------------------\n",
    "# ì‚¬ìš©ì ìˆ˜ê°€ ì•„ì£¼ ë§ìœ¼ë©´(ìˆ˜ë§Œ ì´ìƒ) t-SNEëŠ” ëŠë¦´ ìˆ˜ ìˆìŒ -> ì•„ë˜ subsample ì˜µì…˜ ì‚¬ìš© ê¶Œì¥\n",
    "use_subsample = False\n",
    "max_points = 5000\n",
    "\n",
    "if use_subsample and n_users > max_points:\n",
    "    rng = np.random.RandomState(42)\n",
    "    idx = rng.choice(n_users, size=max_points, replace=False)\n",
    "    X_vis_in = X_svd[idx]\n",
    "    labels_vis = labels[idx]\n",
    "else:\n",
    "    X_vis_in = X_svd\n",
    "    labels_vis = labels\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=\"auto\",\n",
    "    init=\"pca\",\n",
    "    random_state=42,\n",
    ")\n",
    "Z = tsne.fit_transform(X_vis_in)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=labels_vis, s=10)\n",
    "plt.title(f\"User Multi-hot -> SVD({svd_dim}) -> KMeans(k={k}) -> t-SNE(2D)\")\n",
    "plt.xlabel(\"t-SNE dim1\")\n",
    "plt.ylabel(\"t-SNE dim2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb29562",
   "metadata": {},
   "source": [
    "# RQ1\n",
    "ì¶”ì²œ íŒŒì´í”„ë¼ì¸ ì†ì—ì„œ LLMì´ ìƒì„±í•œ ë°ì´í„°ì˜ í¸í–¥/í™˜ê° í˜„ìƒ ë¶„ì„\n",
    "\n",
    "í¬í•¨: LLMRec, Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f525e",
   "metadata": {},
   "source": [
    "## LLMRec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3f686c",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# âœ… ê²½ë¡œ ì„¤ì •\n",
    "base_dir = \"data/books/books_llmrec_format/\"\n",
    "save_dir = \"data/books/books_llmrec_format/poster/\"\n",
    "\n",
    "# âœ… 1. ë°ì´í„° ë¡œë“œ (ìš”ì²­í•˜ì‹  ì»¬ëŸ¼ëª… ì ìš©)\n",
    "# header=0ì„ ì“¸ì§€ Noneì„ ì“¸ì§€ëŠ” íŒŒì¼ì— í—¤ë”ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤.\n",
    "# ë³´í†µ namesë¥¼ ì§€ì •í•˜ë©´ header=0 (ì²«ì¤„ ë¬´ì‹œ) í˜¹ì€ header=None (ì²«ì¤„ë¶€í„° ë°ì´í„°)ë¥¼ ìƒí™©ì— ë§ì¶° ì¨ì•¼ í•©ë‹ˆë‹¤.\n",
    "# ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ íŒŒì¼ì— í—¤ë”ê°€ ìˆë‹¤ë©´ header=0ì„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "if os.path.exists(ITEM_ATTR_CSV):\n",
    "    item_attr_df = pd.read_csv(ITEM_ATTR_CSV, names=[\"id\", \"brand\", \"title\", \"category\"], header=0) \n",
    "    print(f\"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ. ì´ {len(item_attr_df)}ê°œ ì•„ì´í…œ\")\n",
    "else:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ITEM_ATTR_CSV}\")\n",
    "    exit()\n",
    "\n",
    "# âœ… 2. ì¹´í…Œê³ ë¦¬ íŒŒì‹± í•¨ìˆ˜ ì •ì˜\n",
    "def parse_categories(cat_raw):\n",
    "    \"\"\"\n",
    "    ë¬¸ìì—´ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸(\"['A', 'B']\")ë‚˜ ë‹¨ìˆœ ë¬¸ìì—´(\"A\")ì„ ì‹¤ì œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    if pd.isna(cat_raw):\n",
    "        return []\n",
    "    \n",
    "    cat_str = str(cat_raw).strip()\n",
    "    \n",
    "    try:\n",
    "        # 1) \"['Fiction', 'History']\" í˜•íƒœ íŒŒì‹±\n",
    "        if cat_str.startswith(\"[\") and cat_str.endswith(\"]\"):\n",
    "            parsed = ast.literal_eval(cat_str)\n",
    "            return [str(c).strip() for c in parsed if c]\n",
    "        \n",
    "        # 2) \"Fiction, History\" ì‰¼í‘œ êµ¬ë¶„ í˜•íƒœ\n",
    "        elif \",\" in cat_str:\n",
    "            return [c.strip() for c in cat_str.split(\",\") if c.strip()]\n",
    "            \n",
    "        # 3) ë‹¨ì¼ ë¬¸ìì—´\n",
    "        else:\n",
    "            return [cat_str]\n",
    "    except:\n",
    "        return [cat_str]\n",
    "\n",
    "# âœ… 3. ì¹´í…Œê³ ë¦¬ ì§‘í•©(Set) ë° ë¦¬ìŠ¤íŠ¸(List) ìƒì„±\n",
    "all_categories = set()      # ì¤‘ë³µ ì—†ëŠ” ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ìš”ì²­í•˜ì‹  ë¶€ë¶„)\n",
    "all_categories_list = []    # ë¹ˆë„ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ì „ì²´ ë¦¬ìŠ¤íŠ¸ (ì‹œê°í™”ìš©)\n",
    "\n",
    "print(\"ğŸ”„ ì¹´í…Œê³ ë¦¬ íŒŒì‹± ì§„í–‰ ì¤‘...\")\n",
    "for item_id in item_attr_df.index:\n",
    "    cat_raw = item_attr_df.loc[item_id, \"category\"]\n",
    "    cats = parse_categories(cat_raw)\n",
    "    \n",
    "    all_categories.update(cats)       # Set ì—…ë°ì´íŠ¸ (Unique)\n",
    "    all_categories_list.extend(cats)  # List í™•ì¥ (Countìš©)\n",
    "\n",
    "print(f\"ğŸ” ê³ ìœ  ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ (Set): {len(all_categories)}\")\n",
    "print(f\"ğŸ“Š ëˆ„ì  ì¹´í…Œê³ ë¦¬ íƒœê·¸ ìˆ˜ (List): {len(all_categories_list)}\")\n",
    "\n",
    "# âœ… 4. ì‹¤ì œ ë¶„í¬ ì‹œê°í™” (Top K)\n",
    "def plot_real_distribution(data_list, title, top_k=20):\n",
    "    if not data_list:\n",
    "        print(\"ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # ë¹ˆë„ ê³„ì‚°\n",
    "    counter = Counter(data_list)\n",
    "    \n",
    "    # ë°ì´í„°í”„ë ˆì„ ë³€í™˜\n",
    "    stat_df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count']).reset_index()\n",
    "    stat_df.rename(columns={'index': 'Category'}, inplace=True)\n",
    "    \n",
    "    # ìƒìœ„ Top K ì •ë ¬\n",
    "    df_topk = stat_df.sort_values(by='Count', ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (ê°€ë¡œ ë§‰ëŒ€)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(df_topk['Category'][::-1], df_topk['Count'][::-1], color='salmon', edgecolor='black', alpha=0.8)\n",
    "\n",
    "    # ìˆ˜ì¹˜ í‘œì‹œ\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width + (width * 0.01), bar.get_y() + bar.get_height()/2, \n",
    "                 f'{int(width)}', ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.title(f\"Real Distribution of {title} (Top {top_k})\", fontsize=15, fontweight='bold')\n",
    "    plt.xlabel(\"Count\", fontsize=12)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # ì €ì¥\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = \"Real_Category_Distribution_Parsed.png\"\n",
    "    plt.savefig(os.path.join(save_dir, filename))\n",
    "    print(f\"ğŸ“Š ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {os.path.join(save_dir, filename)}\")\n",
    "    plt.show()\n",
    "\n",
    "# ì‹¤í–‰\n",
    "plot_real_distribution(all_categories_list, \"Book Categories\", top_k=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "# âœ… íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "file_path = \"data/books/books_llmrec_format/augmented_user_profiling_dict_part5_step0_try0\"\n",
    "save_dir = \"data/books/books_llmrec_format/poster/\"\n",
    "\n",
    "# âœ… ì‹œê°í™”í•  ëŒ€ìƒ í‚¤(Key) ì •ì˜\n",
    "target_keys = [\n",
    "    'age', 'gender', \n",
    "    'liked category', 'disliked category', \n",
    "    'liked author', 'country', 'language'\n",
    "]\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ì´ˆê¸°í™”\n",
    "data_storage = {key: [] for key in target_keys}\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë“œ\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        augmented_dict = pickle.load(f)\n",
    "    print(f\"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ: {file_path} (ì´ {len(augmented_dict)}ëª…)\")\n",
    "else:\n",
    "    print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "    augmented_dict = {}\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "def normalize_gender(val: str):\n",
    "    \"\"\"\n",
    "    gender ê°’ì„ ìµœëŒ€í•œ ìœ í•˜ê²Œ ì •ê·œí™”í•´ì„œ\n",
    "    'm', 'f', 'nb', 'other' ì¤‘ í•˜ë‚˜ë¡œ ë§¤í•‘ (ë˜ëŠ” None)\n",
    "    \"\"\"\n",
    "    if val is None:\n",
    "        return None\n",
    "\n",
    "    s = str(val).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    # ë¬´ì˜ë¯¸ ê°’ ì œê±°\n",
    "    if s in {\"\", \"unknown\", \"none\", \"n/a\", \"na\", \"null\", \"nil\", \"unspecified\", \"?\"}:\n",
    "        return None\n",
    "\n",
    "    # í”í•œ íŒ¨í„´ë“¤(ìš°ì„ ìˆœìœ„ ìˆê²Œ)\n",
    "    # male\n",
    "    if re.fullmatch(r\"(m|male|man|boy|masc|masculine)\", s):\n",
    "        return \"m\"\n",
    "    # female\n",
    "    if re.fullmatch(r\"(f|female|woman|girl|fem|feminine)\", s):\n",
    "        return \"f\"\n",
    "\n",
    "    # ë¬¸ì¥í˜•/ìš°íšŒ í‘œí˜„ë“¤\n",
    "    if any(x in s for x in [\"prefer not\", \"rather not\", \"no answer\", \"not say\", \"private\"]):\n",
    "        return None\n",
    "\n",
    "    # non-binary / genderqueer ë“±\n",
    "    if any(x in s for x in [\"nonbinary\", \"non-binary\", \"nb\", \"genderqueer\", \"gender fluid\", \"genderfluid\", \"agender\"]):\n",
    "        return \"nb\"\n",
    "\n",
    "    # í˜¼í•© í‘œê¸°: \"m/f\", \"male/female\" ë“±\n",
    "    if re.search(r\"\\b(m|male)\\b\", s) and re.search(r\"\\b(f|female)\\b\", s):\n",
    "        return \"other\"\n",
    "\n",
    "    # í•œê¸€ë„ ê°™ì´ ì²˜ë¦¬(í˜¹ì‹œ ë“¤ì–´ì˜¤ë©´)\n",
    "    if any(x in s for x in [\"ë‚¨\", \"ë‚¨ì\", \"ë‚¨ì„±\"]):\n",
    "        return \"m\"\n",
    "    if any(x in s for x in [\"ì—¬\", \"ì—¬ì\", \"ì—¬ì„±\"]):\n",
    "        return \"f\"\n",
    "\n",
    "    # \"m.\" \"f.\" ê°™ì´ ëì— ì ì´ ë¶™ì€ ê²½ìš°\n",
    "    if re.fullmatch(r\"m\\.\", s):\n",
    "        return \"m\"\n",
    "    if re.fullmatch(r\"f\\.\", s):\n",
    "        return \"f\"\n",
    "\n",
    "    # ì—¬ê¸°ê¹Œì§€ ëª» ì¡ìœ¼ë©´ ê¸°íƒ€ë¡œ ë‘ê±°ë‚˜ ë²„ë¦¼(ì›í•˜ë©´ other ëŒ€ì‹  Noneìœ¼ë¡œ)\n",
    "    return \"other\"\n",
    "\n",
    "def clean_and_parse(profile_text):\n",
    "    if profile_text is None:\n",
    "        return None\n",
    "\n",
    "    s = str(profile_text).strip()\n",
    "\n",
    "    # 1) ì½”ë“œíœìŠ¤ì—ì„œ {...}ë§Œ ë½‘ê¸°\n",
    "    m = re.search(r\"```(?:json)?\\s*(\\{.*\\})\\s*```\", s, flags=re.DOTALL | re.IGNORECASE)\n",
    "    if m:\n",
    "        s = m.group(1)\n",
    "    else:\n",
    "        s = s.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    s = s.replace(\"\\r\", \"\").strip()\n",
    "\n",
    "    # 2) Python dict literalì´ë©´ ast ë¨¼ì € (ì‘ì€ë”°ì˜´í‘œ í‚¤/ê°’ì´ë©´ ê±°ì˜ 100% ì´ìª½)\n",
    "    #    ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±: \"{'age':\" íŒ¨í„´\n",
    "    if re.search(r\"\\{\\s*'\", s):\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # 3) JSONìš© ë¶ˆë²• escapeë§Œ ìµœì†Œ ìˆ˜ì •\n",
    "    s_json = s.replace(\"\\\\'\", \"'\")  # JSONì—ì„œ ë¶ˆë²•ì¸ \\' ì œê±°\n",
    "\n",
    "    # 4) JSON ì‹œë„\n",
    "    try:\n",
    "        return json.loads(s_json)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 5) ë§ˆì§€ë§‰ ë³´í—˜: ast ì¬ì‹œë„ (í˜¹ì‹œ íœ´ë¦¬ìŠ¤í‹±ì´ ë¹—ë‚˜ê°„ ê²½ìš°)\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def split_authors(val):\n",
    "    results = []\n",
    "\n",
    "    items = val if isinstance(val, list) else [val]\n",
    "\n",
    "    for item in items:\n",
    "        if not item:\n",
    "            continue\n",
    "        s = str(item).strip().lower()\n",
    "\n",
    "        # êµ¬ë¶„ì í†µì¼ (ì‰¼í‘œ ì œì™¸)\n",
    "        s = re.sub(r\"\\s*(?:&|;|\\||/)\\s*\", \" , \", s)\n",
    "        s = re.sub(r\"\\s+(?:and|with)\\s+\", \" , \", s)\n",
    "\n",
    "        # \"last, first\" í˜•íƒœëŠ” ì‰¼í‘œë¥¼ ì´ë¦„ ì¼ë¶€ë¡œ ë³¼ ê°€ëŠ¥ì„±ì´ ì»¤ì„œ ë³´í˜¸\n",
    "        # ì˜ˆ: \"rowling, j. k.\" ê°™ì€ ê²½ìš°\n",
    "        looks_like_last_first = bool(re.search(r\"^[a-z\\.\\-]+,\\s*[a-z]\", s))\n",
    "\n",
    "        if looks_like_last_first:\n",
    "            parts = [s.strip()]  # í†µì§¸ë¡œ í•˜ë‚˜ë¡œ ì·¨ê¸‰\n",
    "        else:\n",
    "            # ì‰¼í‘œ ê¸°ì¤€ ë¶„ë¦¬\n",
    "            parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "\n",
    "        results.extend(parts)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# âœ… ë©”ì¸ ë£¨í”„: ë°ì´í„° ì¶”ì¶œ\n",
    "for user_id, profile_text in augmented_dict.items():\n",
    "    profile = clean_and_parse(profile_text)\n",
    "    \n",
    "    if not profile:\n",
    "        print(f\"âš ï¸ íŒŒì‹± ì‹¤íŒ¨: User {user_id}\")\n",
    "        continue\n",
    "\n",
    "    for key in target_keys:\n",
    "        val = profile.get(key, None)\n",
    "        if not val:\n",
    "            continue\n",
    "\n",
    "        if key == \"liked author\":\n",
    "            authors = split_authors(val)\n",
    "            data_storage[key].extend(authors)\n",
    "            continue\n",
    "\n",
    "        # ---- ê¸°ì¡´ ë¡œì§ ----\n",
    "        if isinstance(val, list):\n",
    "            cleaned_items = [str(item).strip().lower() for item in val if item]\n",
    "            data_storage[key].extend(cleaned_items)\n",
    "\n",
    "        elif isinstance(val, str):\n",
    "            v = val.strip()\n",
    "            if v.lower() in ['unknown', 'none', 'n/a', '']:\n",
    "                continue\n",
    "\n",
    "            if key == 'gender':\n",
    "                g = normalize_gender(v)\n",
    "                if g is not None:\n",
    "                    data_storage[key].append(g)\n",
    "                continue\n",
    "\n",
    "            if ',' in v:\n",
    "                data_storage[key].extend([x.strip().lower() for x in v.split(',')])\n",
    "            else:\n",
    "                data_storage[key].append(v.lower())\n",
    "\n",
    "\n",
    "\n",
    "# âœ… ì‹œê°í™” í•¨ìˆ˜\n",
    "def plot_distribution(category_name, data_list, top_k=10):\n",
    "    if not data_list:\n",
    "        print(f\"âš ï¸ {category_name} - ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # ë¹ˆë„ ê³„ì‚°\n",
    "    counter = Counter(data_list)\n",
    "    \n",
    "    # ë°ì´í„°í”„ë ˆì„ ë³€í™˜\n",
    "    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count']).reset_index()\n",
    "    df.rename(columns={'index': 'Category'}, inplace=True)\n",
    "    \n",
    "    # ìƒìœ„ kê°œ ì¶”ì¶œ\n",
    "    df_topk = df.sort_values(by='Count', ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(df_topk['Category'], df_topk['Count'], color='skyblue', edgecolor='black', alpha=0.8)\n",
    "\n",
    "    # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + (yval * 0.01), int(yval), \n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.title(f\"Distribution of Predicted {category_name.title()} (Top {top_k})\", fontsize=15, fontweight='bold')\n",
    "    plt.xlabel(category_name.title(), fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ì´ë¯¸ì§€ ì €ì¥\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = f\"Predicted_{category_name.replace(' ', '_')}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"ğŸ“Š ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "    #plt.show()\n",
    "\n",
    "# âœ… ì‹¤í–‰: ê° í‚¤ë³„ë¡œ ì‹œê°í™” ìˆ˜í–‰\n",
    "print(\"\\n========== ì‹œê°í™” ì‹œì‘ ==========\")\n",
    "\n",
    "# í•­ëª© íŠ¹ì„±ì— ë”°ë¼ Top-K ì¡°ì ˆ\n",
    "plot_distribution('age', data_storage['age'], top_k=15)\n",
    "plot_distribution('gender', data_storage['gender'], top_k=5)\n",
    "plot_distribution('country', data_storage['country'], top_k=15)\n",
    "plot_distribution('language', data_storage['language'], top_k=10)\n",
    "\n",
    "# ì¹´í…Œê³ ë¦¬ì™€ ì €ìëŠ” ì¢…ë¥˜ê°€ ë§ìœ¼ë¯€ë¡œ Top 20ê¹Œì§€ í™•ì¸\n",
    "plot_distribution('liked category', data_storage['liked category'], top_k=20)\n",
    "plot_distribution('disliked category', data_storage['disliked category'], top_k=20)\n",
    "plot_distribution('liked author', data_storage['liked author'], top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139d2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_TXT = os.path.join(\n",
    "    \"data/books/books_llmrec_format\",\n",
    "    \"liked_author_by_user.txt\"\n",
    ")\n",
    "\n",
    "with open(OUT_TXT, \"w\", encoding=\"utf-8\") as f:\n",
    "    for user_id, profile_text in augmented_dict.items():\n",
    "        profile = clean_and_parse(profile_text)\n",
    "\n",
    "        if not profile:\n",
    "            continue\n",
    "\n",
    "        val = profile.get(\"liked author\", None)\n",
    "        if not val:\n",
    "            continue\n",
    "\n",
    "        # author ë¶„ë¦¬\n",
    "        authors = split_authors(val)\n",
    "\n",
    "        if not authors:\n",
    "            continue\n",
    "\n",
    "        # ì¤‘ë³µ ì œê±° + ìˆœì„œ ìœ ì§€\n",
    "        seen = set()\n",
    "        uniq_authors = []\n",
    "        for a in authors:\n",
    "            if a not in seen:\n",
    "                seen.add(a)\n",
    "                uniq_authors.append(a)\n",
    "\n",
    "        # í•œ ì¤„ë¡œ ì €ì¥\n",
    "        line = f\"{user_id}\\t\" + \" | \".join(uniq_authors) + \"\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "print(f\"âœ… liked author txt ì €ì¥ ì™„ë£Œ: {OUT_TXT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from typing import Dict, Any, Optional, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "file_path = \"data/books/books_llmrec_format/augmented_user_profiling_dict_part1_step0\"\n",
    "base_save_dir = \"data/books/books_llmrec_format/poster/Gender\"\n",
    "\n",
    "target_keys = [\n",
    "    \"age\", \"gender\",\n",
    "    \"liked category\", \"disliked category\",\n",
    "    \"liked author\", \"country\", \"language\",\n",
    "]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Parsing / Normalization\n",
    "# -----------------------------\n",
    "def clean_and_parse(profile_text: str, model: str = \"gpt-4o\") -> Optional[Dict[str, Any]]:\n",
    "    if not isinstance(profile_text, str):\n",
    "        return None\n",
    "\n",
    "    if model == \"gpt-4o\":\n",
    "        cleaned = (\n",
    "            profile_text.strip()\n",
    "            .replace(\"'''\", \"\")\n",
    "            .replace(\"```json\", \"\")\n",
    "            .replace(\"```\", \"\")\n",
    "            .replace(\"\\n\", \"\")\n",
    "            .replace(\"\\r\", \"\")\n",
    "            .replace(\"children's\", \"childrens\")\n",
    "            .replace(\"Children\\\\'s\", \"childrens\")\n",
    "        )\n",
    "    else:\n",
    "        cleaned = profile_text.strip()\n",
    "\n",
    "    try:\n",
    "        cleaned_json = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        return json.loads(cleaned_json)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            return ast.literal_eval(cleaned)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def normalize_gender(raw: Any) -> Optional[str]:\n",
    "    if raw is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(raw, list) and len(raw) > 0:\n",
    "        raw = raw[0]\n",
    "\n",
    "    if not isinstance(raw, str):\n",
    "        return None\n",
    "\n",
    "    s = raw.strip().lower()\n",
    "    if s in {\"m\", \"male\", \"man\", \"boy\", \"masculine\"}:\n",
    "        return \"m\"\n",
    "    if s in {\"f\", \"female\", \"woman\", \"girl\", \"feminine\"}:\n",
    "        return \"f\"\n",
    "\n",
    "    if len(s) >= 1:\n",
    "        if s[0] == \"m\":\n",
    "            return \"m\"\n",
    "        if s[0] == \"f\":\n",
    "            return \"f\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "GENRE_NORMALIZATION = {\n",
    "    \"thrillers & suspense\": \"thriller & suspense\",\n",
    "    \"thriller and suspense\": \"thriller & suspense\",\n",
    "    \"thrillers and suspense\": \"thriller & suspense\",\n",
    "}\n",
    "\n",
    "def normalize_genre(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    return GENRE_NORMALIZATION.get(s, s)\n",
    "\n",
    "def maybe_normalize_by_key(key: str, s: str) -> str:\n",
    "    # ì¥ë¥´/ì¹´í…Œê³ ë¦¬ ê³„ì—´ì—ë§Œ ì ìš© (ì›í•˜ë©´ í‚¤ ì¶”ê°€ ê°€ëŠ¥)\n",
    "    if key in {\"liked category\", \"disliked category\"}:\n",
    "        return normalize_genre(s)\n",
    "    return s\n",
    "\n",
    "def normalize_item_value(key: str, val: Any) -> List[str]:\n",
    "    out: List[str] = []\n",
    "\n",
    "    if val is None:\n",
    "        return out\n",
    "\n",
    "    def push(x: str):\n",
    "        x = x.strip().lower()\n",
    "        if not x or x in {\"unknown\", \"none\", \"n/a\"}:\n",
    "            return\n",
    "        x = maybe_normalize_by_key(key, x)\n",
    "        out.append(x)\n",
    "\n",
    "    if isinstance(val, list):\n",
    "        for item in val:\n",
    "            if item is None:\n",
    "                continue\n",
    "            push(str(item))\n",
    "        return out\n",
    "\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip()\n",
    "        if not s:\n",
    "            return out\n",
    "        if s.lower() in {\"unknown\", \"none\", \"n/a\"}:\n",
    "            return out\n",
    "\n",
    "        if \",\" in s:\n",
    "            for p in s.split(\",\"):\n",
    "                push(p)\n",
    "            return out\n",
    "\n",
    "        if key == \"gender\":\n",
    "            push(s[0])   # m/fë§Œ ë‚¨ê¸°ê²Œ ë¨\n",
    "        else:\n",
    "            push(s)\n",
    "        return out\n",
    "\n",
    "    push(str(val))\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Plotting\n",
    "# -----------------------------\n",
    "def plot_distribution(\n",
    "    key: str,\n",
    "    data_list: List[str],\n",
    "    save_dir: str,\n",
    "    gender: str,\n",
    "    n_users: int,\n",
    "    top_k: int = 10,\n",
    "):\n",
    "\n",
    "    if not data_list:\n",
    "        print(f\"{key}: no data\")\n",
    "        return\n",
    "\n",
    "    counter = Counter(data_list)\n",
    "    df = (\n",
    "        pd.DataFrame.from_dict(counter, orient=\"index\", columns=[\"Count\"])\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"Category\"})\n",
    "    )\n",
    "\n",
    "    df_topk = df.sort_values(by=\"Count\", ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(df_topk[\"Category\"], df_topk[\"Count\"], alpha=0.85, edgecolor=\"black\")\n",
    "\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            yval + (yval * 0.01),\n",
    "            int(yval),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    title = f\"{gender} (N={n_users}) â€” {key.replace('_', ' ').title()} (Top {top_k})\"\n",
    "    plt.title(title, fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "    plt.xlabel(key.replace('_', ' ').title(), fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    filename = f\"Predicted_{key.replace(' ', '_')}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.savefig(save_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"saved: {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def get_topk_for_key(key: str) -> int:\n",
    "    if key == \"age\":\n",
    "        return 15\n",
    "    if key == \"gender\":\n",
    "        return 5\n",
    "    if key == \"country\":\n",
    "        return 15\n",
    "    if key == \"language\":\n",
    "        return 10\n",
    "    if key in {\"liked category\", \"disliked category\", \"liked author\"}:\n",
    "        return 20\n",
    "    return 10\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main() -> None:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"file not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        augmented_dict = pickle.load(f)\n",
    "\n",
    "    print(f\"loaded: {file_path} (users: {len(augmented_dict)})\")\n",
    "\n",
    "    profiles_by_gender: Dict[str, List[Dict[str, Any]]] = {\"m\": [], \"f\": []}\n",
    "    failed = 0\n",
    "    unknown_gender = 0\n",
    "\n",
    "    for user_id, profile_text in augmented_dict.items():\n",
    "        profile = clean_and_parse(profile_text)\n",
    "        if not profile:\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        g = normalize_gender(profile.get(\"gender\"))\n",
    "        if g not in {\"m\", \"f\"}:\n",
    "            unknown_gender += 1\n",
    "            continue\n",
    "\n",
    "        profile[\"gender\"] = g\n",
    "        profiles_by_gender[g].append(profile)\n",
    "\n",
    "    print(f\"parsed_ok_male: {len(profiles_by_gender['m'])}\")\n",
    "    print(f\"parsed_ok_female: {len(profiles_by_gender['f'])}\")\n",
    "    print(f\"parse_failed: {failed}\")\n",
    "    print(f\"unknown_gender_excluded: {unknown_gender}\")\n",
    "\n",
    "    save_dir_map = {\n",
    "        \"m\": os.path.join(base_save_dir, \"Male\"),\n",
    "        \"f\": os.path.join(base_save_dir, \"Female\"),\n",
    "    }\n",
    "\n",
    "    for g in [\"m\", \"f\"]:\n",
    "        group_profiles = profiles_by_gender[g]\n",
    "        save_dir = save_dir_map[g]\n",
    "\n",
    "        gender_name = \"Male\" if g == \"m\" else \"Female\"\n",
    "        n_users = len(group_profiles)\n",
    "\n",
    "        data_storage = {k: [] for k in target_keys}        \n",
    "\n",
    "        for profile in group_profiles:\n",
    "            for key in target_keys:\n",
    "                vals = normalize_item_value(key, profile.get(key))\n",
    "                if vals:\n",
    "                    data_storage[key].extend(vals)\n",
    "\n",
    "        for key in target_keys:\n",
    "            k = get_topk_for_key(key)\n",
    "            plot_distribution(\n",
    "                key=key,\n",
    "                data_list=data_storage[key],\n",
    "                save_dir=save_dir,\n",
    "                gender=gender_name,\n",
    "                n_users=n_users,\n",
    "                top_k=k,\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855436c7",
   "metadata": {},
   "source": [
    "### RQ3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78432cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# âœ… ê¸°ë³¸ ì„¤ì •\n",
    "base_dir = \"data/books/books_llmrec_format/\"\n",
    "save_dir = \"data/books/books_llmrec_format/poster/step_comparison/\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# âœ… ë¶„ì„í•  íŒŒì¼ ëª©ë¡ (part5_step0 ~ step4)\n",
    "file_names = [f\"augmented_user_profiling_dict_part5_step{i}\" for i in range(5)]\n",
    "\n",
    "# âœ… ì‹œê°í™”í•  ëŒ€ìƒ í‚¤(Key) ì •ì˜\n",
    "target_keys = [\n",
    "    'age', 'gender', \n",
    "    'liked category', 'disliked category', \n",
    "    'liked author', 'country', 'language'\n",
    "]\n",
    "\n",
    "# âœ… ë°ì´í„° íŒŒì‹± ë° ì¶”ì¶œ í•¨ìˆ˜ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)\n",
    "def clean_and_parse(profile_text, model=\"gpt-4o\"):\n",
    "    if model == \"gpt-4o\":\n",
    "        cleaned = profile_text.strip().replace(\"'''\", \"\").replace('```json', '').replace('```', '').replace('\\n', '').replace('\\r', '').replace(\"children's\", \"childrens\").replace(\"Children\\'s\", \"childrens\")\n",
    "    else:\n",
    "        cleaned = profile_text.strip()\n",
    "    try:\n",
    "        cleaned = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            return ast.literal_eval(cleaned)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "# âœ… ì „ì²´ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_all_steps_data(base_dir, file_names, target_keys):\n",
    "    # êµ¬ì¡°: { 'step0': {'age': [], 'gender': []...}, 'step1': ... }\n",
    "    all_steps_data = {} \n",
    "\n",
    "    for fname in file_names:\n",
    "        step_name = fname.split('_dict_')[-1] # ì˜ˆ: part5_step0\n",
    "        file_path = os.path.join(base_dir, fname)\n",
    "        \n",
    "        step_data = {key: [] for key in target_keys}\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"ğŸ“‚ íŒŒì¼ ë¡œë“œ ì¤‘: {fname} ...\", end=\" \")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                augmented_dict = pickle.load(f)\n",
    "            print(f\"ì™„ë£Œ (ì´ {len(augmented_dict)}ëª…)\")\n",
    "            \n",
    "            for user_id, profile_text in augmented_dict.items():\n",
    "                profile = clean_and_parse(profile_text)\n",
    "                if not profile: continue\n",
    "\n",
    "                for key in target_keys:\n",
    "                    val = profile.get(key, None)\n",
    "                    if val:\n",
    "                        # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬\n",
    "                        if isinstance(val, list):\n",
    "                            cleaned_items = [str(item).strip().lower() for item in val if item]\n",
    "                            step_data[key].extend(cleaned_items)\n",
    "                        # ë¬¸ìì—´ ì²˜ë¦¬\n",
    "                        elif isinstance(val, str):\n",
    "                            val = val.strip()\n",
    "                            if val.lower() in ['unknown', 'none', 'n/a', '']: continue\n",
    "                            \n",
    "                            if ',' in val:\n",
    "                                cleaned_items = [item.strip().lower() for item in val.split(',')]\n",
    "                                step_data[key].extend(cleaned_items)\n",
    "                            else:\n",
    "                                if key == 'gender':\n",
    "                                    step_data[key].append(val.lower()[0])\n",
    "                                else:\n",
    "                                    step_data[key].append(val.lower())\n",
    "        else:\n",
    "            print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "        \n",
    "        all_steps_data[step_name] = step_data\n",
    "        \n",
    "    return all_steps_data\n",
    "\n",
    "# âœ… í†µí•© ì‹œê°í™” í•¨ìˆ˜ (2í–‰ 3ì—´ ê·¸ë¦¬ë“œ)\n",
    "def plot_grid_distribution(target_key, all_steps_data, top_k=10):\n",
    "    steps = sorted(all_steps_data.keys()) # step0 ~ step4 ìˆœì„œ ë³´ì¥\n",
    "    \n",
    "    # 2í–‰ 3ì—´ subplot ìƒì„± (ì´ 6ì¹¸ ì¤‘ 5ì¹¸ ì‚¬ìš©)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten() # 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì¸ë±ì‹± í¸ì˜ì„± í™•ë³´\n",
    "    \n",
    "    print(f\"\\nğŸ¨ ê·¸ë¦¬ëŠ” ì¤‘: {target_key}\")\n",
    "\n",
    "    for i, step_name in enumerate(steps):\n",
    "        if i >= 5: break # 5ê°œê¹Œì§€ë§Œ ì²˜ë¦¬\n",
    "        \n",
    "        data_list = all_steps_data[step_name][target_key]\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if not data_list:\n",
    "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center')\n",
    "            ax.set_title(f\"{step_name}\", fontsize=12, fontweight='bold')\n",
    "            continue\n",
    "\n",
    "        # ë¹ˆë„ ê³„ì‚° ë° ìƒìœ„ Kê°œ ì¶”ì¶œ\n",
    "        counter = Counter(data_list)\n",
    "        df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count']).reset_index()\n",
    "        df.rename(columns={'index': 'Category'}, inplace=True)\n",
    "        df_topk = df.sort_values(by='Count', ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "        # ë§‰ëŒ€ ê·¸ë˜í”„\n",
    "        bars = ax.bar(df_topk['Category'], df_topk['Count'], color='skyblue', edgecolor='black', alpha=0.8)\n",
    "\n",
    "        # ìˆ˜ì¹˜ í‘œì‹œ\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, yval + (yval * 0.01), int(yval), \n",
    "                     ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        # ì„œë¸Œí”Œë¡¯ ìŠ¤íƒ€ì¼ë§\n",
    "        title_step = step_name.replace(\"part5_\", \"\").replace(\"_\", \" \").upper()\n",
    "        ax.set_title(f\"{title_step}\", fontsize=12, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 6ë²ˆì§¸ ì¹¸(ë§ˆì§€ë§‰ ì¹¸)ì€ ë¹„ì›Œë‘ê¸° (ë°ì´í„°ê°€ 5ê°œì´ë¯€ë¡œ)\n",
    "    if len(steps) < 6:\n",
    "        for j in range(len(steps), 6):\n",
    "            axes[j].axis('off')\n",
    "\n",
    "    # ì „ì²´ íƒ€ì´í‹€ ë° ë ˆì´ì•„ì›ƒ ì¡°ì •\n",
    "    plt.suptitle(f\"Distribution Changes: {target_key.title()} (Step 0 - 4)\", fontsize=20, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Main Title ê³µê°„ í™•ë³´\n",
    "\n",
    "    # ì €ì¥\n",
    "    filename = f\"Comparison_{target_key.replace(' ', '_')}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"âœ… ì €ì¥ ì™„ë£Œ: {filename}\")\n",
    "    # plt.show() # í•„ìš”ì‹œ ì£¼ì„ í•´ì œ\n",
    "    plt.close() # ë©”ëª¨ë¦¬ í•´ì œë¥¼ ìœ„í•´ ë‹«ê¸°\n",
    "\n",
    "# ==========================================\n",
    "# ğŸš€ ë©”ì¸ ì‹¤í–‰ë¶€\n",
    "# ==========================================\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ\n",
    "print(\"========== ë°ì´í„° ë¡œë”© ì‹œì‘ ==========\")\n",
    "db = load_all_steps_data(base_dir, file_names, target_keys)\n",
    "\n",
    "# 2. ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "print(\"\\n========== ê·¸ë˜í”„ ìƒì„± ì‹œì‘ ==========\")\n",
    "\n",
    "# ê° í‚¤ë³„ë¡œ Top K ì„¤ì •í•˜ì—¬ ê·¸ë¦¬ê¸°\n",
    "plot_grid_distribution('age', db, top_k=15)\n",
    "plot_grid_distribution('gender', db, top_k=5)\n",
    "plot_grid_distribution('country', db, top_k=10)\n",
    "plot_grid_distribution('language', db, top_k=10)\n",
    "\n",
    "plot_grid_distribution('liked category', db, top_k=15)\n",
    "plot_grid_distribution('disliked category', db, top_k=15)\n",
    "plot_grid_distribution('liked author', db, top_k=15)\n",
    "\n",
    "print(\"\\nğŸ‰ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffc2e29",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e20267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Paths (Books)\n",
    "# =========================\n",
    "PART1_PATH = \"data/books/books_llmrec_format/augmented_user_profiling_dict_part5_step4_try1\"\n",
    "PART5_PATH = \"data/books/books_llmrec_format/augmented_user_profiling_dict_part5_step4_try0\"\n",
    "\n",
    "OUT_DIR = \"data/books/books_llmrec_format/poster/\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_SUMMARY_CSV = os.path.join(OUT_DIR, \"profile_rawstring_consistency_part1_vs_part5_summary.csv\")\n",
    "OUT_COUNTS_CSV  = os.path.join(OUT_DIR, \"profile_rawstring_consistency_part1_vs_part5_counts.csv\")\n",
    "OUT_USERS_TXT   = os.path.join(OUT_DIR, \"profile_rawstring_consistency_part1_vs_part5_mismatch_users.txt\")\n",
    "\n",
    "# =========================\n",
    "# Target keys (Books)\n",
    "# =========================\n",
    "TARGET_KEYS = [\n",
    "    \"age\", \"gender\",\n",
    "    \"liked category\", \"disliked category\",\n",
    "    \"liked author\", \"country\", \"language\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Load pickles\n",
    "# =========================\n",
    "def load_pickle(path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return {str(k): v for k, v in obj.items()}\n",
    "\n",
    "part1 = load_pickle(PART1_PATH)\n",
    "part5 = load_pickle(PART5_PATH)\n",
    "\n",
    "common_users = sorted(set(part1.keys()) & set(part5.keys()), key=lambda x: int(x) if x.isdigit() else x)\n",
    "print(f\"âœ… part1 users: {len(part1)}\")\n",
    "print(f\"âœ… part5 users: {len(part5)}\")\n",
    "print(f\"âœ… common users: {len(common_users)}\")\n",
    "\n",
    "# =========================\n",
    "# Parse + key normalization only\n",
    "# (ê°’ ì •ê·œí™”ëŠ” í•˜ì§€ ì•ŠìŒ)\n",
    "# =========================\n",
    "def clean_and_parse(profile_text: Any) -> Optional[Dict[str, Any]]:\n",
    "    if profile_text is None:\n",
    "        return None\n",
    "\n",
    "    cleaned = str(profile_text).strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    cleaned = cleaned.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "    # json\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # python literal\n",
    "    try:\n",
    "        obj = ast.literal_eval(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # single quote -> double quote í›„ json\n",
    "    try:\n",
    "        cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        obj = json.loads(cleaned_forced)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_key_name(k: str) -> str:\n",
    "    return str(k).lower().strip().replace(\" \", \"_\")\n",
    "\n",
    "def normalize_keys(profile: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        nk = normalize_key_name(k)\n",
    "\n",
    "        # Booksì—ì„œ ì˜¬ ìˆ˜ ìˆëŠ” í”í•œ ë³€í˜•ì„ ìµœëŒ€í•œ í¡ìˆ˜ (ê°’ì€ ê·¸ëŒ€ë¡œ)\n",
    "        # liked category / liked_category / liked categories / liked_categories ë“±\n",
    "        if nk in {\"liked_categories\", \"liked_category\"}:\n",
    "            nk = \"liked_category\"\n",
    "        if nk in {\"disliked_categories\", \"disliked_category\"}:\n",
    "            nk = \"disliked_category\"\n",
    "        if nk in {\"liked_authors\", \"liked_author\"}:\n",
    "            nk = \"liked_author\"\n",
    "\n",
    "        new_profile[nk] = v\n",
    "    return new_profile\n",
    "\n",
    "# TARGET_KEYSë„ normalize_key_name ê¸°ì¤€ìœ¼ë¡œ ë§¤ì¹­ë˜ê²Œ ë³€í™˜\n",
    "TARGET_KEYS_NORM = [normalize_key_name(k) for k in TARGET_KEYS]\n",
    "\n",
    "# =========================\n",
    "# Raw-string comparison\n",
    "# =========================\n",
    "def val_to_raw(v: Any, sort_list: bool = False) -> Any:\n",
    "    \"\"\"\n",
    "    ê°’ ìì²´ë¥¼ \"ì›ë³¸ ë¬¸ìì—´ì— ê°€ê¹Œìš´ í˜•íƒœ\"ë¡œ ë¹„êµí•˜ê¸° ìœ„í•œ ë³€í™˜.\n",
    "    - str: stripë§Œ\n",
    "    - list: ì›ì†Œë¥¼ strë¡œ ë³€í™˜í•œ list (sort_list=Trueë©´ ì •ë ¬í•´ì„œ ìˆœì„œ ì˜í–¥ ì œê±°)\n",
    "    - ê¸°íƒ€: str(v).strip()\n",
    "    \"\"\"\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, str):\n",
    "        return v.strip()\n",
    "    if isinstance(v, list):\n",
    "        lst = [str(x) for x in v]\n",
    "        return sorted(lst) if sort_list else lst\n",
    "    return str(v).strip()\n",
    "\n",
    "def raw_equal(a: Any, b: Any) -> bool:\n",
    "    return a == b\n",
    "\n",
    "# ë¦¬ìŠ¤íŠ¸ ìˆœì„œê¹Œì§€ ê°™ì•„ì•¼ \"ê°™ìŒ\"ìœ¼ë¡œ ë³¼ì§€ ê²°ì •\n",
    "# - True: ìˆœì„œ ë¬´ì‹œ (ì •ë ¬ í›„ ë¹„êµ)\n",
    "# - False: ìˆœì„œê¹Œì§€ í¬í•¨ (ê·¸ëŒ€ë¡œ ë¹„êµ)\n",
    "SORT_LIST_BEFORE_COMPARE = False\n",
    "\n",
    "rows = []\n",
    "mismatch_users = []\n",
    "\n",
    "for uid in common_users:\n",
    "    p1_raw = clean_and_parse(part1[uid])\n",
    "    p5_raw = clean_and_parse(part5[uid])\n",
    "\n",
    "    if not p1_raw or not p5_raw:\n",
    "        rows.append({\n",
    "            \"user_id\": uid,\n",
    "            \"parse_ok_part1\": bool(p1_raw),\n",
    "            \"parse_ok_part5\": bool(p5_raw),\n",
    "            \"mismatch_count\": None,\n",
    "            \"mismatch_keys\": \"PARSE_FAIL\",\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    p1 = normalize_keys(p1_raw)\n",
    "    p5 = normalize_keys(p5_raw)\n",
    "\n",
    "    mism_keys = []\n",
    "    detail = {}\n",
    "\n",
    "    for key_norm in TARGET_KEYS_NORM:\n",
    "        v1 = val_to_raw(p1.get(key_norm, None), sort_list=SORT_LIST_BEFORE_COMPARE)\n",
    "        v2 = val_to_raw(p5.get(key_norm, None), sort_list=SORT_LIST_BEFORE_COMPARE)\n",
    "\n",
    "        if not raw_equal(v1, v2):\n",
    "            mism_keys.append(key_norm)\n",
    "\n",
    "        detail[f\"{key_norm}_part1\"] = v1\n",
    "        detail[f\"{key_norm}_part5\"] = v2\n",
    "\n",
    "    mismatch_count = len(mism_keys)\n",
    "    if mismatch_count > 0:\n",
    "        mismatch_users.append(uid)\n",
    "\n",
    "    row = {\n",
    "        \"user_id\": uid,\n",
    "        \"parse_ok_part1\": True,\n",
    "        \"parse_ok_part5\": True,\n",
    "        \"mismatch_count\": mismatch_count,\n",
    "        \"mismatch_keys\": \",\".join(mism_keys) if mism_keys else \"\",\n",
    "    }\n",
    "    row.update(detail)\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# Save outputs\n",
    "# =========================\n",
    "df.to_csv(OUT_SUMMARY_CSV, index=False)\n",
    "print(f\"âœ… Saved summary CSV: {OUT_SUMMARY_CSV}\")\n",
    "\n",
    "# fieldë³„ mismatch count (parse okë§Œ)\n",
    "df_ok = df[(df[\"parse_ok_part1\"] == True) & (df[\"parse_ok_part5\"] == True)].copy()\n",
    "\n",
    "field_counts = Counter()\n",
    "for keys in df_ok[\"mismatch_keys\"].fillna(\"\").tolist():\n",
    "    if not keys:\n",
    "        continue\n",
    "    for k in keys.split(\",\"):\n",
    "        k = k.strip()\n",
    "        if k:\n",
    "            field_counts[k] += 1\n",
    "\n",
    "counts_df = pd.DataFrame(\n",
    "    sorted(field_counts.items(), key=lambda x: x[1], reverse=True),\n",
    "    columns=[\"field\", \"mismatch_user_count\"]\n",
    ")\n",
    "counts_df.to_csv(OUT_COUNTS_CSV, index=False)\n",
    "print(f\"âœ… Saved mismatch counts CSV: {OUT_COUNTS_CSV}\")\n",
    "\n",
    "with open(OUT_USERS_TXT, \"w\") as f:\n",
    "    for uid in mismatch_users:\n",
    "        f.write(f\"{uid}\\n\")\n",
    "print(f\"âœ… Saved mismatch users list: {OUT_USERS_TXT}\")\n",
    "\n",
    "print(\"\\n===== QUICK STATS =====\")\n",
    "print(\"parsed OK users:\", len(df_ok))\n",
    "print(\"users with >=1 mismatch:\", int((df_ok[\"mismatch_count\"].fillna(0) > 0).sum()))\n",
    "if (df_ok[\"mismatch_count\"].fillna(0) > 0).any():\n",
    "    print(\"avg mismatch_count among mismatched:\",\n",
    "          float(df_ok.loc[df_ok[\"mismatch_count\"].fillna(0) > 0, \"mismatch_count\"].mean()))\n",
    "print(\"\\nTop mismatch fields:\")\n",
    "print(counts_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31e869",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c5314",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Paths (Books)\n",
    "# =========================\n",
    "META_JSONL_PATH = \"data/books/item_meta_2017_kcore10_user_item_split_filtered.json\"\n",
    "PKL_PATH = \"Augmentation/data/books/aug_triplets_part1_step0_AB.pkl\"\n",
    "\n",
    "OUT_DIR = \"Augmentation/data/books/results\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def load_books_item_meta_jsonl(meta_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    JSONL í•œ ì¤„ ì˜ˆì‹œ:\n",
    "    {\"item_id\": 0, \"asin\": \"...\", \"title\": \"...\", \"category\": [\"Books\", \"Literature & Fiction\", \"Genre Fiction\"], ...}\n",
    "\n",
    "    ë°˜í™˜ df ì»¬ëŸ¼:\n",
    "    - item_id (int)\n",
    "    - title (str, optional)\n",
    "    - category (list[str], optional)\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    with open(meta_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if \"item_id\" not in obj:\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"item_id\": int(obj[\"item_id\"]),\n",
    "                \"title\": obj.get(\"title\", None),\n",
    "                \"category\": obj.get(\"category\", None),\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "def build_category_long_table(\n",
    "    meta_df: pd.DataFrame,\n",
    "    mode: str = \"leaf\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    (item_id, category_token) long-form ìƒì„±.\n",
    "\n",
    "    mode:\n",
    "    - \"leaf\": category pathì˜ ë§ˆì§€ë§‰ í† í°ë§Œ ì‚¬ìš©\n",
    "      ì˜ˆ: [\"Books\",\"Literature & Fiction\",\"Genre Fiction\"] -> \"Genre Fiction\"\n",
    "    - \"all\": category pathì˜ ëª¨ë“  í† í°ì„ ê°ê° ì¹´ìš´íŠ¸\n",
    "      ì˜ˆ: -> \"Books\", \"Literature & Fiction\", \"Genre Fiction\"\n",
    "    - \"path\": category path ì „ì²´ë¥¼ \" > \"ë¡œ joiní•´ì„œ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ì¹´ìš´íŠ¸\n",
    "      ì˜ˆ: -> \"Books > Literature & Fiction > Genre Fiction\"\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for _, r in meta_df.iterrows():\n",
    "        item_id = int(r[\"item_id\"])\n",
    "        cat = r.get(\"category\", None)\n",
    "\n",
    "        if cat is None or (isinstance(cat, float) and pd.isna(cat)):\n",
    "            continue\n",
    "\n",
    "        if isinstance(cat, str):\n",
    "            # ì–´ë–¤ ì „ì²˜ë¦¬ì—ì„œ ë¬¸ìì—´ë¡œ ë“¤ì–´ì™”ì„ ê°€ëŠ¥ì„± ë°©ì–´\n",
    "            # \"['Books', '...']\" í˜•íƒœë©´ evalì€ ìœ„í—˜í•˜ë‹ˆ ìµœì†Œ ì²˜ë¦¬ë§Œ\n",
    "            cat_list = [cat.strip()] if cat.strip() else []\n",
    "        elif isinstance(cat, list):\n",
    "            cat_list = [str(x).strip() for x in cat if x is not None and str(x).strip()]\n",
    "        else:\n",
    "            cat_list = [str(cat).strip()] if str(cat).strip() else []\n",
    "\n",
    "        if not cat_list:\n",
    "            continue\n",
    "\n",
    "        if mode == \"leaf\":\n",
    "            rows.append((item_id, cat_list[-1]))\n",
    "        elif mode == \"all\":\n",
    "            for tok in cat_list:\n",
    "                rows.append((item_id, tok))\n",
    "        elif mode == \"path\":\n",
    "            rows.append((item_id, \" > \".join(cat_list)))\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    return pd.DataFrame(rows, columns=[\"item_id\", \"category_token\"])\n",
    "\n",
    "\n",
    "def analyze_pos_category_distribution(\n",
    "    triplets_pkl_path: str,\n",
    "    cat_long_df: pd.DataFrame,\n",
    "    topk: int = 30,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    ë°˜í™˜:\n",
    "    - counts: pos ì„ íƒ ë¹ˆë„ ê¸°ë°˜ category ë¶„í¬ (count, prob)\n",
    "    - cat_item_count_df: ìœ ë‹ˆí¬ pos ì•„ì´í…œ ê¸°ì¤€ category ë¶„í¬ (item_count)\n",
    "    \"\"\"\n",
    "    triplets = load_pickle(triplets_pkl_path)\n",
    "\n",
    "    pos_items = []\n",
    "    for t in triplets:\n",
    "        if not isinstance(t, (list, tuple)) or len(t) < 3:\n",
    "            continue\n",
    "        _, pos, _ = t[0], t[1], t[2]\n",
    "        try:\n",
    "            pos_items.append(int(pos))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if len(pos_items) == 0:\n",
    "        empty1 = pd.DataFrame(columns=[\"category_token\", \"count\", \"prob\"])\n",
    "        empty2 = pd.DataFrame(columns=[\"category_token\", \"item_count\"])\n",
    "        return empty1, empty2\n",
    "\n",
    "    # === pos_items ë‚´ ìœ ë‹ˆí¬ ì•„ì´í…œë³„ ì„ íƒ íšŸìˆ˜ ===\n",
    "    pos_item_counter = Counter(pos_items)\n",
    "    pos_item_count_df = (\n",
    "        pd.DataFrame(pos_item_counter.items(), columns=[\"item_id\", \"pos_count\"])\n",
    "        .sort_values(\"pos_count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"[POS ITEM COUNTS] unique items: {len(pos_item_count_df)}\")\n",
    "    print(pos_item_count_df.head(20))\n",
    "    print(\"count stats:\\n\", pos_item_count_df[\"pos_count\"].describe())\n",
    "\n",
    "    # === ìœ ë‹ˆí¬ ì•„ì´í…œ ê¸°ì¤€ category ì¹´ìš´íŠ¸ ===\n",
    "    unique_pos_items = pos_item_count_df[\"item_id\"].unique()\n",
    "\n",
    "    unique_item_cats = (\n",
    "        pd.DataFrame({\"item_id\": unique_pos_items})\n",
    "        .merge(cat_long_df, on=\"item_id\", how=\"left\")\n",
    "        .dropna(subset=[\"category_token\"])\n",
    "        .drop_duplicates(subset=[\"item_id\", \"category_token\"])\n",
    "    )\n",
    "\n",
    "    cat_item_count_df = (\n",
    "        unique_item_cats.groupby(\"category_token\")\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "        .rename(\"item_count\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    print(\"[UNIQUE POS ITEMS] category count (item-based)\")\n",
    "    print(cat_item_count_df.head(20))\n",
    "\n",
    "    # === ë¹ˆë„ ê¸°ë°˜ category ë¶„í¬ ===\n",
    "    pos_df = pd.DataFrame({\"item_id\": pos_items})\n",
    "    merged = pos_df.merge(cat_long_df, on=\"item_id\", how=\"left\").dropna(subset=[\"category_token\"])\n",
    "\n",
    "    counts = (\n",
    "        merged.groupby(\"category_token\")\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "        .rename(\"count\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    total = int(counts[\"count\"].sum()) if len(counts) > 0 else 0\n",
    "    counts[\"prob\"] = counts[\"count\"] / max(1, total)\n",
    "\n",
    "    if topk is not None:\n",
    "        counts = counts.head(topk)\n",
    "\n",
    "    return counts, cat_item_count_df\n",
    "\n",
    "\n",
    "def plot_bar_vertical(\n",
    "    df: pd.DataFrame,\n",
    "    x_col: str,\n",
    "    y_col: str,\n",
    "    title: str,\n",
    "    topk: int = 10,\n",
    "    save_path: str | None = None,\n",
    "):\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "\n",
    "    d = df.sort_values(y_col, ascending=False).head(topk).copy()\n",
    "\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    bars = plt.bar(d[x_col], d[y_col])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            h,\n",
    "            f\"{int(h)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"saved figure to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Run\n",
    "# =========================\n",
    "meta_df = load_books_item_meta_jsonl(META_JSONL_PATH)\n",
    "print(\"meta_df:\", meta_df.shape)\n",
    "print(meta_df.head(3))\n",
    "\n",
    "# ì—¬ê¸° modeë¥¼ ë°”ê¿”ê°€ë©° ë³´ë©´ ë¨: \"leaf\" ì¶”ì²œ\n",
    "MODE = \"leaf\"   # \"leaf\" | \"all\" | \"path\"\n",
    "cat_long_df = build_category_long_table(meta_df, mode=MODE)\n",
    "print(\"cat_long_df:\", cat_long_df.shape)\n",
    "print(cat_long_df.head(5))\n",
    "\n",
    "pos_cat_df, unique_pos_cat_df = analyze_pos_category_distribution(PKL_PATH, cat_long_df, topk=30)\n",
    "\n",
    "# CSV ì €ì¥\n",
    "pos_cat_csv = f\"{OUT_DIR}/pos_category_dist_part1_step0_AB_{MODE}.csv\"\n",
    "uniq_cat_csv = f\"{OUT_DIR}/unique_pos_item_category_dist_part1_step0_AB_{MODE}.csv\"\n",
    "pos_cat_df.to_csv(pos_cat_csv, index=False)\n",
    "unique_pos_cat_df.to_csv(uniq_cat_csv, index=False)\n",
    "print(\"saved:\", pos_cat_csv)\n",
    "print(\"saved:\", uniq_cat_csv)\n",
    "\n",
    "print(\"\\n[AB] positive(pos) category distribution (top 30)\")\n",
    "print(pos_cat_df.head(30))\n",
    "\n",
    "# Plot (frequency-based)\n",
    "plot_bar_vertical(\n",
    "    pos_cat_df,\n",
    "    x_col=\"category_token\",\n",
    "    y_col=\"count\",\n",
    "    title=f\"[POS] Category distribution (frequency-based, Top 10) | mode={MODE}\",\n",
    "    topk=10,\n",
    "    save_path=f\"{OUT_DIR}/pos_category_bar_part1_step0_AB_{MODE}.png\",\n",
    ")\n",
    "\n",
    "# Plot (unique-item-based)\n",
    "unique_plot_df = unique_pos_cat_df.rename(columns={\"item_count\": \"count\"})\n",
    "plot_bar_vertical(\n",
    "    unique_plot_df,\n",
    "    x_col=\"category_token\",\n",
    "    y_col=\"count\",\n",
    "    title=f\"[POS UNIQUE ITEMS] Category distribution (item-based, Top 10) | mode={MODE}\",\n",
    "    topk=10,\n",
    "    save_path=f\"{OUT_DIR}/pos_unique_item_category_bar_part1_step0_AB_{MODE}.png\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7646e99e",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8ee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "AB_PATH = \"Augmentation/data/books/aug_triplets_part1_step0_AB.pkl\"\n",
    "BA_PATH = \"Augmentation/data/books/aug_triplets_part1_step0_BA.pkl\"\n",
    "\n",
    "def load_triplets(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)  # List[Tuple[int,int,int]]\n",
    "\n",
    "def build_pos_counter(triplets):\n",
    "    \"\"\"\n",
    "    key = (u, a, b) where a<b are the unordered pair\n",
    "    value = Counter({pos_item: count})\n",
    "    \"\"\"\n",
    "    d = defaultdict(Counter)\n",
    "    for u, pos, neg in triplets:\n",
    "        a, b = sorted((int(pos), int(neg)))\n",
    "        key = (int(u), a, b)\n",
    "        d[key][int(pos)] += 1\n",
    "    return d\n",
    "\n",
    "ab = load_triplets(AB_PATH)\n",
    "ba = load_triplets(BA_PATH)\n",
    "\n",
    "print(\"AB triplets:\", len(ab))\n",
    "print(\"BA triplets:\", len(ba))\n",
    "\n",
    "ab_map = build_pos_counter(ab)\n",
    "ba_map = build_pos_counter(ba)\n",
    "\n",
    "common_keys = set(ab_map.keys()) & set(ba_map.keys())\n",
    "only_ab = set(ab_map.keys()) - set(ba_map.keys())\n",
    "only_ba = set(ba_map.keys()) - set(ab_map.keys())\n",
    "\n",
    "if only_ab or only_ba:\n",
    "    print(f\"WARNING: key mismatch. only_ab={len(only_ab)}, only_ba={len(only_ba)}\")\n",
    "\n",
    "mismatch_total = 0\n",
    "match_total = 0\n",
    "total_compared = 0\n",
    "\n",
    "# optional: mismatch examples\n",
    "examples = []\n",
    "\n",
    "for key in common_keys:\n",
    "    # key=(u,a,b)\n",
    "    u, a, b = key\n",
    "\n",
    "    ab_counts = ab_map[key]  # Counter(pos)\n",
    "    ba_counts = ba_map[key]\n",
    "\n",
    "    total_ab = sum(ab_counts.values())\n",
    "    total_ba = sum(ba_counts.values())\n",
    "    if total_ab != total_ba:\n",
    "        # ê°™ì€ keyì— ëŒ€í•´ AB/BA ìƒ˜í”Œ ìˆ˜ê°€ ë‹¤ë¥´ë©´ ë¹„êµê°€ ì• ë§¤í•¨\n",
    "        # ê·¸ë˜ë„ ê°€ëŠ¥í•œ ë²”ìœ„(min)ê¹Œì§€ë§Œ ë¹„êµí•˜ê±°ë‚˜, ê·¸ëƒ¥ ìŠ¤í‚µí•  ìˆ˜ë„ ìˆìŒ.\n",
    "        # ì—¬ê¸°ì„  \"ê°€ëŠ¥í•œ ë§¤ì¹­\" ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°.\n",
    "        pass\n",
    "\n",
    "    # ê°™ì€ posë¡œ ë§¤ì¹­ ê°€ëŠ¥í•œ ê°œìˆ˜\n",
    "    match_a = min(ab_counts.get(a, 0), ba_counts.get(a, 0))\n",
    "    match_b = min(ab_counts.get(b, 0), ba_counts.get(b, 0))\n",
    "    matches = match_a + match_b\n",
    "\n",
    "    # ë¹„êµ ê°€ëŠ¥í•œ ì´ ê°œìˆ˜(ë©€í‹°ì…‹ì´ë¯€ë¡œ ì–‘ìª½ ì¤‘ ì‘ì€ ìª½ ê¸°ì¤€)\n",
    "    total = min(total_ab, total_ba)\n",
    "\n",
    "    mismatches = total - matches\n",
    "\n",
    "    match_total += matches\n",
    "    mismatch_total += mismatches\n",
    "    total_compared += total\n",
    "\n",
    "    if mismatches > 0 and len(examples) < 20:\n",
    "        examples.append({\n",
    "            \"user\": u, \"pair\": (a, b),\n",
    "            \"AB\": dict(ab_counts), \"BA\": dict(ba_counts),\n",
    "            \"mismatches\": mismatches, \"total\": total\n",
    "        })\n",
    "\n",
    "print(\"\\n===== RESULT =====\")\n",
    "print(\"Compared pairs(keys):\", len(common_keys))\n",
    "print(\"Total comparable samples:\", total_compared)\n",
    "print(\"Match count:\", match_total)\n",
    "print(\"Mismatch count (pos differs AB vs BA):\", mismatch_total)\n",
    "if total_compared > 0:\n",
    "    print(\"Mismatch rate:\", mismatch_total / total_compared)\n",
    "\n",
    "print(\"\\n===== mismatch examples (up to 20) =====\")\n",
    "for ex in examples:\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc50df",
   "metadata": {},
   "source": [
    "# RQ2 & RQ3\n",
    "LLMì´ ìƒì„±í•œ ë°ì´í„°ë¡œ ì¶”ì²œì„ ë°˜ë³µí•œ (í”¼ë“œë°±ë£¨í”„ ì´í›„)ê²°ê³¼ ë¶„ì„(Part1: RQ2, Part5: RQ3)\n",
    "\n",
    "í¬í•¨: LLMRec, A-LLMRec, Augmentation, TR_CF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed917c",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfef477",
   "metadata": {},
   "source": [
    "### A-LLMRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# 2) ê° ì‹œë‚˜ë¦¬ì˜¤ë³„ pred_count ë° user ì§‘í•© ìˆ˜ì§‘\n",
    "scenarios = [\n",
    "    ('case2','part1'),\n",
    "    ('case2','part5'),\n",
    "]\n",
    "pred_counts = {}\n",
    "pred_user_sets = {}\n",
    "for case, part in scenarios:\n",
    "    path = f\"{BASE_DIR}/predict_label_{part}.json\"\n",
    "    with open(path) as f:\n",
    "        raw = json.load(f)\n",
    "    # ë¬¸ìì—´ í‚¤ â†’ ì •ìˆ˜, ì˜ˆì¸¡ ìˆ˜\n",
    "    cnt_dict = {int(u): len(titles) for u, titles in raw.items()}\n",
    "    key = f\"{case}_{part}\"\n",
    "    pred_counts[key] = cnt_dict\n",
    "    pred_user_sets[key] = set(cnt_dict.keys())\n",
    "\n",
    "# 3) ê³µí†µ ìœ ì € ì§‘í•©: train âˆ© label (ìš”ì²­í•˜ì‹  ë°©ì‹), intí˜•ìœ¼ë¡œ ë³€í™˜ \n",
    "common_users = sorted(set(train_data) & set(ground_truth))\n",
    "\n",
    "# 4) ì‹¤ì œ(interactions) ì´í•© ê³„ì‚° (common usersë§Œ)\n",
    "actual_total = sum(len(ground_truth[u]) for u in common_users)\n",
    "print(f\"Actual total interactions (common users): {actual_total}, {len(common_users)}ëª…\\n\")\n",
    "\n",
    "# 5) ì‹œë‚˜ë¦¬ì˜¤ë³„ ê³„ì‚° ë° ì¶œë ¥ + caseë³„ missing user ì¹´ìš´íŠ¸\n",
    "case_missing_users = {'case1': set(), 'case2': set()}\n",
    "common_users = [int(u) for u in common_users]  # ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "for case, part in scenarios:\n",
    "    key = f\"{case}_{part}\"\n",
    "    pred_users = pred_user_sets[key]\n",
    "    missing_users = set(common_users) - pred_users  # ì˜ˆì¸¡ì— ì—†ëŠ” common user\n",
    "    case_missing_users[case].update(missing_users)  # caseë³„ ëˆ„ì \n",
    "\n",
    "    pred_total = sum(pred_counts[key].get(u, 0) for u in common_users)\n",
    "    missing    = actual_total - pred_total\n",
    "    missing_pct= missing / actual_total * 100\n",
    "\n",
    "    print(f\"{key}:\")\n",
    "    print(f\"  Predicted total (common users) = {pred_total}\")\n",
    "    print(f\"  Missing interactions           = {missing} ({missing_pct:.2f}%)\")\n",
    "    print(f\"  Missing users (count)          = {len(missing_users)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_userlist_counts(path: str) -> dict[int, int]:\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    out = {}\n",
    "    if isinstance(data, dict):\n",
    "        for u, arr in data.items():\n",
    "            try:\n",
    "                uid = int(u)\n",
    "            except:\n",
    "                continue\n",
    "            if isinstance(arr, list):\n",
    "                out[uid] = len(arr)\n",
    "            elif isinstance(arr, int):\n",
    "                out[uid] = arr\n",
    "            else:\n",
    "                out[uid] = 0\n",
    "    return out\n",
    "\n",
    "def summarize_case2_part(part: int, max_step: int) -> pd.DataFrame:\n",
    "    rows, all_users = [], set()\n",
    "\n",
    "    # ëª¨ë“  stepì—ì„œ user id ìœ ë‹ˆì˜¨ ìˆ˜ì§‘\n",
    "    for step in range(1, max_step+1):\n",
    "        miss_path = os.path.join(BASE_DIR, f\"missing_titles_books_case2_part{part}_step{step}.json\")\n",
    "        dup_path  = os.path.join(BASE_DIR, f\"skipped_duplicates_books_case2_part{part}_step{step}.json\")\n",
    "        miss = load_userlist_counts(miss_path)\n",
    "        dup  = load_userlist_counts(dup_path)\n",
    "        all_users |= set(miss.keys()) | set(dup.keys())\n",
    "\n",
    "    # stepë³„ ì¹´ìš´íŠ¸(ì—†ìœ¼ë©´ 0)\n",
    "    for step in range(1, max_step+1):\n",
    "        miss_path = os.path.join(BASE_DIR, f\"missing_titles_books_case2_part{part}_step{step}.json\")\n",
    "        dup_path  = os.path.join(BASE_DIR, f\"skipped_duplicates_books_case2_part{part}_step{step}.json\")\n",
    "        miss = load_userlist_counts(miss_path)\n",
    "        dup  = load_userlist_counts(dup_path)\n",
    "        for u in sorted(all_users):\n",
    "            rows.append({\n",
    "                \"user_id\": u,\n",
    "                \"step\": step,\n",
    "                \"missing_title\": int(miss.get(u, 0)),\n",
    "                \"skipped_duplicates\": int(dup.get(u, 0)),\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"user_id\",\"step\"]).reset_index(drop=True)\n",
    "\n",
    "# ìš”ì•½ ìƒì„±\n",
    "df1  = summarize_case2_part(part=1,  max_step=1)\n",
    "df5  = summarize_case2_part(part=5,  max_step=5)\n",
    "\n",
    "# (1) part5: stepë³„ í•©ê³„ í‘œ ì¶œë ¥\n",
    "step_totals1 = (\n",
    "    df1.groupby(\"step\")[[\"missing_title\",\"skipped_duplicates\"]]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .astype({\"step\": int, \"missing_title\": int, \"skipped_duplicates\": int})\n",
    ")\n",
    "print(step_totals1.to_string(index=False))\n",
    "\n",
    "step_totals5 = (\n",
    "    df5.groupby(\"step\")[[\"missing_title\",\"skipped_duplicates\"]]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .astype({\"step\": int, \"missing_title\": int, \"skipped_duplicates\": int})\n",
    ")\n",
    "print(step_totals5.to_string(index=False))\n",
    "\n",
    "\n",
    "# (2) parts overall totals (part5/part10 ì „ì²´ í•©ê³„) ì¶œë ¥\n",
    "overall1  = df1[[\"missing_title\",\"skipped_duplicates\"]].sum().astype(int).to_dict()\n",
    "overall5  = df5[[\"missing_title\",\"skipped_duplicates\"]].sum().astype(int).to_dict()\n",
    "\n",
    "parts_overall = pd.DataFrame([\n",
    "    {\"part\": 1,  **overall1},\n",
    "    {\"part\": 5,  **overall5},\n",
    "])[[\"part\",\"missing_title\",\"skipped_duplicates\"]]\n",
    "\n",
    "print(\"\\n[parts overall totals]\")\n",
    "print(parts_overall.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b968c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "PARTS = 5\n",
    "base_dir_hallu = \"data/books/A-LLMRec_format/A-LLMRec_results\"\n",
    "PREFIX = \"predict_label_part5_step\"   # í•„ìš”í•˜ë©´ predict_label ìª½ìœ¼ë¡œ ë°”ê¿”\n",
    "TRY0, TRY1 = 0, 1\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        uid = int(k)\n",
    "        if v is None:\n",
    "            out[uid] = []\n",
    "        elif isinstance(v, list):\n",
    "            out[uid] = [int(x) for x in v]\n",
    "        else:\n",
    "            out[uid] = [int(v)]\n",
    "    return out\n",
    "\n",
    "def compare_step(d0, d1, step):\n",
    "    all_users = sorted(set(d0.keys()) | set(d1.keys()))\n",
    "    rows = []\n",
    "    changed = 0\n",
    "    for u in all_users:\n",
    "        a = d0.get(u, [])\n",
    "        b = d1.get(u, [])\n",
    "        if a == b:\n",
    "            continue\n",
    "        changed += 1\n",
    "        sa, sb = set(a), set(b)\n",
    "        rows.append({\n",
    "            \"step\": step,\n",
    "            \"user_id\": u,\n",
    "            \"try0\": a,\n",
    "            \"try1\": b,\n",
    "            \"len_try0\": len(a),\n",
    "            \"len_try1\": len(b),\n",
    "            \"only_in_try0\": sorted(list(sa - sb)),\n",
    "            \"only_in_try1\": sorted(list(sb - sa)),\n",
    "            \"set_equal\": (sa == sb),\n",
    "        })\n",
    "    return changed, pd.DataFrame(rows)\n",
    "\n",
    "# ---- run ----\n",
    "summary_rows = []\n",
    "detail_dfs = []\n",
    "\n",
    "for step in range(1, PARTS+1):\n",
    "    p0 = os.path.join(base_dir_hallu, f\"{PREFIX}{step}_try{TRY0}.json\")\n",
    "    p1 = os.path.join(base_dir_hallu, f\"{PREFIX}{step}_try{TRY1}.json\")\n",
    "\n",
    "    d0 = load_json(p0)\n",
    "    d1 = load_json(p1)\n",
    "\n",
    "    changed, df_detail = compare_step(d0, d1, step)\n",
    "    union_users = len(set(d0.keys()) | set(d1.keys()))\n",
    "    exact_equal = union_users - changed\n",
    "\n",
    "    # setì€ ê°™ì€ë° ìˆœì„œë§Œ ë‹¤ë¥¸ ì¼€ì´ìŠ¤\n",
    "    order_only = int(df_detail[\"set_equal\"].sum()) if len(df_detail) else 0\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"step\": step,\n",
    "        \"users_try0\": len(d0),\n",
    "        \"users_try1\": len(d1),\n",
    "        \"users_union\": union_users,\n",
    "        \"users_changed(+1 per user)\": changed,\n",
    "        \"users_exact_equal\": exact_equal,\n",
    "        \"changed_set_equal_but_order_diff\": order_only,\n",
    "        \"file_try0\": p0,\n",
    "        \"file_try1\": p1,\n",
    "    })\n",
    "\n",
    "    detail_dfs.append(df_detail)\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_detail_all = pd.concat(detail_dfs, ignore_index=True) if detail_dfs else pd.DataFrame()\n",
    "\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5f48d",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0e6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9141fb2",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Performance Check\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_prediction(pred_dict, ground_truth, common_users):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    hit_ratio_list = []\n",
    "    ndcg_list = []\n",
    "    hit_item_list = []\n",
    "\n",
    "    for user_str, pred_items in pred_dict.items():\n",
    "        if int(user_str) in common_users:\n",
    "            user = int(user_str)\n",
    "            if str(user) not in ground_truth:\n",
    "                continue\n",
    "\n",
    "            gt_items = set(ground_truth[str(user)])  # flat list of item_ids\n",
    "            if not gt_items:\n",
    "                continue\n",
    "\n",
    "            k = len(gt_items)\n",
    "            pred_topk = pred_items[:k]\n",
    "            hit_items = set(pred_topk) & gt_items\n",
    "\n",
    "            precision = len(hit_items) / k\n",
    "            recall = len(hit_items) / len(gt_items)\n",
    "            hit_ratio = 1.0 if len(hit_items) > 0 else 0.0\n",
    "\n",
    "            hit_item_list.append((user, k, len(hit_items)))\n",
    "\n",
    "            dcg = sum([1.0 / np.log2(i + 2) for i, item in enumerate(pred_topk) if item in gt_items])\n",
    "            idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(gt_items), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            hit_ratio_list.append(hit_ratio)\n",
    "            ndcg_list.append(ndcg)\n",
    "\n",
    "    return {\n",
    "        \"precision\": np.mean(precision_list),\n",
    "        \"recall\": np.mean(recall_list),\n",
    "        \"hit_ratio\": np.mean(hit_ratio_list),\n",
    "        \"ndcg\": np.mean(ndcg_list),\n",
    "        \"hit_details\": hit_item_list\n",
    "    }\n",
    "\n",
    "with open(PRED_P1) as f:\n",
    "    pred_dict  = json.load(f)\n",
    "print(len(pred_dict))\n",
    "# í‰ê°€\n",
    "result = evaluate_prediction(pred_dict, ground_truth, common_users)\n",
    "\n",
    "print(f\"Part1 Results:\")\n",
    "print(f\"precision: {result['precision']:.4f}, recall: {result['recall']:.4f}, \"\n",
    "        f\"hit_ratio: {result['hit_ratio']:.4f}, ndcg: {result['ndcg']:.4f}\")\n",
    "\n",
    "with open(PRED_P5) as f:\n",
    "    pred_dict  = json.load(f)\n",
    "print(len(pred_dict))\n",
    "\n",
    "# í‰ê°€\n",
    "result = evaluate_prediction(pred_dict, ground_truth, common_users)\n",
    "\n",
    "print(f\"Part5 Results:\")\n",
    "print(f\"precision: {result['precision']:.4f}, recall: {result['recall']:.4f}, \"\n",
    "        f\"hit_ratio: {result['hit_ratio']:.4f}, ndcg: {result['ndcg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c1569",
   "metadata": {},
   "source": [
    "## Popularity & Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139029d0",
   "metadata": {},
   "source": [
    "### LLMRec, A-LLMRec, Augmentation, TR CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55576f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Popularity from train_data\n",
    "# ---------------------------\n",
    "item_pop = Counter()\n",
    "for items in train_data.values():\n",
    "    for it in items:\n",
    "        item_pop[int(it)] += 1\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Load predictions\n",
    "# ---------------------------\n",
    "with open(PRED_P1) as f:\n",
    "    pred_p1 = json.load(f)\n",
    "with open(PRED_P5) as f:\n",
    "    pred_p5 = json.load(f)\n",
    "\n",
    "print(len(train_data), \"users in train_data\")\n",
    "print(len(ground_truth), \"users in ground_truth (Label)\")\n",
    "print(len(pred_p1), \"users in pred_p1\")\n",
    "print(len(pred_p5), \"users in pred_p5\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Common users\n",
    "# ---------------------------\n",
    "common_users_pred  = set(pred_p1.keys()) & set(pred_p5.keys())\n",
    "common_users_label = set(ground_truth.keys())\n",
    "\n",
    "# common_usersëŠ” (train âˆ© label)ë¡œ ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •\n",
    "common_users_final = (\n",
    "    set(map(str, common_users))\n",
    "    & common_users_pred\n",
    "    & common_users_label\n",
    ")\n",
    "\n",
    "print(\"len(common_users_final):\", len(common_users_final))\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Popularity metric\n",
    "# ---------------------------\n",
    "def avg_popularity_from_item_ids(user_items: dict[str, list[int]]):\n",
    "    \"\"\"\n",
    "    Return: dict[uid(str)] -> avg_popularity\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for uid, items in user_items.items():\n",
    "        ids = [int(x) for x in items]\n",
    "        pops = [item_pop.get(it, 0) for it in ids]\n",
    "        out[str(uid)] = float(np.mean(pops)) if pops else 0.0\n",
    "    return out\n",
    "\n",
    "# ê³µí†µ ì‚¬ìš©ìë§Œ í•„í„°ë§\n",
    "label_common = {u: ground_truth[u] for u in common_users_final}\n",
    "p1_common    = {u: pred_p1[u]     for u in common_users_final}\n",
    "p5_common    = {u: pred_p5[u]     for u in common_users_final}\n",
    "\n",
    "m_label = avg_popularity_from_item_ids(label_common)\n",
    "m_p1    = avg_popularity_from_item_ids(p1_common)\n",
    "m_p5    = avg_popularity_from_item_ids(p5_common)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Î” transitions (Popularity only)\n",
    "# ---------------------------\n",
    "d_label_p1 = []\n",
    "d_label_p5 = []\n",
    "d_p1_p5    = []\n",
    "\n",
    "for u in common_users_final:\n",
    "    lp = m_label.get(u, 0.0)\n",
    "    p1 = m_p1.get(u, 0.0)\n",
    "    p5 = m_p5.get(u, 0.0)\n",
    "\n",
    "    d_label_p1.append(p1 - lp)\n",
    "    d_label_p5.append(p5 - lp)\n",
    "    d_p1_p5.append(p5 - p1)\n",
    "\n",
    "df_delta = pd.DataFrame({\n",
    "    \"delta\": d_label_p1 + d_label_p5 + d_p1_p5,\n",
    "    \"transition\": (\n",
    "        [\"Label â†’ Part1\"] * len(d_label_p1)\n",
    "        + [\"Label â†’ Part5\"] * len(d_label_p5)\n",
    "        + [\"Part1 â†’ Part5\"] * len(d_p1_p5)\n",
    "    ),\n",
    "    \"metric\": [\"Popularity\"] * (len(d_label_p1) + len(d_label_p5) + len(d_p1_p5)),\n",
    "    \"case\": [\"Books\"] * (len(d_label_p1) + len(d_label_p5) + len(d_p1_p5)),\n",
    "})\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Plot\n",
    "# ---------------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=df_delta,\n",
    "    x=\"transition\",\n",
    "    y=\"delta\",\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "\n",
    "ax.set_title(\"Popularity Change â€” Label / Part1 / Part5 (Books)\")\n",
    "ax.set_xlabel(\"Transition\")\n",
    "ax.set_ylabel(\"Î” Average Popularity (train-based counts)\")\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary (Popularity Î”)\")\n",
    "print(df_delta.groupby(\"transition\")[\"delta\"].describe().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ML-1M: Label vs Part1 (4 baselines) Popularity Change Boxplot =====\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Paths (Part1 predictions of 4 baselines)\n",
    "# ---------------------------\n",
    "BASELINES_P1 = {\n",
    "    \"A-LLMRec\": \"A-LLMRec/books_results/predict_label_part1.json\",\n",
    "    \"LLMRec\": \"data/books/books_llmrec_format/predict_label_part1.json\",\n",
    "    \"Augmentation\": \"Augmentation/data/books/predict_label_part1.json\",\n",
    "    #\"TraditionalCF\": \"data/books/traditionalCF/predict_label_part1.json\",\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Popularity from train_data\n",
    "# ---------------------------\n",
    "def build_item_popularity(train_data: dict) -> Counter:\n",
    "    item_pop = Counter()\n",
    "    for items in train_data.values():\n",
    "        for it in items:\n",
    "            item_pop[int(it)] += 1\n",
    "    return item_pop\n",
    "\n",
    "item_pop = build_item_popularity(train_data)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Utilities\n",
    "# ---------------------------\n",
    "def load_json(path: str) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def avg_popularity_from_item_ids(user_items: dict, item_pop: Counter) -> dict[str, float]:\n",
    "    \"\"\"user_items: dict[uid]->list[item_id], return dict[str(uid)]->avg_popularity\"\"\"\n",
    "    out = {}\n",
    "    for uid, items in user_items.items():\n",
    "        if items is None:\n",
    "            out[str(uid)] = 0.0\n",
    "            continue\n",
    "        ids = [int(x) for x in items]\n",
    "        pops = [item_pop.get(it, 0) for it in ids]\n",
    "        out[str(uid)] = float(np.mean(pops)) if len(pops) > 0 else 0.0\n",
    "    return out\n",
    "\n",
    "def filter_dict_by_users(d: dict, users: set[str]) -> dict:\n",
    "    return {u: d[u] for u in users if u in d}\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Common users base\n",
    "#   - common_users: (train âˆ© gt)ë¼ê³  ê°€ì •(ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©)\n",
    "# ---------------------------\n",
    "common_users_base = set(map(str, common_users))  # uidë¥¼ strë¡œ í†µì¼\n",
    "label_users = set(map(str, ground_truth.keys()))\n",
    "common_users_base = common_users_base & label_users\n",
    "\n",
    "print(\"len(common_users_base):\", len(common_users_base))\n",
    "\n",
    "# Label popularity (ê³ ì •)\n",
    "label_common = filter_dict_by_users({str(k): v for k, v in ground_truth.items()}, common_users_base)\n",
    "m_label = avg_popularity_from_item_ids(label_common, item_pop)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) For each baseline: compute Î” (Part1 - Label)\n",
    "# ---------------------------\n",
    "rows = []\n",
    "baseline_pred = {}\n",
    "\n",
    "for bname, ppath in BASELINES_P1.items():\n",
    "    pred = load_json(ppath)\n",
    "    pred = {str(k): v for k, v in pred.items()}  # uid str í†µì¼\n",
    "    baseline_pred[bname] = pred\n",
    "\n",
    "    # ì´ baselineì—ì„œ ì‹¤ì œë¡œ predê°€ ìˆëŠ” ì‚¬ìš©ìë§Œ (Labelë„ ìˆì–´ì•¼ í•¨)\n",
    "    users_b = common_users_base & set(pred.keys())\n",
    "\n",
    "    # ì‚¬ìš©ì í•„í„°\n",
    "    pred_common = filter_dict_by_users(pred, users_b)\n",
    "\n",
    "    # popularity metric\n",
    "    m_pred = avg_popularity_from_item_ids(pred_common, item_pop)\n",
    "\n",
    "    # delta\n",
    "    for u in users_b:\n",
    "        lp = m_label.get(u, 0.0)\n",
    "        pp = m_pred.get(u, 0.0)\n",
    "        rows.append({\n",
    "            \"uid\": u,\n",
    "            \"baseline\": bname,\n",
    "            \"delta\": pp - lp,\n",
    "            \"metric\": \"Popularity\",\n",
    "            \"case\": \"ML-1M\",\n",
    "        })\n",
    "\n",
    "    print(f\"{bname}: users used = {len(users_b)}\")\n",
    "\n",
    "df_delta = pd.DataFrame(rows)\n",
    "print(\"df_delta shape:\", df_delta.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Plot (single plot comparing 4 baselines)\n",
    "# ---------------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=df_delta,\n",
    "    x=\"baseline\",\n",
    "    y=\"delta\",\n",
    "    order=list(BASELINES_P1.keys()),\n",
    ")\n",
    "\n",
    "ax.set_title(\"Popularity Change â€” GroundTruth â†’ Simulation Data (4 baselines, Books)\")\n",
    "ax.set_xlabel(\"Baseline (Part1 prediction)\")\n",
    "ax.set_ylabel(\"Î” Average Popularity (Part1 - Label, train-based counts)\")\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt ì €ì¥\n",
    "os.makedirs(\"data/books/plots\", exist_ok=True)\n",
    "plt.savefig(\"data/books/plots/popularity.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Summary\n",
    "# ---------------------------\n",
    "print(\"Summary (Î” Popularity = Part1 - Label)\")\n",
    "print(df_delta.groupby(\"baseline\")[\"delta\"].describe().round(3))\n",
    "print(\"plot is saved -> data/books/plots/popularity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Books: Part1 -> Part5 (4 baselines) Popularity Change Boxplot =====\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Paths (Part1 predictions of 4 baselines)\n",
    "# ---------------------------\n",
    "BASELINES_P1 = {\n",
    "    \"A-LLMRec\": \"A-LLMRec/books_results/predict_label_part1.json\",\n",
    "    \"LLMRec\": \"data/books/books_llmrec_format/predict_label_part1.json\",\n",
    "    \"Augmentation\": \"Augmentation/data/books/predict_label_part1.json\",\n",
    "    #\"TraditionalCF\": \"data/books/traditionalCF/predict_label_part1.json\",\n",
    "}\n",
    "\n",
    "def part1_to_part5_path(p1_path: str) -> str:\n",
    "    return p1_path.replace(\"predict_label_part1.json\", \"predict_label_part5.json\")\n",
    "\n",
    "BASELINES_P5 = {k: part1_to_part5_path(v) for k, v in BASELINES_P1.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Popularity from train_data\n",
    "# ---------------------------\n",
    "def build_item_popularity(train_data: dict) -> Counter:\n",
    "    item_pop = Counter()\n",
    "    for items in train_data.values():\n",
    "        for it in items:\n",
    "            item_pop[int(it)] += 1\n",
    "    return item_pop\n",
    "\n",
    "item_pop = build_item_popularity(train_data)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Utilities\n",
    "# ---------------------------\n",
    "def load_json(path: str) -> dict:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def avg_popularity_from_item_ids(user_items: dict, item_pop: Counter) -> dict[str, float]:\n",
    "    \"\"\"user_items: dict[uid]->list[item_id] (or int), return dict[str(uid)]->avg_popularity\"\"\"\n",
    "    out = {}\n",
    "    for uid, items in user_items.items():\n",
    "        if items is None:\n",
    "            out[str(uid)] = 0.0\n",
    "            continue\n",
    "        if isinstance(items, int):\n",
    "            items = [items]\n",
    "        ids = [int(x) for x in items]\n",
    "        pops = [item_pop.get(it, 0) for it in ids]\n",
    "        out[str(uid)] = float(np.mean(pops)) if len(pops) > 0 else 0.0\n",
    "    return out\n",
    "\n",
    "def filter_dict_by_users(d: dict, users: set[str]) -> dict:\n",
    "    return {u: d[u] for u in users if u in d}\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Common users base\n",
    "#   - common_users: ê¸°ì¡´ ì½”ë“œì—ì„œ ì“°ë˜ ê·¸ëŒ€ë¡œ ìˆë‹¤ê³  ê°€ì •\n",
    "# ---------------------------\n",
    "common_users_base = set(map(str, common_users))\n",
    "print(\"len(common_users_base):\", len(common_users_base))\n",
    "\n",
    "# ---------------------------\n",
    "# 4) For each baseline: compute Î” (Part5 - Part1)\n",
    "# ---------------------------\n",
    "rows = []\n",
    "\n",
    "for bname in BASELINES_P1.keys():\n",
    "    p1_path = BASELINES_P1[bname]\n",
    "    p5_path = BASELINES_P5[bname]\n",
    "\n",
    "    pred_p1 = load_json(p1_path)\n",
    "    pred_p5 = load_json(p5_path)\n",
    "\n",
    "    pred_p1 = {str(k): v for k, v in pred_p1.items()}\n",
    "    pred_p5 = {str(k): v for k, v in pred_p5.items()}\n",
    "\n",
    "    # ì´ baselineì—ì„œ Part1 & Part5 ë‘˜ ë‹¤ ìˆëŠ” ì‚¬ìš©ìë§Œ + common_users\n",
    "    users_b = common_users_base & set(pred_p1.keys()) & set(pred_p5.keys())\n",
    "\n",
    "    # ì‚¬ìš©ì í•„í„°\n",
    "    p1_common = filter_dict_by_users(pred_p1, users_b)\n",
    "    p5_common = filter_dict_by_users(pred_p5, users_b)\n",
    "\n",
    "    # popularity metric\n",
    "    m_p1 = avg_popularity_from_item_ids(p1_common, item_pop)\n",
    "    m_p5 = avg_popularity_from_item_ids(p5_common, item_pop)\n",
    "\n",
    "    # delta\n",
    "    for u in users_b:\n",
    "        p1v = m_p1.get(u, 0.0)\n",
    "        p5v = m_p5.get(u, 0.0)\n",
    "        rows.append({\n",
    "            \"uid\": u,\n",
    "            \"baseline\": bname,\n",
    "            \"delta\": p5v - p1v,\n",
    "            \"metric\": \"Popularity\",\n",
    "            \"case\": \"Books\",\n",
    "        })\n",
    "\n",
    "    print(f\"{bname}: users used = {len(users_b)}\")\n",
    "    print(f\"  - p1: {p1_path}\")\n",
    "    print(f\"  - p5: {p5_path}\")\n",
    "\n",
    "df_delta = pd.DataFrame(rows)\n",
    "print(\"df_delta shape:\", df_delta.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Plot (single plot comparing 4 baselines)\n",
    "# ---------------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=df_delta,\n",
    "    x=\"baseline\",\n",
    "    y=\"delta\",\n",
    "    order=list(BASELINES_P1.keys()),\n",
    ")\n",
    "\n",
    "ax.set_title(\"Popularity Change â€” Recommendation â†’ Feedback Loop (3 baselines, Books)\")\n",
    "ax.set_xlabel(\"Baseline\")\n",
    "ax.set_ylabel(\"Î” Average Popularity (Part5 - Part1, train-based counts)\")\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(\"data/books/plots\", exist_ok=True)\n",
    "out_path = \"data/books/plots/popularity_p1_to_p5.png\"\n",
    "plt.savefig(out_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Summary\n",
    "# ---------------------------\n",
    "print(\"Summary (Î” Popularity = Part5 - Part1)\")\n",
    "print(df_delta.groupby(\"baseline\")[\"delta\"].describe().round(3))\n",
    "print(\"plot is saved ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f906f857",
   "metadata": {},
   "source": [
    "### Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb865a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import networkx as nx\n",
    "\n",
    "# Louvain\n",
    "import community as community_louvain  # pip install python-louvain\n",
    "\n",
    "# =========================\n",
    "# ê²½ë¡œ/ì„¤ì •\n",
    "# =========================\n",
    "STEPS    = list(range(PARTS))\n",
    "OUT_DIR  = os.path.join(BASE_DIR, \"figs_tsne_cluster_progress\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TSNE_PERPLEXITY      = 30\n",
    "TSNE_RANDOM_STATE    = 42\n",
    "KMEANS_RANDOM_STATE  = 42\n",
    "K_USERS = 2   # placeholder; Louvain ê²°ê³¼ë¡œ ê°±ì‹ \n",
    "K_ITEMS = 2\n",
    "\n",
    "SUBSAMPLE_USERS_FOR_TSNE = None\n",
    "SUBSAMPLE_ITEMS_FOR_TSNE = None\n",
    "\n",
    "# ===== ì»¤ë®¤ë‹ˆí‹° íƒì§€(ê·¸ë˜í”„ êµ¬ì„±) ì„¤ì • =====\n",
    "USE_COSINE       = True        # Louvainìš© ê°€ì¤‘ì¹˜ë¡œ cosine ìœ ì‚¬ë„ ì‚¬ìš©\n",
    "SIM_THRESHOLD    = 0.5         # ê°„ì„  ìƒì„± ì„ê³„ê°’ (cosine sim >= threshold)\n",
    "MAX_EDGES_PER_NODE = None      # ë…¸ë“œë‹¹ ìµœëŒ€ ê°„ì„  ìˆ˜ ì œí•œ(ì˜ˆ: 50). Noneì´ë©´ ì œí•œ ì—†ìŒ.\n",
    "EDGE_WEIGHT_NAME = \"w\"         # ê°€ì¤‘ì¹˜ ì†ì„±ëª…\n",
    "\n",
    "# =========================\n",
    "# ìœ í‹¸\n",
    "# =========================\n",
    "\n",
    "def _get_items(d, uid):\n",
    "    \"\"\"\n",
    "    d: {user_id: [items,...]} with user_id possibly str or int\n",
    "    uid: int\n",
    "    returns list of items (possibly empty)\n",
    "    \"\"\"\n",
    "    if uid in d:\n",
    "        return d[uid]\n",
    "    suid = str(uid)\n",
    "    if suid in d:\n",
    "        return d[suid]\n",
    "    return []\n",
    "\n",
    "def build_user_item_graph_common(train_data, pred_dict, common_users,\n",
    "                                 alpha: float = 0.3,\n",
    "                                 drop_isolates: bool = True,\n",
    "                                 min_degree: int | None = None) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    train_data + pred_dictë¡œ user-item ì´ë¶„ ê·¸ë˜í”„ ìƒì„±í•˜ë˜,\n",
    "    userëŠ” ì˜¤ì§ common_usersì— í•œì •.\n",
    "\n",
    "    - alpha: ì˜ˆì¸¡ ì—£ì§€ ê°€ì¤‘ì¹˜\n",
    "    - drop_isolates: ì°¨ìˆ˜ 0ì¸ ë…¸ë“œ ì œê±°\n",
    "    - min_degree: ì§€ì • ì‹œ, ì°¨ìˆ˜ < min_degree ë…¸ë“œ ì œê±°(ë°˜ë³µ 1íšŒ)\n",
    "\n",
    "    return: networkx.Graph (ë¬´ë°©í–¥, ê°„ì„  ì†ì„± 'w'ì— ê°€ì¤‘ì¹˜ ì €ì¥)\n",
    "    \"\"\"\n",
    "    # 1) ì—ì§€ ê°€ì¤‘ì¹˜ ëˆ„ì (ë©”ëª¨ë¦¬ ì ˆì•½ + í•©ì‚° ì •í™•ì„±)\n",
    "    ew = defaultdict(float)  # key=(u_node, i_node) -> weight\n",
    "\n",
    "    # 2) train ì—£ì§€\n",
    "    for u in common_users:\n",
    "        u = int(u)\n",
    "        items = _get_items(train_data, u)\n",
    "        for it in items:\n",
    "            ew[(f\"U{u}\", f\"I{int(it)}\")] += 1.0\n",
    "\n",
    "    # 3) predict ì—£ì§€\n",
    "    for u in common_users:\n",
    "        u = int(u)\n",
    "        items = _get_items(pred_dict, u)\n",
    "        for it in items:\n",
    "            ew[(f\"U{u}\", f\"I{int(it)}\")] += alpha\n",
    "\n",
    "    # 4) ê·¸ë˜í”„ êµ¬ì„±\n",
    "    G = nx.Graph()\n",
    "    # add_edges_from with attributes\n",
    "    G.add_nodes_from([f\"U{u}\" for u in common_users])  # ìœ ì € ë…¸ë“œëŠ” ë¯¸ë¦¬ ì¶”ê°€(ê³ ì • ì§‘í•©)\n",
    "    G.add_edges_from([(u, i, {\"w\": w}) for (u, i), w in ew.items() if w > 0])\n",
    "\n",
    "    # 5) ê³ ë¦½/ì €ì°¨ìˆ˜ ë…¸ë“œ ì •ë¦¬(ì˜µì…˜)\n",
    "    if drop_isolates:\n",
    "        iso = list(nx.isolates(G))\n",
    "        if iso:\n",
    "            G.remove_nodes_from(iso)\n",
    "    if min_degree is not None and min_degree > 0:\n",
    "        low = [n for n, d in dict(G.degree()).items() if d < min_degree]\n",
    "        if low:\n",
    "            G.remove_nodes_from(low)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "def louvain_detect(G: nx.Graph, weight_name: str = \"w\"):\n",
    "    \"\"\"\n",
    "    Louvain ì»¤ë®¤ë‹ˆí‹° íƒì§€ (ì„ì˜ ë…¸ë“œëª… ì§€ì›)\n",
    "    return:\n",
    "      node_order: List[hashable]        # ë¼ë²¨ ë°°ì—´ê³¼ 1:1ë¡œ ë§¤í•‘ë˜ëŠ” ë…¸ë“œ ìˆœì„œ\n",
    "      labels_arr: np.ndarray[int]       # node_orderì™€ ê°™ì€ ìˆœì„œì˜ ì»¤ë®¤ë‹ˆí‹° ë¼ë²¨(0..C-1)\n",
    "      num_comms: int\n",
    "      part: Dict[node, community_id]    # ì›ë³¸ ë§¤í•‘(ë„¤ê°€ ì§ì ‘ ì¡°íšŒí•  ë•Œ í¸í•¨)\n",
    "    \"\"\"\n",
    "    part = community_louvain.best_partition(G, weight=weight_name, random_state=42)\n",
    "    # ë¼ë²¨ì„ 0..C-1ë¡œ ì••ì¶•\n",
    "    uniq = {}\n",
    "    cur = 0\n",
    "    for n, c in part.items():\n",
    "        if c not in uniq:\n",
    "            uniq[c] = cur\n",
    "            cur += 1\n",
    "        part[n] = uniq[c]\n",
    "    node_order = list(G.nodes())\n",
    "    labels_arr = np.array([part[n] for n in node_order], dtype=int)\n",
    "    return node_order, labels_arr, cur, part\n",
    "\n",
    "\n",
    "def compute_centers(coords: np.ndarray, labels: np.ndarray, K: int):\n",
    "    centers = {}\n",
    "    for k in range(K):\n",
    "        mask = (labels == k)\n",
    "        if np.any(mask):\n",
    "            centers[k] = coords[mask].mean(axis=0)\n",
    "    return centers\n",
    "# =========================\n",
    "print(\"=== Part1 Louvain ===\")\n",
    "# G_users = build_user_item_graph(train_data, pred_p1, alpha=1.0)\n",
    "G_users = build_user_item_graph_common(\n",
    "    train_data=train_data,\n",
    "    pred_dict=pred_p1,\n",
    "    common_users=common_users,\n",
    "    alpha=1.0,          # ì˜ˆì¸¡ ì—£ì§€ ì˜í–¥ ê°•í™”\n",
    "    drop_isolates=True,\n",
    "    min_degree=None\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Graph nodes={G_users.number_of_nodes()}, edges={G_users.number_of_edges()}\")\n",
    "\n",
    "# ---- (ìˆ˜ì •) Louvain í˜¸ì¶œ ë° ê²°ê³¼ ìš”ì•½ ----\n",
    "node_order, node_labels_final, K_USERS, part = louvain_detect(G_users, weight_name=EDGE_WEIGHT_NAME)\n",
    "print(f\"Diversity Score(Detected communities (total nodes) based): {1/K_USERS}\")\n",
    "\n",
    "# ì»¤ë®¤ë‹ˆí‹° í¬ê¸° ì¶œë ¥\n",
    "_, counts = np.unique(node_labels_final, return_counts=True)\n",
    "print(\"Community sizes (all nodes U+I):\", K_USERS, counts.tolist())\n",
    "\n",
    "\n",
    "# =========================\n",
    "print(\"\\n=== Part5 Louvain ===\")\n",
    "#G_users = build_user_item_graph(train_data, pred_p5, alpha=1.0)\n",
    "G_users = build_user_item_graph_common(\n",
    "    train_data=train_data,\n",
    "    pred_dict=pred_p5,\n",
    "    common_users=common_users,\n",
    "    alpha=1.0,          # ì˜ˆì¸¡ ì—£ì§€ ì˜í–¥ ê°•í™”\n",
    "    drop_isolates=True,\n",
    "    min_degree=None\n",
    ")\n",
    "\n",
    "print(f\"Graph nodes={G_users.number_of_nodes()}, edges={G_users.number_of_edges()}\")\n",
    "\n",
    "# ---- (ìˆ˜ì •) Louvain í˜¸ì¶œ ë° ê²°ê³¼ ìš”ì•½ ----\n",
    "node_order, node_labels_final, K_USERS, part = louvain_detect(G_users, weight_name=EDGE_WEIGHT_NAME)\n",
    "print(f\"Diversity Score(Detected communities (total nodes) based): {1/K_USERS}\")\n",
    "\n",
    "# ì»¤ë®¤ë‹ˆí‹° í¬ê¸° ì¶œë ¥\n",
    "_, counts = np.unique(node_labels_final, return_counts=True)\n",
    "print(\"Community sizes (all nodes U+I):\", K_USERS, counts.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc78ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------------\n",
    "# 1) ì•„ì´í…œ ì„ë² ë”© ë¡œë“œ\n",
    "# ---------------------------\n",
    "def load_item_embeddings(base_dir: str, parts: int, step: int):\n",
    "    \"\"\"\n",
    "    ipath = os.path.join(base_dir, f\"item_emb_part{parts}_step{step}.npy\")\n",
    "    ë¥¼ ë¡œë“œí•´ì„œ (num_items, d) ë°°ì—´ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    ipath = os.path.join(base_dir, f\"item_emb_part{parts}_step{step}.npy\")\n",
    "    if not os.path.exists(ipath):\n",
    "        raise FileNotFoundError(ipath)\n",
    "    I = np.load(ipath)\n",
    "    if I.ndim != 2:\n",
    "        raise ValueError(f\"item_emb shape invalid: {I.shape}\")\n",
    "    return I\n",
    "\n",
    "# ---------------------------\n",
    "# 2) í—¬í¼: pred_dictì—ì„œ ìœ ì € í‚¤ ì•ˆì „ ì¡°íšŒ\n",
    "# ---------------------------\n",
    "def _get_user_items(pred_dict, uid):\n",
    "    if uid in pred_dict:\n",
    "        return pred_dict[uid]\n",
    "    suid = str(uid)\n",
    "    if suid in pred_dict:\n",
    "        return pred_dict[suid]\n",
    "    return []\n",
    "\n",
    "# ---------------------------\n",
    "# 3) ì‚¬ìš©ìë³„ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "# ---------------------------\n",
    "def avg_pairwise_cos_sim_for_user(item_ids, item_emb_norm):\n",
    "    \"\"\"\n",
    "    item_ids: ì¶”ì²œëœ item id ë¦¬ìŠ¤íŠ¸(ì •ìˆ˜ or ë¬¸ìì—´)\n",
    "    item_emb_norm: (N, d) ì •ê·œí™”ëœ ì„ë² ë”© (ê° í–‰ L2=1)\n",
    "    return: í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ìƒì‚¼ê° í‰ê· ). ìœ íš¨ ì•„ì´í…œ <2ì´ë©´ np.nan\n",
    "    \"\"\"\n",
    "    # ì •ìˆ˜ ìºìŠ¤íŒ… + ì¤‘ë³µ ì œê±°(ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ë‚´ ì¤‘ë³µ ì•„ì´í…œì´ í‰ê· ì„ ì™œê³¡í•˜ì§€ ì•Šë„ë¡)\n",
    "    try:\n",
    "        idx = [int(x) for x in item_ids]\n",
    "    except Exception:\n",
    "        idx = []\n",
    "    # ìœ íš¨ ë²”ìœ„ë§Œ ìœ ì§€\n",
    "    N = item_emb_norm.shape[0]\n",
    "    idx = [i for i in dict.fromkeys(idx) if 0 <= i < N]  # dict.fromkeys: ìˆœì„œ ë³´ì¡´ ì¤‘ë³µ ì œê±°\n",
    "\n",
    "    m = len(idx)\n",
    "    if m < 2:\n",
    "        return np.nan\n",
    "\n",
    "    V = item_emb_norm[idx]            # (m, d) ì´ë¯¸ ì •ê·œí™”ë¨\n",
    "    S = V @ V.T                       # (m, m) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ í–‰ë ¬\n",
    "    # ìƒì‚¼ê°(diag ì œì™¸) í‰ê· \n",
    "    triu_idx = np.triu_indices(m, k=1)\n",
    "    sims = S[triu_idx]\n",
    "    return float(np.mean(sims)) if sims.size > 0 else np.nan\n",
    "\n",
    "# ---------------------------\n",
    "# 4) ë©”ì¸: ì‚¬ìš©ìë³„/ì „ì²´ í‰ê·  ê³„ì‚°\n",
    "# ---------------------------\n",
    "def compute_rec_list_avg_cos_sim(\n",
    "    pred_dict: dict,\n",
    "    base_dir: str,\n",
    "    parts: int,\n",
    "    step: int,\n",
    "    users: list | set | None = None,   # Noneì´ë©´ pred_dictì˜ ëª¨ë“  ìœ ì €\n",
    "    clip01: bool = True,               # ìˆ˜ì¹˜ ì˜¤ì°¨ ë³´ì •ìš© [0,1] í´ë¦¬í•‘\n",
    "):\n",
    "    \"\"\"\n",
    "    ë°˜í™˜:\n",
    "      user2avg_sim: {user_id(int): í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„(ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘)}\n",
    "      global_mean: ì „ì²´ ì‚¬ìš©ì í‰ê· (na ì œì™¸)\n",
    "      user2diversity: {user_id: 1 - avg_sim}  # ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘\n",
    "      global_div_mean: ì „ì²´ ì‚¬ìš©ì diversity í‰ê· \n",
    "    \"\"\"\n",
    "    # 1) ì„ë² ë”© ë¡œë“œ & ì •ê·œí™”\n",
    "    I = load_item_embeddings(base_dir, parts, step)       # (num_items, d)\n",
    "    norms = np.linalg.norm(I, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0\n",
    "    In = I / norms\n",
    "\n",
    "    # 2) ëŒ€ìƒ ì‚¬ìš©ì ì§‘í•©\n",
    "    if users is None:\n",
    "        # pred_dict í‚¤ê°€ str/int ì„ì—¬ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª¨ë‘ int ìºìŠ¤íŒ… ì‹œë„\n",
    "        cand = []\n",
    "        for k in pred_dict.keys():\n",
    "            try:\n",
    "                cand.append(int(k))\n",
    "            except Exception:\n",
    "                # intë¡œ ì•ˆ ë°”ë€ŒëŠ” í‚¤ëŠ” ìŠ¤í‚µ\n",
    "                pass\n",
    "        users = cand\n",
    "    else:\n",
    "        users = [int(u) for u in users]\n",
    "\n",
    "    user2avg_sim = {}\n",
    "    user2diversity = {}\n",
    "\n",
    "    for u in tqdm(users, total=len(users)):\n",
    "        items = _get_user_items(pred_dict, u)\n",
    "        avg_sim = avg_pairwise_cos_sim_for_user(items, In)\n",
    "        if np.isnan(avg_sim):\n",
    "            user2avg_sim[u] = np.nan\n",
    "            user2diversity[u] = np.nan\n",
    "        else:\n",
    "            if clip01:\n",
    "                avg_sim = max(0.0, min(1.0, avg_sim))\n",
    "            user2avg_sim[u] = avg_sim\n",
    "            user2diversity[u] = 1.0 - avg_sim  # ì ìˆ˜ â†‘ = ë” ë‹¤ì–‘\n",
    "\n",
    "    # 3) ì „ì²´ í‰ê· (na ì œì™¸)\n",
    "    vals = np.array([v for v in user2avg_sim.values() if not np.isnan(v)], dtype=float)\n",
    "    global_mean = float(vals.mean()) if vals.size > 0 else np.nan\n",
    "\n",
    "    div_vals = np.array([v for v in user2diversity.values() if not np.isnan(v)], dtype=float)\n",
    "    global_div_mean = float(div_vals.mean()) if div_vals.size > 0 else np.nan\n",
    "\n",
    "    return user2avg_sim, global_mean, user2diversity, global_div_mean\n",
    "\n",
    "# pred_dict: predict_label.jsonì„ ë¡œë“œí•œ dict (user_id -> [item_ids...])\n",
    "# base_dir, parts, step: ë„¤ ì‹¤í—˜ ê²½ë¡œ/ìŠ¤í…\n",
    "\n",
    "user2sim, mean_sim, user2div, mean_div = compute_rec_list_avg_cos_sim(\n",
    "    pred_dict=pred_p1,\n",
    "    base_dir=BASE_DIR,\n",
    "    parts=1,\n",
    "    step=0,      # í˜¹ì€ í‰ê°€í•˜ê³  ì‹¶ì€ step\n",
    "    users=common_users,             # ë˜ëŠ” common_usersë¡œ í•œì •í•˜ë ¤ë©´ users=common_users\n",
    "    clip01=True\n",
    ")\n",
    "\n",
    "print(f\"[Avg CosSim] ì „ì²´ ì‚¬ìš©ì í‰ê· : {mean_sim:.6f} (ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘)\")\n",
    "print(f\"[Diversity]  ì „ì²´ ì‚¬ìš©ì í‰ê· : {mean_div:.6f} (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘)\")\n",
    "\n",
    "\n",
    "user2sim, mean_sim, user2div, mean_div = compute_rec_list_avg_cos_sim(\n",
    "    pred_dict=pred_p5,\n",
    "    base_dir=BASE_DIR,\n",
    "    parts=5,\n",
    "    step=4,      # í˜¹ì€ í‰ê°€í•˜ê³  ì‹¶ì€ step\n",
    "    users=common_users,             # ë˜ëŠ” common_usersë¡œ í•œì •í•˜ë ¤ë©´ users=common_users\n",
    "    clip01=True\n",
    ")\n",
    "\n",
    "print(f\"[Avg CosSim] ì „ì²´ ì‚¬ìš©ì í‰ê· : {mean_sim:.6f} (ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘)\")\n",
    "print(f\"[Diversity]  ì „ì²´ ì‚¬ìš©ì í‰ê· : {mean_div:.6f} (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘)\")\n",
    "\n",
    "# íŠ¹ì • ì‚¬ìš©ì uì˜ ì ìˆ˜ í™•ì¸\n",
    "# print(user2sim[u], user2div[u])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3353dc",
   "metadata": {},
   "source": [
    "# RQ4\n",
    "í”¼ë“œë°± ë£¨í”„ë¡œ ìƒì„±ëœ ì„ë² ë”©ìœ¼ë¡œ ë¶„í„° ë°œìƒí•  ìˆ˜ ìˆëŠ” ìœ„í—˜ì„± ë¶„ì„(ì–‘ê·¹í™”, í•„í„°ë²„ë¸” ë“±)\n",
    "\n",
    "í¬í•¨: LLMRec, A-LLMRec, Augmentation, TR_CF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f224d5",
   "metadata": {},
   "source": [
    "## Polarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d87cda",
   "metadata": {},
   "source": [
    "### 5step í•œë²ˆì—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc18e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# =========================\n",
    "# ê²½ë¡œ/ì„¤ì •\n",
    "# =========================\n",
    "STEPS    = list(range(PARTS))\n",
    "CLUSTER_STEP = STEPS[-1]              # ìµœì¢… stepì—ì„œ KMeans ìˆ˜í–‰\n",
    "OUT_DIR  = os.path.join(BASE_DIR, \"figs_tsne_cluster_progress\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TSNE_PERPLEXITY      = 30\n",
    "TSNE_RANDOM_STATE    = 42\n",
    "KMEANS_RANDOM_STATE  = 42\n",
    "K_USERS = 2\n",
    "K_ITEMS = 2\n",
    "\n",
    "# (ì„ íƒ) t-SNE ê°€ì† ìƒ˜í”Œë§\n",
    "SUBSAMPLE_USERS_FOR_TSNE = None  # ì˜ˆ: 5000\n",
    "SUBSAMPLE_ITEMS_FOR_TSNE = None  # ì˜ˆ: 5000\n",
    "\n",
    "# =========================\n",
    "# ìœ í‹¸\n",
    "# =========================\n",
    "def load_users_items_for_step(base_dir: str, parts: int, step: int):\n",
    "    upath = os.path.join(base_dir, f\"user_emb_part{parts}_step{step}.npy\")\n",
    "    ipath = os.path.join(base_dir, f\"item_emb_part{parts}_step{step}.npy\")\n",
    "    if not os.path.exists(upath):\n",
    "        raise FileNotFoundError(upath)\n",
    "    U = np.load(upath)  # shape: (num_users_step, d)\n",
    "    I = np.load(ipath) if os.path.exists(ipath) else None  # shape: (num_items_step, d) or None\n",
    "    if U.ndim != 2:\n",
    "        raise ValueError(f\"user_emb shape invalid at step{step}: {U.shape}\")\n",
    "    if I is not None and I.ndim != 2:\n",
    "        raise ValueError(f\"item_emb shape invalid at step{step}: {I.shape}\")\n",
    "    return U, I\n",
    "\n",
    "\n",
    "def align_users_for_step(common_users, num_users_step: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    common_users: set/list/ndarray ëª¨ë‘ í—ˆìš©.\n",
    "    num_users_step: ì´ stepì˜ user ì¶• ê¸¸ì´ (ex. user_by_step[s].shape[0])\n",
    "    return: [N_u'] ì •ë ¬ëœ ìœ íš¨ ìœ ì € id ë°°ì—´\n",
    "    \"\"\"\n",
    "    # 1) ì…ë ¥ì„ ë„˜íŒŒì´ int64 ë°°ì—´ë¡œ í†µì¼\n",
    "    if isinstance(common_users, (set, list, tuple)):\n",
    "        cu = np.asarray(sorted(common_users), dtype=np.int64)  # ì¬í˜„ì„± ìœ„í•´ ì •ë ¬ ê¶Œì¥\n",
    "    else:\n",
    "        cu = np.asarray(common_users, dtype=np.int64)\n",
    "\n",
    "    if cu.size == 0:\n",
    "        return cu\n",
    "\n",
    "    # 2) ìœ íš¨ ë²”ìœ„ ë§ˆìŠ¤í‚¹\n",
    "    mask = (cu >= 0) & (cu < num_users_step)\n",
    "    return cu[mask]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) ê° step ì„ë² ë”© ë¡œë“œ\n",
    "# =========================\n",
    "print(\"Loading augmentation embeddings per step...\")\n",
    "user_by_step = {}\n",
    "item_by_step = {}\n",
    "dims = set()\n",
    "\n",
    "for s in STEPS:\n",
    "    U, I = load_users_items_for_step(BASE_DIR, PARTS, s)\n",
    "    user_by_step[s] = U\n",
    "    item_by_step[s] = I\n",
    "    dims.add(U.shape[1])\n",
    "    if I is not None:\n",
    "        dims.add(I.shape[1])\n",
    "    print(f\"  - step{s}: users={U.shape}, items={None if I is None else I.shape}\")\n",
    "\n",
    "if len(dims) != 1:\n",
    "    raise RuntimeError(f\"ì„ë² ë”© ì°¨ì›ì´ stepë³„ë¡œ ë‹¤ë¦…ë‹ˆë‹¤: {dims}\")\n",
    "D = dims.pop()\n",
    "\n",
    "# =========================\n",
    "# 2) trainâˆ©label ê³µí†µ ìœ ì € ì‚°ì¶œ + stepë³„ ì •ë ¬\n",
    "# =========================\n",
    "print(f\"common users: {len(common_users)}\")\n",
    "\n",
    "aligned_users_by_step = {s: align_users_for_step(common_users, user_by_step[s].shape[0])\n",
    "                         for s in STEPS}\n",
    "print({s: len(aligned_users_by_step[s]) for s in STEPS})\n",
    "\n",
    "# =========================\n",
    "# 3) ì•„ì´í…œ ê³µí†µ ë¶€ë¶„ ì •ë ¬(ìµœì†Œ ê¸¸ì´)\n",
    "# =========================\n",
    "item_lengths = [item_by_step[s].shape[0] for s in STEPS if item_by_step[s] is not None]\n",
    "if len(item_lengths) == 0:\n",
    "    I_base = 0\n",
    "else:\n",
    "    I_base = min(item_lengths)\n",
    "    uniq = set(item_lengths)\n",
    "    if len(uniq) != 1:\n",
    "        print(f\"[warn] stepë³„ ì•„ì´í…œ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤: {uniq} â†’ ì•ì—ì„œ {I_base}ê°œë§Œ ê³µí†µ ì‚¬ìš©\")\n",
    "    for s in STEPS:\n",
    "        I = item_by_step[s]\n",
    "        item_by_step[s] = None if I is None or I.shape[0] < I_base else I[:I_base]\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "if I_base > 0:\n",
    "    if SUBSAMPLE_ITEMS_FOR_TSNE is not None and SUBSAMPLE_ITEMS_FOR_TSNE < I_base:\n",
    "        keep_items = np.sort(rng.choice(I_base, size=SUBSAMPLE_ITEMS_FOR_TSNE, replace=False))\n",
    "    else:\n",
    "        keep_items = np.arange(I_base)\n",
    "else:\n",
    "    keep_items = np.array([], dtype=int)\n",
    "\n",
    "# =========================\n",
    "# 4) ìµœì¢… stepì—ì„œ KMeans â†’ ë ˆì´ë¸” ìƒì„±\n",
    "# =========================\n",
    "print(f\"KMeans on final step: step{CLUSTER_STEP}\")\n",
    "U_final = user_by_step[CLUSTER_STEP]\n",
    "ids_final = aligned_users_by_step[CLUSTER_STEP]\n",
    "if len(ids_final) < K_USERS:\n",
    "    raise RuntimeError(f\"ìµœì¢… stepì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ê°€ëŠ¥í•œ ê³µí†µ ìœ ì €ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(ids_final)}\")\n",
    "\n",
    "emb_final_users = U_final[ids_final, :]  # (|ids_final|, D)\n",
    "user_kmeans = KMeans(n_clusters=K_USERS, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "user_labels_final = user_kmeans.fit_predict(emb_final_users)\n",
    "user_center_idx = {k: user_labels_final == k for k in range(K_USERS)}\n",
    "\n",
    "# ì•„ì´í…œ KMeans (ìµœì¢… step ê³µí†µ ì•„ì´í…œ)\n",
    "if item_by_step[CLUSTER_STEP] is not None and len(keep_items) > 0:\n",
    "    I_final = item_by_step[CLUSTER_STEP][keep_items]  # (|keep_items|, D)\n",
    "    item_kmeans = KMeans(n_clusters=K_ITEMS, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "    item_labels_final = item_kmeans.fit_predict(I_final)\n",
    "else:\n",
    "    item_kmeans = None\n",
    "    item_labels_final = None\n",
    "    print(\"[warn] ìµœì¢… step ì•„ì´í…œì´ ì—†ì–´ item clustering ìƒëµ\")\n",
    "\n",
    "# =========================\n",
    "# 5) t-SNEì— ë„£ì„ ë°ì´í„° ë¸”ë¡ êµ¬ì„± (users + items across steps)\n",
    "# =========================\n",
    "print(\"Preparing t-SNE blocks...\")\n",
    "ids_for_tsne = {}      # step -> user_id ë°°ì—´(ìƒ˜í”Œë§ ë°˜ì˜)\n",
    "users_for_tsne = {}    # step -> user_emb ë°°ì—´\n",
    "item_for_tsne  = {}    # step -> item_emb ë°°ì—´ (ê³µí†µ keep_itemsì— í•´ë‹¹)\n",
    "\n",
    "# ìœ ì € ìƒ˜í”Œë§ ê¸°ì¤€: ìµœì¢… stepì˜ ids_final ì§‘í•©\n",
    "if SUBSAMPLE_USERS_FOR_TSNE is not None and SUBSAMPLE_USERS_FOR_TSNE < len(ids_final):\n",
    "    sample_users = np.sort(rng.choice(ids_final, size=SUBSAMPLE_USERS_FOR_TSNE, replace=False))\n",
    "else:\n",
    "    sample_users = ids_final\n",
    "\n",
    "for s in STEPS:\n",
    "    ids_s = aligned_users_by_step[s]\n",
    "    if len(ids_s) == 0:\n",
    "        ids_for_tsne[s] = np.array([], dtype=np.int64)\n",
    "        users_for_tsne[s] = np.empty((0, D), dtype=np.float32)\n",
    "    else:\n",
    "        mask = np.isin(ids_s, sample_users)\n",
    "        ids_for_tsne[s] = ids_s[mask]\n",
    "        users_for_tsne[s] = user_by_step[s][ids_for_tsne[s], :]\n",
    "\n",
    "    I = item_by_step[s]\n",
    "    item_for_tsne[s] = None if I is None or len(keep_items) == 0 else I[keep_items]\n",
    "\n",
    "# =========================\n",
    "# 6) t-SNE ìˆ˜í–‰ (ëª¨ë“  stepì˜ ìœ ì €/ì•„ì´í…œ í•©ì³ì„œ)\n",
    "# =========================\n",
    "print(\"Fitting joint t-SNE on users + items across steps\")\n",
    "blocks = []\n",
    "slices = {}  # step -> {\"user\": (st,en), \"item\": (st,en) or None}\n",
    "cursor = 0\n",
    "\n",
    "for s in STEPS:\n",
    "    Ublk = users_for_tsne[s]\n",
    "    blocks.append(Ublk)\n",
    "    u_st, u_en = cursor, cursor + len(Ublk)\n",
    "    cursor = u_en\n",
    "\n",
    "    Iblk = item_for_tsne[s]\n",
    "    if Iblk is not None and len(Iblk) > 0:\n",
    "        blocks.append(Iblk)\n",
    "        i_st, i_en = cursor, cursor + len(Iblk)\n",
    "        cursor = i_en\n",
    "        slices[s] = {\"user\": (u_st, u_en), \"item\": (i_st, i_en)}\n",
    "    else:\n",
    "        slices[s] = {\"user\": (u_st, u_en), \"item\": None}\n",
    "\n",
    "if len(blocks) == 0 or sum(len(b) for b in blocks) < 3:\n",
    "    raise RuntimeError(\"t-SNEì— ì‚¬ìš©í•  í‘œë³¸ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "X_all = np.vstack(blocks)\n",
    "perp = min(TSNE_PERPLEXITY, max(5, (len(X_all) - 1)//3))\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=perp,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    "    random_state=TSNE_RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    verbose=1\n",
    ")\n",
    "X_tsne = tsne.fit_transform(X_all)\n",
    "\n",
    "# ì¢Œí‘œ ë²”ìœ„(ê³µí†µ ì¶•)\n",
    "xmin, ymin = X_tsne.min(axis=0)\n",
    "xmax, ymax = X_tsne.max(axis=0)\n",
    "xpad = 0.05 * (xmax - xmin) if xmax > xmin else 0.5\n",
    "ypad = 0.05 * (ymax - ymin) if ymax > ymin else 0.5\n",
    "\n",
    "# ìµœì¢… step ì¤‘ì‹¬ ì¢Œí‘œ(ì‚¬ìš©ì/ì•„ì´í…œ)\n",
    "palette = [\"tab:blue\",\"tab:orange\",\"tab:green\",\"tab:red\",\"tab:purple\",\n",
    "           \"tab:brown\",\"tab:pink\",\"tab:gray\",\"tab:olive\",\"tab:cyan\"]\n",
    "\n",
    "def compute_centers(coords: np.ndarray, labels: np.ndarray, K: int):\n",
    "    centers = {}\n",
    "    for k in range(K):\n",
    "        mask = (labels == k)\n",
    "        if np.any(mask):\n",
    "            centers[k] = coords[mask].mean(axis=0)\n",
    "    return centers\n",
    "\n",
    "# ìµœì¢… step ì‚¬ìš©ì/ì•„ì´í…œ ì¢Œí‘œ ìŠ¬ë¼ì´ìŠ¤\n",
    "u_st_f, u_en_f = slices[CLUSTER_STEP][\"user\"]\n",
    "coords_u_final = X_tsne[u_st_f:u_en_f]\n",
    "# user_labels_finalëŠ” ids_final(=ìƒ˜í”Œë§ ì „ ìµœì¢… step ê³µí†µìœ ì €)ì— ëŒ€í•œ ë ˆì´ë¸”\n",
    "# t-SNEì—ëŠ” sample_usersë§Œ í¬í•¨ëì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ids_for_tsne[CLUSTER_STEP] ìˆœì„œì— ë§ì¶° ë ˆì´ë¸” ì¬ì •ë ¬\n",
    "ids_tsne_final = ids_for_tsne[CLUSTER_STEP]\n",
    "# ids_tsne_finalì˜ ê° user_idê°€ ids_final ë‚´ì—ì„œì˜ indexë¥¼ ì°¾ì•„ ë§¤í•‘\n",
    "idx_map_final = {int(u): i for i, u in enumerate(ids_final.tolist())}\n",
    "labels_u_for_plot = np.array([user_labels_final[idx_map_final[int(u)]] for u in ids_tsne_final], dtype=int)\n",
    "user_centers = compute_centers(coords_u_final, labels_u_for_plot, K_USERS)\n",
    "\n",
    "if slices[CLUSTER_STEP][\"item\"] is not None and item_labels_final is not None:\n",
    "    i_st_f, i_en_f = slices[CLUSTER_STEP][\"item\"]\n",
    "    coords_i_final = X_tsne[i_st_f:i_en_f]\n",
    "    item_centers = compute_centers(coords_i_final, item_labels_final, K_ITEMS)\n",
    "else:\n",
    "    item_centers = {}\n",
    "\n",
    "# =========================\n",
    "# 7) ê·¸ë¦¼ ì €ì¥ â€” Users only / Items only / Users+Items\n",
    "# =========================\n",
    "def plot_users_only():\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        u_st, u_en = slices[s][\"user\"]\n",
    "        coords_u = X_tsne[u_st:u_en]\n",
    "        # ìµœì¢… step ê¸°ì¤€ ë¼ë²¨ë¡œ ìƒ‰ì¹ \n",
    "        if s == CLUSTER_STEP:\n",
    "            labels_plot = labels_u_for_plot\n",
    "        else:\n",
    "            # ë‹¤ë¥¸ stepì€ ê°™ì€ ì‚¬ìš©ì(ìƒ˜í”Œë§ëœ ids)ì— ëŒ€í•´ ë™ì¼í•œ ë¼ë²¨ì„ ì‚¬ìš©\n",
    "            # ids_for_tsne[s] -> ids_tsne_finalë¡œ ë§¤í•‘\n",
    "            ids_tsne_s = ids_for_tsne[s]\n",
    "            labels_plot = np.empty((len(ids_tsne_s),), dtype=int)\n",
    "            for i, uid in enumerate(ids_tsne_s):\n",
    "                labels_plot[i] = labels_u_for_plot[np.where(ids_tsne_final == uid)[0][0]]\n",
    "        for k in range(K_USERS):\n",
    "            mk = (labels_plot == k)\n",
    "            if np.any(mk):\n",
    "                ax.scatter(coords_u[mk,0], coords_u[mk,1], s=8, alpha=0.7,\n",
    "                           c=palette[k % len(palette)], label=f\"Users: C{k}\")\n",
    "        if s == CLUSTER_STEP:\n",
    "            for k, ctr in user_centers.items():\n",
    "                ax.scatter(ctr[0], ctr[1], s=120, marker=\"X\",\n",
    "                           c=palette[k % len(palette)], edgecolor=\"k\")\n",
    "        ax.set_title(f\"step{s} (Users only, colored by step{CLUSTER_STEP})\")\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        # ê³ ì • ì¶•ì„ ì“°ê³  ì‹¶ìœ¼ë©´ ì£¼ì„ í•´ì œ\n",
    "        # ax.set_xlim(xmin - xpad, xmax + xpad)\n",
    "        # ax.set_ylim(ymin - ypad, ymax + ypad)\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_USERS_ONLY.png\")\n",
    "    plt.savefig(out, dpi=220); plt.close()\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "def plot_items_only():\n",
    "    has_any = any(item_for_tsne[s] is not None for s in STEPS) and (item_labels_final is not None)\n",
    "    if not has_any:\n",
    "        print(\"  - Item only plot ìƒëµ(ì•„ì´í…œ ì„ë² ë”©/ë¼ë²¨ ì—†ìŒ)\")\n",
    "        return\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        if slices[s][\"item\"] is not None:\n",
    "            i_st, i_en = slices[s][\"item\"]\n",
    "            coords_i = X_tsne[i_st:i_en]\n",
    "            # ìµœì¢… stepì˜ item_labels_finalì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ê³µí†µ keep_itemsì— ëŒ€ì‘)\n",
    "            for k in range(K_ITEMS):\n",
    "                mk = (item_labels_final == k)\n",
    "                if np.any(mk):\n",
    "                    ax.scatter(coords_i[mk,0], coords_i[mk,1], s=5, alpha=0.35,\n",
    "                               c=palette[(k+2) % len(palette)], label=f\"Items: C{k}\")\n",
    "            if s == CLUSTER_STEP and len(item_centers) > 0:\n",
    "                for k, ctr in item_centers.items():\n",
    "                    ax.scatter(ctr[0], ctr[1], s=110, marker=\"D\",\n",
    "                               c=palette[(k+2) % len(palette)], edgecolor=\"k\")\n",
    "        ax.set_title(f\"step{s} (Items only, colored by step{CLUSTER_STEP})\")\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_ITEMS_ONLY.png\")\n",
    "    plt.savefig(out, dpi=220); plt.close()\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "def plot_users_items():\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        # ì•„ì´í…œ\n",
    "        if slices[s][\"item\"] is not None and item_labels_final is not None:\n",
    "            i_st, i_en = slices[s][\"item\"]\n",
    "            coords_i = X_tsne[i_st:i_en]\n",
    "            for k in range(K_ITEMS):\n",
    "                mk = (item_labels_final == k)\n",
    "                if np.any(mk):\n",
    "                    ax.scatter(coords_i[mk,0], coords_i[mk,1], s=5, alpha=0.35,\n",
    "                               c=palette[(k+2) % len(palette)], label=f\"Items: C{k}\")\n",
    "            if s == CLUSTER_STEP and len(item_centers) > 0:\n",
    "                for k, ctr in item_centers.items():\n",
    "                    ax.scatter(ctr[0], ctr[1], s=110, marker=\"D\",\n",
    "                               c=palette[(k+2) % len(palette)], edgecolor=\"k\")\n",
    "        # ìœ ì €\n",
    "        u_st, u_en = slices[s][\"user\"]\n",
    "        coords_u = X_tsne[u_st:u_en]\n",
    "        # stepë³„ ìœ ì € ë¼ë²¨ êµ¬ì„±\n",
    "        if s == CLUSTER_STEP:\n",
    "            labels_plot = labels_u_for_plot\n",
    "        else:\n",
    "            ids_tsne_s = ids_for_tsne[s]\n",
    "            labels_plot = np.empty((len(ids_tsne_s),), dtype=int)\n",
    "            for i, uid in enumerate(ids_tsne_s):\n",
    "                labels_plot[i] = labels_u_for_plot[np.where(ids_tsne_final == uid)[0][0]]\n",
    "        for k in range(K_USERS):\n",
    "            mk = (labels_plot == k)\n",
    "            if np.any(mk):\n",
    "                ax.scatter(coords_u[mk,0], coords_u[mk,1], s=8, alpha=0.7,\n",
    "                           c=palette[k % len(palette)], label=f\"Users: C{k}\")\n",
    "        if s == CLUSTER_STEP:\n",
    "            for k, ctr in user_centers.items():\n",
    "                ax.scatter(ctr[0], ctr[1], s=120, marker=\"X\",\n",
    "                           c=palette[k % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        ax.set_title(f\"step{s} (Users & Items, colored by step{CLUSTER_STEP})\")\n",
    "        ax.set_xlim(xmin - xpad, xmax + xpad)\n",
    "        ax.set_ylim(ymin - ypad, ymax + ypad)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_USERS_ITEMS.png\")\n",
    "    plt.savefig(out, dpi=220); plt.close()\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "print(\"Plotting...\")\n",
    "plot_users_only()\n",
    "plot_items_only()\n",
    "plot_users_items()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3462e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 9) Raw: í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ê°„ ê±°ë¦¬(stepë³„) ê³„ì‚° & ì‹œê°í™” (t-SNE ì œê±°, 2rowÃ—1col í•˜ë‚˜ì˜ figure) =====\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from itertools import combinations\n",
    "\n",
    "# ì „ì œ: ìœ„ì—ì„œ ë‹¤ìŒ ë³€ìˆ˜ë“¤ì´ ì´ë¯¸ ì •ì˜ë˜ì–´ ìˆìŒ\n",
    "# - STEPS, CLUSTER_STEP, OUT_DIR\n",
    "# - aligned_users_by_step, user_by_step\n",
    "# - item_for_tsne (raw item embedding common subset), item_labels_final ë˜ëŠ” None\n",
    "# - ids_final, user_labels_final\n",
    "# - K_USERS, K_ITEMS\n",
    "\n",
    "_user_label_map_final = {int(u): int(l) for u, l in zip(ids_final.tolist(), user_labels_final.tolist())}\n",
    "\n",
    "def _labels_for_user_ids(user_ids: np.ndarray) -> np.ndarray:\n",
    "    if len(user_ids) == 0:\n",
    "        return np.empty((0,), dtype=int)\n",
    "    out = np.full((len(user_ids),), -1, dtype=int)\n",
    "    for i, u in enumerate(user_ids.tolist()):\n",
    "        out[i] = _user_label_map_final.get(int(u), -1)\n",
    "    return out\n",
    "\n",
    "def _centers_from_points(points: np.ndarray, labels: np.ndarray, K: int) -> dict:\n",
    "    centers = {}\n",
    "    for k in range(K):\n",
    "        m = (labels == k)\n",
    "        centers[k] = points[m].mean(axis=0) if np.any(m) else None\n",
    "    return centers\n",
    "\n",
    "def _dist(a, b):\n",
    "    if a is None or b is None:\n",
    "        return np.nan\n",
    "    return float(np.linalg.norm(a - b))\n",
    "\n",
    "def _pairwise_center_stats(centers: dict):\n",
    "    vecs = [v for v in centers.values() if v is not None]\n",
    "    if len(vecs) < 2:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    dists = [np.linalg.norm(a - b) for a, b in combinations(vecs, 2)]\n",
    "    return (float(np.mean(dists)), float(np.min(dists)), float(np.max(dists)))\n",
    "\n",
    "def _ui_stats(centers_u: dict, centers_i: dict):\n",
    "    vec_u = [v for v in centers_u.values() if v is not None]\n",
    "    vec_i = [v for v in centers_i.values() if v is not None]\n",
    "    if len(vec_u) == 0 or len(vec_i) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    d = []\n",
    "    for u in vec_u:\n",
    "        for i in vec_i:\n",
    "            d.append(np.linalg.norm(u - i))\n",
    "    return (float(np.mean(d)), float(np.min(d)), float(np.max(d)))\n",
    "\n",
    "rows = []\n",
    "for s in STEPS:\n",
    "    # ---- raw space: user centers ----\n",
    "    ids_u_raw = aligned_users_by_step[s]  # (Nr_s,)\n",
    "    if len(ids_u_raw) > 0:\n",
    "        emb_u_raw = user_by_step[s][ids_u_raw, :]\n",
    "    else:\n",
    "        emb_u_raw = np.empty((0, user_by_step[s].shape[1]))\n",
    "    labels_u_raw = _labels_for_user_ids(ids_u_raw)\n",
    "    centers_u_raw = _centers_from_points(emb_u_raw, labels_u_raw, K_USERS)\n",
    "\n",
    "    # ---- raw space: item centers (keep_items common subset) ----\n",
    "    if item_for_tsne[s] is not None and item_labels_final is not None:\n",
    "        emb_i_raw = item_for_tsne[s]          # (Ni_common, d)\n",
    "        labels_i_raw = item_labels_final      # (Ni_common,)\n",
    "        centers_i_raw = _centers_from_points(emb_i_raw, labels_i_raw, K_ITEMS)\n",
    "    else:\n",
    "        centers_i_raw = {k: None for k in range(K_ITEMS)}\n",
    "\n",
    "    # ---- summary stats (raw) ----\n",
    "    uu_raw_mean, uu_raw_min, uu_raw_max = _pairwise_center_stats(centers_u_raw)\n",
    "    ii_raw_mean, ii_raw_min, ii_raw_max = _pairwise_center_stats(centers_i_raw)\n",
    "\n",
    "    row = {\n",
    "        \"step\": s,\n",
    "        \"user_dist_raw_mean\": uu_raw_mean,\n",
    "        \"user_dist_raw_min\":  uu_raw_min,\n",
    "        \"user_dist_raw_max\":  uu_raw_max,\n",
    "        \"item_dist_raw_mean\": ii_raw_mean,\n",
    "        \"item_dist_raw_min\":  ii_raw_min,\n",
    "        \"item_dist_raw_max\":  ii_raw_max,\n",
    "    }\n",
    "\n",
    "    # K==2 ìƒì„¸\n",
    "    if K_USERS == 2:\n",
    "        row[\"user_dist_raw_U0U1\"] = _dist(centers_u_raw.get(0), centers_u_raw.get(1))\n",
    "    if K_ITEMS == 2:\n",
    "        row[\"item_dist_raw_I0I1\"] = _dist(centers_i_raw.get(0), centers_i_raw.get(1))\n",
    "\n",
    "    # user-item distances (raw)\n",
    "    if K_USERS == 2 and K_ITEMS == 2:\n",
    "        row.update({\n",
    "            \"ui_raw_U0_I0\": _dist(centers_u_raw.get(0), centers_i_raw.get(0)),\n",
    "            \"ui_raw_U0_I1\": _dist(centers_u_raw.get(0), centers_i_raw.get(1)),\n",
    "            \"ui_raw_U1_I0\": _dist(centers_u_raw.get(1), centers_i_raw.get(0)),\n",
    "            \"ui_raw_U1_I1\": _dist(centers_u_raw.get(1), centers_i_raw.get(1)),\n",
    "        })\n",
    "    else:\n",
    "        m, mn, mx = _ui_stats(centers_u_raw, centers_i_raw)\n",
    "        row.update({\n",
    "            \"ui_raw_mean\": m,\n",
    "            \"ui_raw_min\":  mn,\n",
    "            \"ui_raw_max\":  mx,\n",
    "        })\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "# ---- CSV ì €ì¥ ----\n",
    "dist_csv = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_center_distances_over_steps_RAW_ONLY.csv\")\n",
    "with open(dist_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "print(f\"  - saved: {dist_csv}\")\n",
    "\n",
    "# ---- Plot: 2rowÃ—1col in ONE figure (raw only) ----\n",
    "steps_ax = [r[\"step\"] for r in rows]\n",
    "\n",
    "ud_r_mean = [r[\"user_dist_raw_mean\"] for r in rows]\n",
    "id_r_mean = [r[\"item_dist_raw_mean\"] for r in rows]\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(9, 7), sharex=True)\n",
    "\n",
    "# (row 1) user-user / item-item raw mean\n",
    "axes[0].plot(steps_ax, ud_r_mean, marker=\"o\", label=\"Users (raw, mean)\")\n",
    "axes[0].plot(steps_ax, id_r_mean, marker=\"o\", label=\"Items (raw, mean)\")\n",
    "axes[0].set_ylabel(\"Center distance\")\n",
    "axes[0].set_title(\"User/User & Item/Item center distance over steps (raw)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# (row 2) user-item raw\n",
    "if K_USERS == 2 and K_ITEMS == 2:\n",
    "    ui_raw_u0i0 = [r.get(\"ui_raw_U0_I0\", np.nan) for r in rows]\n",
    "    ui_raw_u0i1 = [r.get(\"ui_raw_U0_I1\", np.nan) for r in rows]\n",
    "    ui_raw_u1i0 = [r.get(\"ui_raw_U1_I0\", np.nan) for r in rows]\n",
    "    ui_raw_u1i1 = [r.get(\"ui_raw_U1_I1\", np.nan) for r in rows]\n",
    "\n",
    "    axes[1].plot(steps_ax, ui_raw_u0i0, marker=\"s\", label=\"U0â€“I0 (raw)\")\n",
    "    axes[1].plot(steps_ax, ui_raw_u0i1, marker=\"s\", label=\"U0â€“I1 (raw)\")\n",
    "    axes[1].plot(steps_ax, ui_raw_u1i0, marker=\"s\", label=\"U1â€“I0 (raw)\")\n",
    "    axes[1].plot(steps_ax, ui_raw_u1i1, marker=\"s\", label=\"U1â€“I1 (raw)\")\n",
    "    axes[1].legend(ncol=2)\n",
    "else:\n",
    "    ui_raw_mean = [r.get(\"ui_raw_mean\", np.nan) for r in rows]\n",
    "    ui_raw_min  = [r.get(\"ui_raw_min\",  np.nan) for r in rows]\n",
    "    ui_raw_max  = [r.get(\"ui_raw_max\",  np.nan) for r in rows]\n",
    "\n",
    "    axes[1].plot(steps_ax, ui_raw_mean, marker=\"s\", label=\"mean (raw)\")\n",
    "    axes[1].plot(steps_ax, ui_raw_min,  marker=\"s\", label=\"min (raw)\")\n",
    "    axes[1].plot(steps_ax, ui_raw_max,  marker=\"s\", label=\"max (raw)\")\n",
    "    axes[1].legend()\n",
    "\n",
    "axes[1].set_xlabel(\"Step\")\n",
    "axes[1].set_ylabel(\"Center distance\")\n",
    "axes[1].set_title(\"Userâ€“Item center distances over steps (raw)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "dist_png = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_center_distances_over_steps_RAW_ONLY.png\")\n",
    "plt.savefig(dist_png, dpi=220)\n",
    "plt.close()\n",
    "print(f\"  - saved: {dist_png}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c321321b",
   "metadata": {},
   "source": [
    "### 1ìŠ¤í…ë³„ë¡œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb0eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ============================================================\n",
    "# ì‚¬ìš©ì í™˜ê²½ì—ì„œ ë¯¸ë¦¬ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•˜ëŠ” ê°’ë“¤\n",
    "#   - BASE_DIR: ì„ë² ë”©(.npy)ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬\n",
    "#   - PARTS: step ê°œìˆ˜ (ì˜ˆ: 5)\n",
    "#   - common_users: trainâˆ©label ê³µí†µ ìœ ì € id ì§‘í•©/ë¦¬ìŠ¤íŠ¸/ndarray\n",
    "# ============================================================\n",
    "# BASE_DIR = \"...\"\n",
    "# PARTS = 5\n",
    "# common_users = ...\n",
    "\n",
    "# =========================\n",
    "# ê²½ë¡œ/ì„¤ì •\n",
    "# =========================\n",
    "STEPS = list(range(PARTS))\n",
    "CLUSTER_STEP = STEPS[-1]  # ìµœì¢… stepì—ì„œ KMeans ìˆ˜í–‰\n",
    "OUT_DIR = os.path.join(BASE_DIR, \"figs_tsne_cluster_progress\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TSNE_PERPLEXITY = 30\n",
    "TSNE_RANDOM_STATE = 42\n",
    "KMEANS_RANDOM_STATE = 42\n",
    "K_USERS = 2\n",
    "K_ITEMS = 2\n",
    "\n",
    "# (ì„ íƒ) t-SNE ê°€ì† ìƒ˜í”Œë§\n",
    "SUBSAMPLE_USERS_FOR_TSNE = None  # ì˜ˆ: 5000\n",
    "SUBSAMPLE_ITEMS_FOR_TSNE = None  # ì˜ˆ: 5000\n",
    "\n",
    "# (ì „ì—­) t-SNE 2Dì—ì„œ ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬ ìƒìœ„ p%ë¥¼ \"í”Œë¡œíŒ…ì—ì„œë§Œ\" ì œê±°\n",
    "GLOBAL_TRIM_TOP_PCT = 0.0   # ì „ì—­ trimming ì›í•˜ë©´ 0.005 ë“±ìœ¼ë¡œ (0ì´ë©´ ì „ì—­ trim off)\n",
    "\n",
    "# (í•µì‹¬) stepë³„(local) trimming: step1ì˜ ì™¸ë”´ ì  ê°™ì€ ê±¸ ì œê±°\n",
    "STEP_TRIM_TOP_PCT = 0.01        # ê¸°ë³¸: ê° stepì—ì„œ ìƒìœ„ 0.2% ì œê±°\n",
    "STEP_TRIM_TOP_PCT_STEP1 = 0.01  # step==1ë§Œ ë” ì„¸ê²Œ(0.5%) ì œê±°\n",
    "\n",
    "# =========================\n",
    "# ìœ í‹¸\n",
    "# =========================\n",
    "def load_users_items_for_step(base_dir: str, parts: int, step: int):\n",
    "    upath = os.path.join(base_dir, f\"user_emb_part{parts}_step{step}.npy\")\n",
    "    ipath = os.path.join(base_dir, f\"item_emb_part{parts}_step{step}.npy\")\n",
    "    if not os.path.exists(upath):\n",
    "        raise FileNotFoundError(upath)\n",
    "    U = np.load(upath)  # (num_users_step, d)\n",
    "    I = np.load(ipath) if os.path.exists(ipath) else None  # (num_items_step, d) or None\n",
    "    if U.ndim != 2:\n",
    "        raise ValueError(f\"user_emb shape invalid at step{step}: {U.shape}\")\n",
    "    if I is not None and I.ndim != 2:\n",
    "        raise ValueError(f\"item_emb shape invalid at step{step}: {I.shape}\")\n",
    "    return U, I\n",
    "\n",
    "\n",
    "def align_users_for_step(common_users, num_users_step: int) -> np.ndarray:\n",
    "    if isinstance(common_users, (set, list, tuple)):\n",
    "        cu = np.asarray(sorted(common_users), dtype=np.int64)\n",
    "    else:\n",
    "        cu = np.asarray(common_users, dtype=np.int64)\n",
    "\n",
    "    if cu.size == 0:\n",
    "        return cu\n",
    "\n",
    "    mask = (cu >= 0) & (cu < num_users_step)\n",
    "    return cu[mask]\n",
    "\n",
    "\n",
    "def compute_centers(coords: np.ndarray, labels: np.ndarray, K: int):\n",
    "    centers = {}\n",
    "    for k in range(K):\n",
    "        mk = (labels == k)\n",
    "        if np.any(mk):\n",
    "            centers[k] = coords[mk].mean(axis=0)\n",
    "    return centers\n",
    "\n",
    "\n",
    "def stepwise_trim_mask(coords2d: np.ndarray, top_pct: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    coords2d: (N,2)\n",
    "    top_pct: ì´ stepì—ì„œ ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬ ìƒìœ„ top_pct ì œê±° (plot-only)\n",
    "    return: keep mask (True=keep)\n",
    "    \"\"\"\n",
    "    n = 0 if coords2d is None else len(coords2d)\n",
    "    if n == 0:\n",
    "        return np.zeros((0,), dtype=bool)\n",
    "    if top_pct is None or top_pct <= 0:\n",
    "        return np.ones((n,), dtype=bool)\n",
    "    if top_pct >= 1.0:\n",
    "        return np.zeros((n,), dtype=bool)\n",
    "\n",
    "    ctr = np.median(coords2d, axis=0)\n",
    "    dist = np.linalg.norm(coords2d - ctr, axis=1)\n",
    "\n",
    "    q = 1.0 - top_pct\n",
    "    q = min(max(q, 0.0), 1.0)\n",
    "    thr = np.quantile(dist, q)\n",
    "    return dist <= thr\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) ê° step ì„ë² ë”© ë¡œë“œ\n",
    "# =========================\n",
    "print(\"Loading augmentation embeddings per step...\")\n",
    "user_by_step = {}\n",
    "item_by_step = {}\n",
    "dims = set()\n",
    "\n",
    "for s in STEPS:\n",
    "    U, I = load_users_items_for_step(BASE_DIR, PARTS, s)\n",
    "    user_by_step[s] = U\n",
    "    item_by_step[s] = I\n",
    "    dims.add(U.shape[1])\n",
    "    if I is not None:\n",
    "        dims.add(I.shape[1])\n",
    "    print(f\"  - step{s}: users={U.shape}, items={None if I is None else I.shape}\")\n",
    "\n",
    "if len(dims) != 1:\n",
    "    raise RuntimeError(f\"ì„ë² ë”© ì°¨ì›ì´ stepë³„ë¡œ ë‹¤ë¦…ë‹ˆë‹¤: {dims}\")\n",
    "D = dims.pop()\n",
    "\n",
    "# =========================\n",
    "# 2) trainâˆ©label ê³µí†µ ìœ ì € ì‚°ì¶œ + stepë³„ ì •ë ¬\n",
    "# =========================\n",
    "print(f\"common users: {len(common_users)}\")\n",
    "aligned_users_by_step = {\n",
    "    s: align_users_for_step(common_users, user_by_step[s].shape[0])\n",
    "    for s in STEPS\n",
    "}\n",
    "print({s: len(aligned_users_by_step[s]) for s in STEPS})\n",
    "\n",
    "# =========================\n",
    "# 3) ì•„ì´í…œ ê³µí†µ ë¶€ë¶„ ì •ë ¬(ìµœì†Œ ê¸¸ì´)\n",
    "# =========================\n",
    "item_lengths = [item_by_step[s].shape[0] for s in STEPS if item_by_step[s] is not None]\n",
    "if len(item_lengths) == 0:\n",
    "    I_base = 0\n",
    "else:\n",
    "    I_base = min(item_lengths)\n",
    "    uniq = set(item_lengths)\n",
    "    if len(uniq) != 1:\n",
    "        print(f\"[warn] stepë³„ ì•„ì´í…œ ìˆ˜ê°€ ë‹¤ë¦…ë‹ˆë‹¤: {uniq} â†’ ì•ì—ì„œ {I_base}ê°œë§Œ ê³µí†µ ì‚¬ìš©\")\n",
    "    for s in STEPS:\n",
    "        I = item_by_step[s]\n",
    "        item_by_step[s] = None if (I is None or I.shape[0] < I_base) else I[:I_base]\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "if I_base > 0:\n",
    "    if SUBSAMPLE_ITEMS_FOR_TSNE is not None and SUBSAMPLE_ITEMS_FOR_TSNE < I_base:\n",
    "        keep_items = np.sort(rng.choice(I_base, size=SUBSAMPLE_ITEMS_FOR_TSNE, replace=False))\n",
    "    else:\n",
    "        keep_items = np.arange(I_base, dtype=int)\n",
    "else:\n",
    "    keep_items = np.array([], dtype=int)\n",
    "\n",
    "# =========================\n",
    "# 4) ìµœì¢… stepì—ì„œ KMeans â†’ ë ˆì´ë¸” ìƒì„±\n",
    "# =========================\n",
    "print(f\"KMeans on final step: step{CLUSTER_STEP}\")\n",
    "U_final = user_by_step[CLUSTER_STEP]\n",
    "ids_final = aligned_users_by_step[CLUSTER_STEP]\n",
    "if len(ids_final) < K_USERS:\n",
    "    raise RuntimeError(f\"ìµœì¢… stepì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ê°€ëŠ¥í•œ ê³µí†µ ìœ ì €ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: {len(ids_final)}\")\n",
    "\n",
    "emb_final_users = U_final[ids_final, :]  # (|ids_final|, D)\n",
    "user_kmeans = KMeans(n_clusters=K_USERS, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "user_labels_final_full = user_kmeans.fit_predict(emb_final_users)  # ids_final ìˆœì„œ ê¸°ì¤€\n",
    "\n",
    "# ì•„ì´í…œ KMeans (ìµœì¢… step ê³µí†µ ì•„ì´í…œ)\n",
    "if item_by_step[CLUSTER_STEP] is not None and len(keep_items) > 0:\n",
    "    I_final = item_by_step[CLUSTER_STEP][keep_items, :]  # (|keep_items|, D)\n",
    "    item_kmeans = KMeans(n_clusters=K_ITEMS, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "    item_labels_final_full = item_kmeans.fit_predict(I_final)  # keep_items ìˆœì„œ ê¸°ì¤€\n",
    "else:\n",
    "    item_kmeans = None\n",
    "    item_labels_final_full = None\n",
    "    print(\"[warn] ìµœì¢… step ì•„ì´í…œì´ ì—†ì–´ item clustering ìƒëµ\")\n",
    "\n",
    "# ids_final uid -> label ë§¤í•‘ (ìµœì¢… step ê¸°ì¤€)\n",
    "uid2label = {int(uid): int(lb) for uid, lb in zip(ids_final.tolist(), user_labels_final_full.tolist())}\n",
    "\n",
    "# =========================\n",
    "# 5) t-SNEì— ë„£ì„ ë°ì´í„° ë¸”ë¡ êµ¬ì„± (users + items across steps)\n",
    "# =========================\n",
    "print(\"Preparing t-SNE blocks...\")\n",
    "ids_for_tsne = {}      # step -> user_id ë°°ì—´(ìƒ˜í”Œë§ ë°˜ì˜)\n",
    "users_for_tsne = {}    # step -> user_emb ë°°ì—´\n",
    "item_for_tsne  = {}    # step -> item_emb ë°°ì—´ (ê³µí†µ keep_itemsì— í•´ë‹¹)\n",
    "\n",
    "# ìœ ì € ìƒ˜í”Œë§ ê¸°ì¤€: ìµœì¢… stepì˜ ids_final ì§‘í•©\n",
    "if SUBSAMPLE_USERS_FOR_TSNE is not None and SUBSAMPLE_USERS_FOR_TSNE < len(ids_final):\n",
    "    sample_users = np.sort(rng.choice(ids_final, size=SUBSAMPLE_USERS_FOR_TSNE, replace=False))\n",
    "else:\n",
    "    sample_users = ids_final\n",
    "\n",
    "for s in STEPS:\n",
    "    ids_s = aligned_users_by_step[s]\n",
    "    if len(ids_s) == 0:\n",
    "        ids_for_tsne[s] = np.array([], dtype=np.int64)\n",
    "        users_for_tsne[s] = np.empty((0, D), dtype=np.float32)\n",
    "    else:\n",
    "        mask = np.isin(ids_s, sample_users)\n",
    "        ids_for_tsne[s] = ids_s[mask]\n",
    "        users_for_tsne[s] = user_by_step[s][ids_for_tsne[s], :]\n",
    "\n",
    "    I = item_by_step[s]\n",
    "    item_for_tsne[s] = None if (I is None or len(keep_items) == 0) else I[keep_items, :]\n",
    "\n",
    "# =========================\n",
    "# 6) t-SNE ìˆ˜í–‰ (ëª¨ë“  stepì˜ ìœ ì €/ì•„ì´í…œ í•©ì³ì„œ)\n",
    "# =========================\n",
    "print(\"Fitting joint t-SNE on users + items across steps\")\n",
    "blocks = []\n",
    "slices = {}  # step -> {\"user\": (st,en), \"item\": (st,en) or None}\n",
    "cursor = 0\n",
    "\n",
    "for s in STEPS:\n",
    "    Ublk = users_for_tsne[s]\n",
    "    blocks.append(Ublk)\n",
    "    u_st, u_en = cursor, cursor + len(Ublk)\n",
    "    cursor = u_en\n",
    "\n",
    "    Iblk = item_for_tsne[s]\n",
    "    if Iblk is not None and len(Iblk) > 0:\n",
    "        blocks.append(Iblk)\n",
    "        i_st, i_en = cursor, cursor + len(Iblk)\n",
    "        cursor = i_en\n",
    "        slices[s] = {\"user\": (u_st, u_en), \"item\": (i_st, i_en)}\n",
    "    else:\n",
    "        slices[s] = {\"user\": (u_st, u_en), \"item\": None}\n",
    "\n",
    "if len(blocks) == 0 or sum(len(b) for b in blocks) < 3:\n",
    "    raise RuntimeError(\"t-SNEì— ì‚¬ìš©í•  í‘œë³¸ì´ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "X_all = np.vstack(blocks)\n",
    "perp = min(TSNE_PERPLEXITY, max(5, (len(X_all) - 1)//3))\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=perp,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    "    random_state=TSNE_RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    verbose=1\n",
    ")\n",
    "X_tsne = tsne.fit_transform(X_all)\n",
    "\n",
    "# =========================\n",
    "# (ì„ íƒ) ì „ì—­ trimming (plot-only)\n",
    "# =========================\n",
    "if GLOBAL_TRIM_TOP_PCT is not None and GLOBAL_TRIM_TOP_PCT > 0:\n",
    "    ctr2d = np.median(X_tsne, axis=0)\n",
    "    dist2d = np.linalg.norm(X_tsne - ctr2d, axis=1)\n",
    "    thr = np.quantile(dist2d, 1.0 - GLOBAL_TRIM_TOP_PCT)\n",
    "    global_keep_mask = dist2d <= thr\n",
    "    print(f\"[global-trim] keep {global_keep_mask.sum()}/{len(global_keep_mask)} \"\n",
    "          f\"({100.0 * global_keep_mask.mean():.2f}%) points\")\n",
    "else:\n",
    "    global_keep_mask = np.ones((X_tsne.shape[0],), dtype=bool)\n",
    "\n",
    "# ì¶• ë²”ìœ„: ì „ì—­ keep ê¸°ì¤€(ì—†ìœ¼ë©´ ì „ì²´)\n",
    "X_plot = X_tsne[global_keep_mask]\n",
    "xmin, ymin = X_plot.min(axis=0)\n",
    "xmax, ymax = X_plot.max(axis=0)\n",
    "xpad = 0.05 * (xmax - xmin) if xmax > xmin else 0.5\n",
    "ypad = 0.05 * (ymax - ymin) if ymax > ymin else 0.5\n",
    "\n",
    "palette = [\"tab:blue\",\"tab:orange\",\"tab:green\",\"tab:red\",\"tab:purple\",\n",
    "           \"tab:brown\",\"tab:pink\",\"tab:gray\",\"tab:olive\",\"tab:cyan\"]\n",
    "\n",
    "# =========================\n",
    "# 6.5) ìµœì¢… step ì‚¬ìš©ì/ì•„ì´í…œì˜ (plotìš©) ë¼ë²¨/ì„¼í„° ì¤€ë¹„\n",
    "#     - center ê³„ì‚°ë„ trimming(ì „ì—­/ë¡œì»¬) ì ìš©í•´ì„œ ì¼ê´€ë˜ê²Œ\n",
    "# =========================\n",
    "# final users slice\n",
    "u_st_f, u_en_f = slices[CLUSTER_STEP][\"user\"]\n",
    "coords_u_final_all = X_tsne[u_st_f:u_en_f]\n",
    "ids_tsne_final_all = ids_for_tsne[CLUSTER_STEP]\n",
    "labels_u_final_all = np.array([uid2label[int(uid)] for uid in ids_tsne_final_all], dtype=int)\n",
    "\n",
    "# ì „ì—­ trim ì ìš©\n",
    "g_keep_u_f = global_keep_mask[u_st_f:u_en_f]\n",
    "coords_u_final_g = coords_u_final_all[g_keep_u_f]\n",
    "ids_tsne_final_g = ids_tsne_final_all[g_keep_u_f]\n",
    "labels_u_final_g = labels_u_final_all[g_keep_u_f]\n",
    "\n",
    "# ìµœì¢… stepì— ëŒ€í•´ ë¡œì»¬ trimë„ ì ìš© (ê¸°ë³¸ step ê·œì¹™)\n",
    "local_pct_f = STEP_TRIM_TOP_PCT_STEP1 if CLUSTER_STEP == 1 else STEP_TRIM_TOP_PCT\n",
    "l_keep_u_f = stepwise_trim_mask(coords_u_final_g, top_pct=local_pct_f)\n",
    "coords_u_final = coords_u_final_g[l_keep_u_f]\n",
    "ids_tsne_final = ids_tsne_final_g[l_keep_u_f]\n",
    "labels_u_for_plot = labels_u_final_g[l_keep_u_f]\n",
    "\n",
    "user_centers = compute_centers(coords_u_final, labels_u_for_plot, K_USERS)\n",
    "\n",
    "# final items slice\n",
    "if slices[CLUSTER_STEP][\"item\"] is not None and item_labels_final_full is not None:\n",
    "    i_st_f, i_en_f = slices[CLUSTER_STEP][\"item\"]\n",
    "    coords_i_final_all = X_tsne[i_st_f:i_en_f]\n",
    "    labels_i_final_all = item_labels_final_full\n",
    "\n",
    "    # ì „ì—­ trim\n",
    "    g_keep_i_f = global_keep_mask[i_st_f:i_en_f]\n",
    "    coords_i_final_g = coords_i_final_all[g_keep_i_f]\n",
    "    labels_i_final_g = labels_i_final_all[g_keep_i_f]\n",
    "\n",
    "    # ë¡œì»¬ trim(ìµœì¢… step)\n",
    "    local_pct_i_f = STEP_TRIM_TOP_PCT_STEP1 if CLUSTER_STEP == 1 else STEP_TRIM_TOP_PCT\n",
    "    l_keep_i_f = stepwise_trim_mask(coords_i_final_g, top_pct=local_pct_i_f)\n",
    "    coords_i_final = coords_i_final_g[l_keep_i_f]\n",
    "    labels_i_final = labels_i_final_g[l_keep_i_f]\n",
    "\n",
    "    item_centers = compute_centers(coords_i_final, labels_i_final, K_ITEMS)\n",
    "else:\n",
    "    item_centers = {}\n",
    "\n",
    "# final uid -> position (trimmed final ê¸°ì¤€)\n",
    "pos_map_final = {int(uid): i for i, uid in enumerate(ids_tsne_final.tolist())}\n",
    "\n",
    "# =========================\n",
    "# 7) ê·¸ë¦¼ ì €ì¥ â€” Users only / Items only / Users+Items\n",
    "# =========================\n",
    "def plot_users_only():\n",
    "    step_dir = os.path.join(OUT_DIR, \"step_by_step\")\n",
    "    os.makedirs(step_dir, exist_ok=True)\n",
    "\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        fig_s, ax_s = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "        u_st, u_en = slices[s][\"user\"]\n",
    "        coords_u_all = X_tsne[u_st:u_en]\n",
    "        ids_tsne_s_all = ids_for_tsne[s]\n",
    "\n",
    "        if len(ids_tsne_s_all) == 0:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            plt.close(fig_s)\n",
    "            continue\n",
    "\n",
    "        # (1) ì „ì—­ trim\n",
    "        g_keep = global_keep_mask[u_st:u_en]\n",
    "        coords_u_g = coords_u_all[g_keep]\n",
    "        ids_u_g = ids_tsne_s_all[g_keep]\n",
    "\n",
    "        if len(ids_u_g) == 0:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            plt.close(fig_s)\n",
    "            continue\n",
    "\n",
    "        # (2) stepë³„ ë¡œì»¬ trim\n",
    "        local_pct = STEP_TRIM_TOP_PCT_STEP1 if s == 1 else STEP_TRIM_TOP_PCT\n",
    "        l_keep = stepwise_trim_mask(coords_u_g, top_pct=local_pct)\n",
    "        coords_u = coords_u_g[l_keep]\n",
    "        ids_tsne_s = ids_u_g[l_keep]\n",
    "\n",
    "        if len(ids_tsne_s) == 0:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            plt.close(fig_s)\n",
    "            continue\n",
    "\n",
    "        # final ê¸°ì¤€ ë¼ë²¨ë¡œ ìƒ‰ì¹ : finalì— ì—†ëŠ” uidëŠ” ë²„ë¦¼\n",
    "        valid_mask = np.array([int(uid) in pos_map_final for uid in ids_tsne_s], dtype=bool)\n",
    "        coords_valid = coords_u[valid_mask]\n",
    "        ids_valid = ids_tsne_s[valid_mask]\n",
    "\n",
    "        if len(ids_valid) == 0:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            plt.close(fig_s)\n",
    "            continue\n",
    "\n",
    "        labels_plot = np.array([labels_u_for_plot[pos_map_final[int(uid)]] for uid in ids_valid], dtype=int)\n",
    "\n",
    "        for k in range(K_USERS):\n",
    "            mk = (labels_plot == k)\n",
    "            if np.any(mk):\n",
    "                ax.scatter(coords_valid[mk, 0], coords_valid[mk, 1],\n",
    "                           s=8, alpha=0.7, c=palette[k % len(palette)],\n",
    "                           label=f\"Users: C{k}\")\n",
    "                ax_s.scatter(coords_valid[mk, 0], coords_valid[mk, 1],\n",
    "                             s=10, alpha=0.7, c=palette[k % len(palette)],\n",
    "                             label=f\"Users: C{k}\")\n",
    "\n",
    "        if s == CLUSTER_STEP:\n",
    "            for k, ctr in user_centers.items():\n",
    "                ax.scatter(ctr[0], ctr[1], s=120, marker=\"X\",\n",
    "                           c=palette[k % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        for a in (ax, ax_s):\n",
    "            #a.set_xlim(xmin - xpad, xmax + xpad)\n",
    "            #a.set_ylim(ymin - ypad, ymax + ypad)\n",
    "            a.set_xticks([]); a.set_yticks([])\n",
    "\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "        ax_s.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "        fig_s.tight_layout()\n",
    "        out_s = os.path.join(step_dir, f\"step{s}_tsne2d_USERS_ONLY.png\")\n",
    "        fig_s.savefig(out_s, dpi=220)\n",
    "        plt.close(fig_s)\n",
    "        print(\"  - saved:\", out_s)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_USERS_ONLY.png\")\n",
    "    fig.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "\n",
    "def plot_items_only():\n",
    "    has_any = any(item_for_tsne[s] is not None for s in STEPS) and (item_labels_final_full is not None)\n",
    "    if not has_any:\n",
    "        print(\"  - Item only plot ìƒëµ(ì•„ì´í…œ ì„ë² ë”©/ë¼ë²¨ ì—†ìŒ)\")\n",
    "        return\n",
    "\n",
    "    step_dir = os.path.join(OUT_DIR, \"step_by_step\")\n",
    "    os.makedirs(step_dir, exist_ok=True)\n",
    "\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        fig_s, ax_s = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "        if slices[s][\"item\"] is not None:\n",
    "            i_st, i_en = slices[s][\"item\"]\n",
    "            coords_i_all = X_tsne[i_st:i_en]\n",
    "            labels_i_all = item_labels_final_full\n",
    "\n",
    "            # (1) ì „ì—­ trim\n",
    "            g_keep = global_keep_mask[i_st:i_en]\n",
    "            coords_i_g = coords_i_all[g_keep]\n",
    "            labels_i_g = labels_i_all[g_keep]\n",
    "\n",
    "            # (2) stepë³„ ë¡œì»¬ trim\n",
    "            local_pct = STEP_TRIM_TOP_PCT_STEP1 if s == 1 else STEP_TRIM_TOP_PCT\n",
    "            l_keep = stepwise_trim_mask(coords_i_g, top_pct=local_pct)\n",
    "            coords_i = coords_i_g[l_keep]\n",
    "            labels_i = labels_i_g[l_keep]\n",
    "\n",
    "            if len(coords_i) > 0:\n",
    "                for k in range(K_ITEMS):\n",
    "                    mk = (labels_i == k)\n",
    "                    if np.any(mk):\n",
    "                        ax.scatter(coords_i[mk, 0], coords_i[mk, 1],\n",
    "                                   s=5, alpha=0.35, c=palette[(k+2) % len(palette)],\n",
    "                                   label=f\"Items: C{k}\")\n",
    "                        ax_s.scatter(coords_i[mk, 0], coords_i[mk, 1],\n",
    "                                     s=5, alpha=0.35, c=palette[(k+2) % len(palette)],\n",
    "                                     label=f\"Items: C{k}\")\n",
    "\n",
    "                if s == CLUSTER_STEP and len(item_centers) > 0:\n",
    "                    for k, ctr in item_centers.items():\n",
    "                        ax.scatter(ctr[0], ctr[1], s=110, marker=\"D\",\n",
    "                                   c=palette[(k+2) % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        for a in (ax, ax_s):\n",
    "            #a.set_xlim(xmin - xpad, xmax + xpad)\n",
    "            #a.set_ylim(ymin - ypad, ymax + ypad)\n",
    "            a.set_xticks([]); a.set_yticks([])\n",
    "\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "        ax_s.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "        fig_s.tight_layout()\n",
    "        out_s = os.path.join(step_dir, f\"step{s}_tsne2d_ITEMS_ONLY.png\")\n",
    "        fig_s.savefig(out_s, dpi=220)\n",
    "        plt.close(fig_s)\n",
    "        print(\"  - saved:\", out_s)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_ITEMS_ONLY.png\")\n",
    "    fig.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "\n",
    "def plot_users_items():\n",
    "    step_dir = os.path.join(OUT_DIR, \"step_by_step\")\n",
    "    os.makedirs(step_dir, exist_ok=True)\n",
    "\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "\n",
    "        # ì•„ì´í…œ\n",
    "        if slices[s][\"item\"] is not None and item_labels_final_full is not None:\n",
    "            i_st, i_en = slices[s][\"item\"]\n",
    "            coords_i_all = X_tsne[i_st:i_en]\n",
    "            labels_i_all = item_labels_final_full\n",
    "\n",
    "            # (1) ì „ì—­ trim\n",
    "            g_keep = global_keep_mask[i_st:i_en]\n",
    "            coords_i_g = coords_i_all[g_keep]\n",
    "            labels_i_g = labels_i_all[g_keep]\n",
    "\n",
    "            # (2) stepë³„ ë¡œì»¬ trim\n",
    "            local_pct = STEP_TRIM_TOP_PCT_STEP1 if s == 1 else STEP_TRIM_TOP_PCT\n",
    "            l_keep = stepwise_trim_mask(coords_i_g, top_pct=local_pct)\n",
    "            coords_i = coords_i_g[l_keep]\n",
    "            labels_i = labels_i_g[l_keep]\n",
    "\n",
    "            if len(coords_i) > 0:\n",
    "                for k in range(K_ITEMS):\n",
    "                    mk = (labels_i == k)\n",
    "                    if np.any(mk):\n",
    "                        ax.scatter(coords_i[mk, 0], coords_i[mk, 1],\n",
    "                                   s=5, alpha=0.35, c=palette[(k+2) % len(palette)],\n",
    "                                   label=f\"Items: C{k}\")\n",
    "                if s == CLUSTER_STEP and len(item_centers) > 0:\n",
    "                    for k, ctr in item_centers.items():\n",
    "                        ax.scatter(ctr[0], ctr[1], s=110, marker=\"D\",\n",
    "                                   c=palette[(k+2) % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        # ìœ ì €\n",
    "        u_st, u_en = slices[s][\"user\"]\n",
    "        coords_u_all = X_tsne[u_st:u_en]\n",
    "        ids_tsne_s_all = ids_for_tsne[s]\n",
    "\n",
    "        if len(ids_tsne_s_all) > 0:\n",
    "            # (1) ì „ì—­ trim\n",
    "            g_keep = global_keep_mask[u_st:u_en]\n",
    "            coords_u_g = coords_u_all[g_keep]\n",
    "            ids_u_g = ids_tsne_s_all[g_keep]\n",
    "\n",
    "            # (2) stepë³„ ë¡œì»¬ trim\n",
    "            local_pct = STEP_TRIM_TOP_PCT_STEP1 if s == 1 else STEP_TRIM_TOP_PCT\n",
    "            l_keep = stepwise_trim_mask(coords_u_g, top_pct=local_pct)\n",
    "            coords_u = coords_u_g[l_keep]\n",
    "            ids_tsne_s = ids_u_g[l_keep]\n",
    "\n",
    "            # final ê¸°ì¤€ ë¼ë²¨ë¡œ ìƒ‰ì¹  (finalì— ì—†ëŠ” uidëŠ” ë²„ë¦¼)\n",
    "            valid_mask = np.array([int(uid) in pos_map_final for uid in ids_tsne_s], dtype=bool)\n",
    "            coords_valid = coords_u[valid_mask]\n",
    "            ids_valid = ids_tsne_s[valid_mask]\n",
    "\n",
    "            if len(ids_valid) > 0:\n",
    "                labels_plot = np.array(\n",
    "                    [labels_u_for_plot[pos_map_final[int(uid)]] for uid in ids_valid],\n",
    "                    dtype=int\n",
    "                )\n",
    "\n",
    "                for k in range(K_USERS):\n",
    "                    mk = (labels_plot == k)\n",
    "                    if np.any(mk):\n",
    "                        ax.scatter(coords_valid[mk, 0], coords_valid[mk, 1],\n",
    "                                   s=8, alpha=0.7, c=palette[k % len(palette)],\n",
    "                                   label=f\"Users: C{k}\")\n",
    "\n",
    "                if s == CLUSTER_STEP:\n",
    "                    for k, ctr in user_centers.items():\n",
    "                        ax.scatter(ctr[0], ctr[1], s=120, marker=\"X\",\n",
    "                                   c=palette[k % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        ax.set_title(f\"step{s} (Users & Items, colored by step{CLUSTER_STEP})\")\n",
    "        #ax.set_xlim(xmin - xpad, xmax + xpad)\n",
    "        #ax.set_ylim(ymin - ypad, ymax + ypad)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_USERS_ITEMS.png\")\n",
    "    fig.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "\n",
    "print(\"Plotting...\")\n",
    "plot_users_only()\n",
    "plot_items_only()\n",
    "plot_users_items()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352ce72",
   "metadata": {},
   "source": [
    "### Distance: Step1, Step5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2851d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from itertools import combinations\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_user_label_map(user_ids: np.ndarray, user_labels: np.ndarray) -> Dict[int, int]:\n",
    "    return {int(u): int(l) for u, l in zip(user_ids.tolist(), user_labels.tolist())}\n",
    "\n",
    "def get_labels_for_ids(target_ids: np.ndarray, label_map: Dict[int, int]) -> np.ndarray:\n",
    "    if len(target_ids) == 0:\n",
    "        return np.empty((0,), dtype=int)\n",
    "    return np.array([label_map.get(int(u), -1) for u in target_ids.tolist()], dtype=int)\n",
    "\n",
    "def calculate_centers(points: np.ndarray, labels: np.ndarray, k: int) -> Dict[int, Optional[np.ndarray]]:\n",
    "    centers = {}\n",
    "    for cluster_id in range(k):\n",
    "        mask = (labels == cluster_id)\n",
    "        centers[cluster_id] = points[mask].mean(axis=0) if np.any(mask) else None\n",
    "    return centers\n",
    "\n",
    "def calculate_distance(point_a: Optional[np.ndarray], point_b: Optional[np.ndarray]) -> float:\n",
    "    if point_a is None or point_b is None:\n",
    "        return np.nan\n",
    "    return float(np.linalg.norm(point_a - point_b))\n",
    "\n",
    "def get_pairwise_stats(centers: Dict[int, Optional[np.ndarray]]) -> Tuple[float, float, float]:\n",
    "    valid = [v for v in centers.values() if v is not None]\n",
    "    if len(valid) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dists = [np.linalg.norm(a - b) for a, b in combinations(valid, 2)]\n",
    "    return float(np.mean(dists)), float(np.min(dists)), float(np.max(dists))\n",
    "\n",
    "def get_user_item_stats(centers_u: Dict[int, Optional[np.ndarray]],\n",
    "                        centers_i: Dict[int, Optional[np.ndarray]]) -> Tuple[float, float, float]:\n",
    "    vec_u = [v for v in centers_u.values() if v is not None]\n",
    "    vec_i = [v for v in centers_i.values() if v is not None]\n",
    "    if not vec_u or not vec_i:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dists = [np.linalg.norm(u - i) for u in vec_u for i in vec_i]\n",
    "    return float(np.mean(dists)), float(np.min(dists)), float(np.max(dists))\n",
    "\n",
    "# =========================\n",
    "# Main (RAW ONLY) - Only first & last step\n",
    "# =========================\n",
    "\n",
    "# 0) ì‚¬ìš©í•  step: ì²«ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ë§Œ\n",
    "FIRST_STEP = STEPS[0]\n",
    "LAST_STEP = STEPS[-1]\n",
    "TARGET_STEPS = [FIRST_STEP, LAST_STEP]\n",
    "\n",
    "# 1) Prepare label map from final step clustering\n",
    "user_label_map_final = get_user_label_map(ids_final, user_labels_final)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for s in TARGET_STEPS:\n",
    "    step_data = {\"step\": s}\n",
    "\n",
    "    # --- A) RAW: User centers ---\n",
    "    ids_u_raw = aligned_users_by_step[s]\n",
    "    emb_u_raw = user_by_step[s][ids_u_raw, :] if len(ids_u_raw) > 0 else np.empty((0, user_by_step[s].shape[1]))\n",
    "    labels_u_raw = get_labels_for_ids(ids_u_raw, user_label_map_final)\n",
    "    centers_u_raw = calculate_centers(emb_u_raw, labels_u_raw, K_USERS)\n",
    "\n",
    "    # --- B) RAW: Item centers ---\n",
    "    # item_for_tsne[s]ë¥¼ raw item embeddingìœ¼ë¡œ ì“°ê³  ìˆì—ˆìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "    if item_for_tsne[s] is not None and item_labels_final is not None:\n",
    "        emb_i_raw = item_for_tsne[s]\n",
    "        centers_i_raw = calculate_centers(emb_i_raw, item_labels_final, K_ITEMS)\n",
    "    else:\n",
    "        centers_i_raw = {k: None for k in range(K_ITEMS)}\n",
    "\n",
    "    # --- C) Stats (RAW) ---\n",
    "    # User-User / Item-Item (raw)\n",
    "    step_data[\"user_dist_raw_mean\"], step_data[\"user_dist_raw_min\"], step_data[\"user_dist_raw_max\"] = get_pairwise_stats(centers_u_raw)\n",
    "    step_data[\"item_dist_raw_mean\"], step_data[\"item_dist_raw_min\"], step_data[\"item_dist_raw_max\"] = get_pairwise_stats(centers_i_raw)\n",
    "\n",
    "    # Detailed stats when K=2\n",
    "    if K_USERS == 2:\n",
    "        step_data[\"user_dist_raw_U0U1\"] = calculate_distance(centers_u_raw.get(0), centers_u_raw.get(1))\n",
    "    if K_ITEMS == 2:\n",
    "        step_data[\"item_dist_raw_I0I1\"] = calculate_distance(centers_i_raw.get(0), centers_i_raw.get(1))\n",
    "\n",
    "    # User-Item (raw)\n",
    "    if K_USERS == 2 and K_ITEMS == 2:\n",
    "        for u_idx in range(2):\n",
    "            for i_idx in range(2):\n",
    "                key_suffix = f\"U{u_idx}_I{i_idx}\"\n",
    "                step_data[f\"ui_raw_{key_suffix}\"] = calculate_distance(\n",
    "                    centers_u_raw.get(u_idx), centers_i_raw.get(i_idx)\n",
    "                )\n",
    "    else:\n",
    "        step_data[\"ui_raw_mean\"], step_data[\"ui_raw_min\"], step_data[\"ui_raw_max\"] = get_user_item_stats(centers_u_raw, centers_i_raw)\n",
    "\n",
    "    rows.append(step_data)\n",
    "\n",
    "# --- Save CSV (only 2 rows) ---\n",
    "csv_filename = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_RAW_center_distances_first_last.csv\")\n",
    "if rows:\n",
    "    with open(csv_filename, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"  - saved: {csv_filename}\")\n",
    "\n",
    "# =========================\n",
    "# Visualization (RAW, only first & last)\n",
    "# =========================\n",
    "\n",
    "steps_ax = [r[\"step\"] for r in rows]\n",
    "\n",
    "fig, (ax_top, ax_bottom) = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(8, 8), sharex=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# (Top) User-User & Item-Item (RAW)\n",
    "# -------------------------\n",
    "ax_top.plot(\n",
    "    steps_ax,\n",
    "    [r[\"user_dist_raw_mean\"] for r in rows],\n",
    "    marker=\"o\",\n",
    "    label=\"Users (raw, mean)\"\n",
    ")\n",
    "ax_top.plot(\n",
    "    steps_ax,\n",
    "    [r[\"item_dist_raw_mean\"] for r in rows],\n",
    "    marker=\"s\",\n",
    "    label=\"Items (raw, mean)\"\n",
    ")\n",
    "ax_top.set_ylabel(\"Center distance\")\n",
    "ax_top.set_title(\"User/User & Item/Item Center Distance (RAW) - First vs Last\")\n",
    "ax_top.legend()\n",
    "\n",
    "# -------------------------\n",
    "# (Bottom) User-Item (RAW)\n",
    "# -------------------------\n",
    "if K_USERS == 2 and K_ITEMS == 2:\n",
    "    keys = [\"U0_I0\", \"U0_I1\", \"U1_I0\", \"U1_I1\"]\n",
    "    for key_suffix in keys:\n",
    "        ax_bottom.plot(\n",
    "            steps_ax,\n",
    "            [r.get(f\"ui_raw_{key_suffix}\", np.nan) for r in rows],\n",
    "            marker=\"o\",\n",
    "            label=key_suffix.replace(\"_\", \"â€“\")\n",
    "        )\n",
    "    ax_bottom.legend(ncol=2)\n",
    "    title_suffix = \"(Detailed K=2)\"\n",
    "else:\n",
    "    for stat, mk in [(\"mean\", \"o\"), (\"min\", \"s\"), (\"max\", \"^\")]:\n",
    "        ax_bottom.plot(\n",
    "            steps_ax,\n",
    "            [r.get(f\"ui_raw_{stat}\", np.nan) for r in rows],\n",
    "            marker=mk,\n",
    "            label=stat\n",
    "        )\n",
    "    ax_bottom.legend()\n",
    "    title_suffix = \"(Summary)\"\n",
    "\n",
    "ax_bottom.set_xlabel(\"Step\")\n",
    "ax_bottom.set_ylabel(\"Center distance\")\n",
    "ax_bottom.set_title(f\"Userâ€“Item Center Distances (RAW) - First vs Last {title_suffix}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "out = os.path.join(\n",
    "    OUT_DIR,\n",
    "    f\"step{CLUSTER_STEP}_RAW_center_distances_first_last.png\"\n",
    ")\n",
    "plt.savefig(out, dpi=220)\n",
    "plt.close()\n",
    "\n",
    "print(f\"  - saved: {out}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec431f3",
   "metadata": {},
   "source": [
    "## FilterBubble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = f\"{BASE_DIR}/figs_tsne_cluster_progress/step4_center_distances_over_steps_RAW_ONLY.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# step0, step4ê°€ ì •í™•íˆ ìˆëŠ”ì§€ í™•ì¸ (ì—†ìœ¼ë©´ ê°€ì¥ ì‘ì€/í° step ì‚¬ìš©)\n",
    "step_min = int(df[\"step\"].min())\n",
    "step_max = int(df[\"step\"].max())\n",
    "\n",
    "# ë„¤ê°€ ì›í•˜ëŠ” ê²Œ step0/step4 ê³ ì •ì´ë©´ ì•„ë˜ ë‘ ì¤„ì„ ì“°ê³ ,\n",
    "# CSVì— ì—†ìœ¼ë©´ KeyError/ë¹ˆê²°ê³¼ ë‚  ìˆ˜ ìˆìŒ.\n",
    "s0 = 0\n",
    "s4 = 4\n",
    "if not ((df[\"step\"] == s0).any() and (df[\"step\"] == s4).any()):\n",
    "    print(f\"[warn] step {s0}/{s4}ê°€ CSVì— ì—†ì–´ì„œ {step_min}/{step_max}ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "    s0, s4 = step_min, step_max\n",
    "\n",
    "row0 = df.loc[df[\"step\"] == s0].iloc[0]\n",
    "row4 = df.loc[df[\"step\"] == s4].iloc[0]\n",
    "\n",
    "pairs = [\n",
    "    (\"U0\", \"I0\", \"ui_raw_U0_I0\"),\n",
    "    (\"U0\", \"I1\", \"ui_raw_U0_I1\"),\n",
    "    (\"U1\", \"I0\", \"ui_raw_U1_I0\"),\n",
    "    (\"U1\", \"I1\", \"ui_raw_U1_I1\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for u, i, col in pairs:\n",
    "    d0 = float(row0[col])\n",
    "    d4 = float(row4[col])\n",
    "    dec = d0 - d4   # +ë©´ ê°€ê¹Œì›Œì§, -ë©´ ë©€ì–´ì§\n",
    "    results.append({\"U\": u, \"I\": i, \"d_step0\": d0, \"d_step4\": d4, \"decrease\": dec})\n",
    "\n",
    "res = pd.DataFrame(results)\n",
    "\n",
    "# Uë³„ ìš”ì•½: I0/I1 ê°ì†ŒëŸ‰ ë¹„êµ ë° \"I0ìª½ìœ¼ë¡œ ë” ê°€ê¹Œì›Œì¡ŒëŠ”ì§€\" íŒë‹¨\n",
    "summary_rows = []\n",
    "for u in [\"U0\", \"U1\"]:\n",
    "    dec_i0 = float(res[(res[\"U\"] == u) & (res[\"I\"] == \"I0\")][\"decrease\"].iloc[0])\n",
    "    dec_i1 = float(res[(res[\"U\"] == u) & (res[\"I\"] == \"I1\")][\"decrease\"].iloc[0])\n",
    "    # ê°ì†ŒëŸ‰ ì°¨ì´(ì–‘ìˆ˜ë©´ I0ìª½ìœ¼ë¡œ ë” ê°€ê¹Œì›Œì§)\n",
    "    diff = dec_i0 - dec_i1\n",
    "    summary_rows.append({\n",
    "        \"U\": u,\n",
    "        \"step_start\": s0,\n",
    "        \"step_end\": s4,\n",
    "        \"decrease_to_I0\": dec_i0,\n",
    "        \"decrease_to_I1\": dec_i1,\n",
    "        \"diff(I0_minus_I1)\": diff,\n",
    "        \"moved_more_toward\": \"I0\" if diff > 0 else (\"I1\" if diff < 0 else \"Same\")\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "print(\"\\n[raw distances + decreases]\")\n",
    "print(res.to_string(index=False))\n",
    "\n",
    "print(\"\\n[summary: compare decreases]\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization\n",
    "# - U0, U1 ê°ê°ì—ì„œ (I0 ê°ì†ŒëŸ‰, I1 ê°ì†ŒëŸ‰) ë§‰ëŒ€ ë¹„êµ\n",
    "# ---------------------------\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "x = np.arange(2)  # U0, U1\n",
    "width = 0.35\n",
    "\n",
    "dec_i0s = summary[\"decrease_to_I0\"].values\n",
    "dec_i1s = summary[\"decrease_to_I1\"].values\n",
    "\n",
    "ax.bar(x - width/2, dec_i0s, width, label=\"Decrease to I0 (d_step0 - d_step4)\")\n",
    "ax.bar(x + width/2, dec_i1s, width, label=\"Decrease to I1 (d_step0 - d_step4)\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary[\"U\"].values)\n",
    "ax.set_ylabel(\"Distance decrease (+ means closer)\")\n",
    "ax.set_title(f\"Uâ€“I distance decrease from step{s0} to step{s4}\")\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "out_png = os.path.join(os.path.dirname(csv_path), f\"p{s0}_to_p{s4}_U_to_I_distance_decrease.png\")\n",
    "plt.savefig(out_png, dpi=220)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved plot ->\", out_png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1da77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allmrec-openblas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
