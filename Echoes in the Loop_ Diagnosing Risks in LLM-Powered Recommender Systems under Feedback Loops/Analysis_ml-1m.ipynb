{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec253d4",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80125a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import gzip, pickle\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a930cb4",
   "metadata": {},
   "source": [
    "## Path settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc12187",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset\n",
    "ITEM_ATTR_CSV = os.path.join(\"data/ml-1m/ml-1m_llmrec_format\", \"item_attribute.csv\")\n",
    "DATA_BASE_DIR = \"data/ml-1m\"\n",
    "U_ITEM_PATH   = f\"data/ml-1m/5core/item_meta_2017_kcore5_user_item.json\"\n",
    "META_GZ_PATH  = f\"{DATA_BASE_DIR}/ml-1m_text_name_dict.json.gz\"\n",
    "PARTS = 5\n",
    "\n",
    "test_path = os.path.join(DATA_BASE_DIR, \"label.txt\")\n",
    "train_path = os.path.join(DATA_BASE_DIR, \"train.txt\")\n",
    "\n",
    "\n",
    "######################### A-LLMRec #########################\n",
    "BASE_DIR = f\"data/ml-1m/A-LLMRec_format/A-LLMRec_results\"\n",
    "PRED_P1 = f\"{BASE_DIR}/predict_label_part1.json\"\n",
    "PRED_P5 = f\"{BASE_DIR}/predict_label_part5.json\"\n",
    "\n",
    "\n",
    "######################### LLMRec #########################\n",
    "BASE_DIR = \"data/ml-1m/ml-1m_llmrec_format\"\n",
    "PRED_P1    = f\"{BASE_DIR}/predict_label_part1.json\"\n",
    "PRED_P5    = f\"{BASE_DIR}/predict_label_part5.json\"\n",
    "\n",
    "\n",
    "# # ###################### Augmentation #######################\n",
    "# BASE_DIR = f\"data/ml-1m/Augmentation_format\"\n",
    "# PRED_P1    = f\"{BASE_DIR}/predict_label_part1.json\"\n",
    "# PRED_P5    = f\"{BASE_DIR}/predict_label_part5.json\"\n",
    "\n",
    "\n",
    "###################### Traditional CF ######################\n",
    "# model = \"LightGCN\"  # \"MF-BPR\" or \"LightGCN\"\n",
    "# BASE_DIR = f\"data/ml-1m/traditionalCF\"\n",
    "# PRED_P1    = f\"{BASE_DIR}/predict_label_part1.json\"\n",
    "# PRED_P5    = f\"{BASE_DIR}/predict_label_part5.json\"\n",
    "\n",
    "print(PRED_P5)\n",
    "with open(PRED_P5) as f:\n",
    "    pred_dict = json.load(f)\n",
    "print(pred_dict[\"0\"][:30])  # Top 10 predicted item IDs for user 0\n",
    "\n",
    "actual = sum(len(v) for v in pred_dict.values())\n",
    "print(\"actual predict_label interactions:\", actual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971101cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "with open(train_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        u_str, i_str, _ = line.split()\n",
    "        u = str(u_str)         # Key is a string\n",
    "        i = int(i_str)         # Items can be set as integers (or strings).\n",
    "\n",
    "        if u not in train_data:\n",
    "            train_data[u] = []\n",
    "        train_data[u].append(i)\n",
    "\n",
    "ground_truth = {}\n",
    "with open(test_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        u_str, i_str, _ = line.split()\n",
    "        u = str(u_str)\n",
    "        i = int(i_str)\n",
    "        # If there are multiple ground truths, append to the list. If there is only one, overwrite or write only the first one.\n",
    "        if u not in ground_truth:\n",
    "            ground_truth[u] = []\n",
    "        ground_truth[u].append(i)\n",
    "\n",
    "# common user\n",
    "common_users = set(train_data.keys()) & set(ground_truth.keys())\n",
    "print(len(train_data.keys()))\n",
    "print(len(ground_truth.keys()))\n",
    "print(len(common_users))\n",
    "\n",
    "# Find the user with the longest record in the ground truth, check the record length of that user\n",
    "max_len_user = max(ground_truth.items(), key=lambda x: len(x[1]))\n",
    "print(f\"User with the longest ground truth interaction: {max_len_user[0]}, Length: {len(max_len_user[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e199ba63",
   "metadata": {},
   "source": [
    "### Multi-hot-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# -----------------------------\n",
    "# 1) train_data -> multi-hot (sparse CSR)\n",
    "# -----------------------------\n",
    "# train_data: { \"user_id(str)\": [item_id(int), ...], ... }\n",
    "\n",
    "users = list(train_data.keys())\n",
    "random_seed = 42\n",
    "\n",
    "# Collect item universe (full item id)\n",
    "all_items = set()\n",
    "for u in users:\n",
    "    all_items.update(train_data[u])#[-10:])\n",
    "\n",
    "# The item_id may not be continuous as 0..N-1 -> It may be a cold item.\n",
    "item_list = sorted(all_items)\n",
    "item2col = {item: idx for idx, item in enumerate(item_list)}\n",
    "\n",
    "n_users = len(users)\n",
    "n_items = len(item_list)\n",
    "\n",
    "# Create CSR: 1 at position (row=user, col=item)\n",
    "rows = []\n",
    "cols = []\n",
    "data = []\n",
    "for r, u in enumerate(users):\n",
    "    # Duplicate interactions are treated as 1 (multi-hot)\n",
    "    seen_list = train_data[u]#[-10:] \n",
    "    seen = set(seen_list)\n",
    "    for item in seen:\n",
    "        rows.append(r)\n",
    "        cols.append(item2col[item])\n",
    "        data.append(1)\n",
    "\n",
    "X = csr_matrix((data, (rows, cols)), shape=(n_users, n_items), dtype=np.float32)\n",
    "print(\"X shape:\", X.shape, \"nnz:\", X.nnz)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) High-dimensional sparse vector -> low-dimensional embedding (SVD)\n",
    "# -----------------------------\n",
    "# KMeans can be run directly on high-dimensional sparsity, but\n",
    "# Usually, it is much more stable if you reduce it to about 50 to 200 dimensions with SVD.\n",
    "svd_dim = min(100, n_items - 1) if n_items > 1 else 1\n",
    "svd = TruncatedSVD(n_components=svd_dim, random_state=random_seed)\n",
    "X_svd = svd.fit_transform(X)\n",
    "print(\"X_svd shape:\", X_svd.shape, \"explained_var_ratio_sum:\", svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# -----------------------------\n",
    "# 3) K-means\n",
    "# -----------------------------\n",
    "k = 2  # Change to desired number of clusters\n",
    "kmeans = KMeans(n_clusters=k, n_init=10, random_state=random_seed)\n",
    "labels = kmeans.fit_predict(X_svd)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Output number of users per cluster\n",
    "# -----------------------------\n",
    "cluster_cnt = Counter(labels)\n",
    "\n",
    "print(f\"\\nTotal users: {sum(cluster_cnt.values())}\")\n",
    "\n",
    "print(\"\\nUsers per cluster (%)\")\n",
    "for c in sorted(cluster_cnt.keys()):\n",
    "    ratio = cluster_cnt[c] / len(labels) * 100\n",
    "    print(f\"Cluster {c}: {cluster_cnt[c]} users ({ratio:.2f}%)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Create 2D visualization coordinates (t-SNE)\n",
    "# -----------------------------\n",
    "# If the number of users is very large (tens of thousands or more), t-SNE can be slow -> it is recommended to use the subsample option below.\n",
    "use_subsample = False\n",
    "max_points = 5000\n",
    "\n",
    "if use_subsample and n_users > max_points:\n",
    "    rng = np.random.RandomState(random_seed)\n",
    "    idx = rng.choice(n_users, size=max_points, replace=False)\n",
    "    X_vis_in = X_svd[idx]\n",
    "    labels_vis = labels[idx]\n",
    "else:\n",
    "    X_vis_in = X_svd\n",
    "    labels_vis = labels\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=30,\n",
    "    learning_rate=\"auto\",\n",
    "    init=\"pca\",\n",
    "    random_state=random_seed,\n",
    ")\n",
    "Z = tsne.fit_transform(X_vis_in)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(Z[:, 0], Z[:, 1], c=labels_vis, s=10)\n",
    "plt.title(f\"User Multi-hot -> SVD({svd_dim}) -> KMeans(k={k}) -> t-SNE(2D)\")\n",
    "plt.xlabel(\"t-SNE dim1\")\n",
    "plt.ylabel(\"t-SNE dim2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb29562",
   "metadata": {},
   "source": [
    "# RQ1\n",
    "Analysis of bias/hallucination phenomenon of data generated by LLM in recommendation pipeline\n",
    "\n",
    "Includes: LLMRec, Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f525e",
   "metadata": {},
   "source": [
    "## LLMRec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15adacf0",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "\n",
    "SAVE_DIR = os.path.join(BASE_DIR, \"result_analysis\")\n",
    "save_dir = os.path.join(SAVE_DIR, \"poster\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "USER_INFO_TXT = \"data/ml-1m/user_info.txt\"\n",
    "\n",
    "# =========================\n",
    "# Parsers\n",
    "# =========================\n",
    "def parse_genres(genre_raw):\n",
    "    \"\"\"\n",
    "    Convert genre strings in various formats to a list\n",
    "    1) \"['Action', 'Comedy']\"\n",
    "    2) \"Action|Comedy\" (ML-1M original style)\n",
    "    3) \"Action, Comedy\"\n",
    "    4) \"Action\"\n",
    "    \"\"\"\n",
    "    if pd.isna(genre_raw):\n",
    "        return []\n",
    "\n",
    "    genre_str = str(genre_raw).strip()\n",
    "    if not genre_str:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        if genre_str.startswith(\"[\") and genre_str.endswith(\"]\"):\n",
    "            parsed = ast.literal_eval(genre_str)\n",
    "            return [str(g).strip() for g in parsed if str(g).strip()]\n",
    "        elif \"|\" in genre_str:\n",
    "            return [g.strip() for g in genre_str.split(\"|\") if g.strip()]\n",
    "        elif \",\" in genre_str:\n",
    "            return [g.strip() for g in genre_str.split(\",\") if g.strip()]\n",
    "        else:\n",
    "            return [genre_str]\n",
    "    except Exception:\n",
    "        # When parsing fails, the original text is treated as a single genre.\n",
    "        return [genre_str]\n",
    "\n",
    "# (Optional) ML-1M Occupation Code -> Label (use if desired)\n",
    "OCCUPATION_MAP = {\n",
    "    0: \"other\",\n",
    "    1: \"academic/educator\",\n",
    "    2: \"artist\",\n",
    "    3: \"clerical/admin\",\n",
    "    4: \"college/grad student\",\n",
    "    5: \"customer service\",\n",
    "    6: \"doctor/health care\",\n",
    "    7: \"executive/managerial\",\n",
    "    8: \"farmer\",\n",
    "    9: \"homemaker\",\n",
    "    10: \"K-12 student\",\n",
    "    11: \"lawyer\",\n",
    "    12: \"programmer\",\n",
    "    13: \"retired\",\n",
    "    14: \"sales/marketing\",\n",
    "    15: \"scientist\",\n",
    "    16: \"self-employed\",\n",
    "    17: \"technician/engineer\",\n",
    "    18: \"tradesman/craftsman\",\n",
    "    19: \"unemployed\",\n",
    "    20: \"writer\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# Generic plotting utilities\n",
    "# =========================\n",
    "def counter_to_df(counter: Counter, key_name: str):\n",
    "    df = pd.DataFrame(counter.items(), columns=[key_name, \"Count\"])\n",
    "    df = df.sort_values(\"Count\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def plot_and_save_barh(\n",
    "    df: pd.DataFrame,\n",
    "    category_col: str,\n",
    "    count_col: str,\n",
    "    title: str,\n",
    "    save_path: str,\n",
    "    top_k: int = 20,\n",
    "    color: str = \"cornflowerblue\",\n",
    "):\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è Empty dataframe: {title}\")\n",
    "        return\n",
    "\n",
    "    df_plot = df.head(top_k).copy() if len(df) > top_k else df.copy()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(\n",
    "        df_plot[category_col][::-1],\n",
    "        df_plot[count_col][::-1],\n",
    "        color=color,\n",
    "        edgecolor=\"black\",\n",
    "        alpha=0.9,\n",
    "    )\n",
    "\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(\n",
    "            width + (width * 0.01),\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            f\"{int(width)}\",\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.title(title, fontsize=15, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Count\", fontsize=12)\n",
    "    plt.ylabel(category_col, fontsize=12)\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 1) Genre distribution\n",
    "# =========================\n",
    "def load_item_attributes(item_attr_csv: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(item_attr_csv):\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {item_attr_csv}\")\n",
    "    df = pd.read_csv(item_attr_csv, names=[\"id\", \"year\", \"title\", \"genre\"], header=None)\n",
    "    return df\n",
    "\n",
    "def compute_genre_distribution_df(item_attr_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    all_genres = []\n",
    "    for genre_raw in item_attr_df[\"genre\"]:\n",
    "        all_genres.extend(parse_genres(genre_raw))\n",
    "    counter = Counter(all_genres)\n",
    "    return counter_to_df(counter, \"Genre\")\n",
    "\n",
    "def save_genre_distribution(\n",
    "    item_attr_csv: str,\n",
    "    save_dir: str,\n",
    "    top_k: int = 20,\n",
    "):\n",
    "    item_df = load_item_attributes(item_attr_csv)\n",
    "    genre_df = compute_genre_distribution_df(item_df)\n",
    "\n",
    "    save_path = os.path.join(save_dir, \"Real_Genre_Distribution_ML1M.png\")\n",
    "    title = f\"Distribution of Movie Genres (Unique: {genre_df['Genre'].nunique()}, Total tags: {genre_df['Count'].sum()})\"\n",
    "    plot_and_save_barh(\n",
    "        df=genre_df,\n",
    "        category_col=\"Genre\",\n",
    "        count_col=\"Count\",\n",
    "        title=title,\n",
    "        save_path=save_path,\n",
    "        top_k=top_k,\n",
    "    )\n",
    "\n",
    "    print(\"\\nüèÜ Top 5 Genres:\")\n",
    "    print(genre_df.head(5))\n",
    "\n",
    "# =========================\n",
    "# 2) Occupation distribution\n",
    "# =========================\n",
    "def load_user_info(user_info_txt: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(user_info_txt):\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {user_info_txt}\")\n",
    "\n",
    "    # Includes header + possibility of mixing spaces/tabs -> regex delimiter\n",
    "    df = pd.read_csv(user_info_txt, sep=r\"\\s+\", engine=\"python\")\n",
    "    # Expected Column: new_id Gender Age Occupation ZipCode\n",
    "    expected = {\"new_id\", \"Gender\", \"Age\", \"Occupation\", \"ZipCode\"}\n",
    "    if not expected.issubset(set(df.columns)):\n",
    "        raise ValueError(f\"‚ùå Unexpected columns: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "def compute_occupation_distribution_df(user_df: pd.DataFrame, use_label: bool = True) -> pd.DataFrame:\n",
    "    occ_series = user_df[\"Occupation\"].astype(int)\n",
    "\n",
    "    if use_label:\n",
    "        occ_name = occ_series.map(lambda x: OCCUPATION_MAP.get(int(x), f\"occ_{int(x)}\"))\n",
    "        counter = Counter(occ_name.tolist())\n",
    "        return counter_to_df(counter, \"Occupation\")\n",
    "    else:\n",
    "        counter = Counter(occ_series.tolist())\n",
    "        df = counter_to_df(counter, \"Occupation\")\n",
    "        df[\"Occupation\"] = df[\"Occupation\"].astype(str)\n",
    "        return df\n",
    "\n",
    "def save_occupation_distribution(\n",
    "    user_info_txt: str,\n",
    "    save_dir: str,\n",
    "    top_k: int = 20,\n",
    "    use_label: bool = True,\n",
    "):\n",
    "    user_df = load_user_info(user_info_txt)\n",
    "    occ_df = compute_occupation_distribution_df(user_df, use_label=use_label)\n",
    "\n",
    "    save_path = os.path.join(save_dir, \"Real_Occupation_Distribution_ML1M.png\")\n",
    "    title = f\"Distribution of User Occupations (Unique: {occ_df['Occupation'].nunique()}, Total users: {occ_df['Count'].sum()})\"\n",
    "    plot_and_save_barh(\n",
    "        df=occ_df,\n",
    "        category_col=\"Occupation\",\n",
    "        count_col=\"Count\",\n",
    "        title=title,\n",
    "        save_path=save_path,\n",
    "        top_k=top_k,\n",
    "        color=\"lightseagreen\",\n",
    "    )\n",
    "\n",
    "    print(\"\\nüèÜ Top 5 Occupations:\")\n",
    "    print(occ_df.head(5))\n",
    "\n",
    "# =========================\n",
    "# Run\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    save_genre_distribution(ITEM_ATTR_CSV, save_dir, top_k=20)\n",
    "    save_occupation_distribution(USER_INFO_TXT, save_dir, top_k=20, use_label=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad9318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ‚úÖ Route settings\n",
    "file_path = \"data/ml-1m/ml-1m_llmrec_format/augmented_user_profiling_dict_part1_step0\"\n",
    "\n",
    "# ‚úÖ 1. Redefine the key to be analyzed (based on screenshot)\n",
    "# Set based on the keys in the screenshot.\n",
    "target_keys = [\n",
    "    'age', 'gender', 'occupation',\n",
    "    'country', 'language',\n",
    "    'liked_genre', 'disliked_genre', \n",
    "    'liked_directors', 'liked_actors' # Actors are not in the screenshot, but they usually exist in pairs, so add them (remove them if not needed).\n",
    "]\n",
    "\n",
    "# Initialize dictionary to store data\n",
    "data_storage = {key: [] for key in target_keys}\n",
    "\n",
    "# ‚úÖ Load data\n",
    "if os.path.exists(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        augmented_dict = pickle.load(f)\n",
    "    print(f\"‚úÖ ÌååÏùº Î°úÎìú ÏÑ±Í≥µ: {file_path} (Ï¥ù {len(augmented_dict)}Î™Ö)\")\n",
    "else:\n",
    "    print(f\"‚ùå ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {file_path}\")\n",
    "    augmented_dict = {}\n",
    "\n",
    "# ‚úÖ Data parsing and cleansing functions\n",
    "def clean_and_parse(profile_text):\n",
    "    # text tablet\n",
    "    cleaned = profile_text.strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace('```json', '').replace('```', '')\n",
    "    cleaned = cleaned.replace('\\n', ' ').replace('\\r', '')\n",
    "    \n",
    "    # Fix common JSON errors (e.g. Children's -> Childrens)\n",
    "    cleaned = re.sub(r\"(\\w+)['‚Äô]s\", r\"\\1s\", cleaned) \n",
    "\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            return ast.literal_eval(cleaned)\n",
    "        except:\n",
    "            # Attempt to force quote conversion\n",
    "            try:\n",
    "                cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "                return json.loads(cleaned_forced)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "# ‚úÖ Key normalization function (key fixes)\n",
    "def normalize_keys(profile):\n",
    "    \"\"\"\n",
    "    Unify the various key names generated by LLM into the standard format of screenshots (underbar, plural).\n",
    "    Example: 'Liked Genre' -> 'liked_genre', 'liked director' -> 'liked_directors'\n",
    "    \"\"\"\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        # 1. Convert lowercase letters and change spaces to underscores\n",
    "        new_key = k.lower().strip().replace(' ', '_')\n",
    "        \n",
    "        # 2. Singular/plural and exception handling mapping\n",
    "        if new_key == 'liked_director': new_key = 'liked_directors'\n",
    "        if new_key == 'liked_actor': new_key = 'liked_actors'\n",
    "        if new_key == 'disliked_director': new_key = 'disliked_directors'\n",
    "        \n",
    "        new_profile[new_key] = v\n",
    "    return new_profile\n",
    "\n",
    "# ‚úÖ Main loop: data extraction\n",
    "print(\"\\U0001f504 Parsing and extracting data...\")\n",
    "success_count = 0\n",
    "\n",
    "for user_id, profile_text in augmented_dict.items():\n",
    "    raw_profile = clean_and_parse(profile_text)\n",
    "    \n",
    "    if not raw_profile:\n",
    "        continue\n",
    "    \n",
    "    # ‚≠êÔ∏è Apply key normalization (adapted to screenshot format)\n",
    "    profile = normalize_keys(raw_profile)\n",
    "    success_count += 1\n",
    "\n",
    "    for key in target_keys:\n",
    "        val = profile.get(key, None)\n",
    "        \n",
    "        if val:\n",
    "            # 1) In case of a list\n",
    "            if isinstance(val, list):\n",
    "                cleaned_items = [str(item).strip().title() for item in val if item]\n",
    "                data_storage[key].extend(cleaned_items)\n",
    "            \n",
    "            # 2) If it is a string (process strings separated by commas as in the screenshot)\n",
    "            elif isinstance(val, str):\n",
    "                val = val.strip()\n",
    "                if val.lower() in ['unknown', 'none', 'n/a', '', 'null']:\n",
    "                    continue\n",
    "                \n",
    "                # Handles comma separation (e.g. \"Krzysztof Kie≈õlowski, George Miller\")\n",
    "                if ',' in val:\n",
    "                    cleaned_items = [item.strip().title() for item in val.split(',')]\n",
    "                    data_storage[key].extend(cleaned_items)\n",
    "                else:\n",
    "                    # Gender processing\n",
    "                    if key == 'gender':\n",
    "                        g = val.lower()\n",
    "                        if g.startswith('m'): data_storage[key].append('Male')\n",
    "                        elif g.startswith('f'): data_storage[key].append('Female')\n",
    "                        else: data_storage[key].append(val.title())\n",
    "                    else:\n",
    "                        data_storage[key].append(val.title())\n",
    "\n",
    "print(f\"‚úÖ Ï≤òÎ¶¨ ÏôÑÎ£å: {success_count}Î™Ö ÌååÏã± ÏÑ±Í≥µ\")\n",
    "\n",
    "# ‚úÖ Visualization functions\n",
    "def plot_distribution(category_name, data_list, top_k=10, color='mediumseagreen', title_alias=None):\n",
    "    if not data_list:\n",
    "        print(f\"‚ö†Ô∏è {category_name} - Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "        return\n",
    "\n",
    "    # Set the title to display on screen (e.g. liked_genre -> Liked Genre)\n",
    "    display_title = title_alias if title_alias else category_name.replace('_', ' ').title()\n",
    "\n",
    "    counter = Counter(data_list)\n",
    "    df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count']).reset_index()\n",
    "    df.rename(columns={'index': 'Category'}, inplace=True)\n",
    "    \n",
    "    df_topk = df.sort_values(by='Count', ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(df_topk['Category'], df_topk['Count'], color=color, edgecolor='black', alpha=0.8)\n",
    "\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, yval + (yval * 0.01), int(yval), \n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    plt.title(f\"User Profile: {display_title} (Top {top_k})\", fontsize=15, fontweight='bold')\n",
    "    plt.xlabel(display_title, fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = f\"Profile_{category_name}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"üìä Ï†ÄÏû• ÏôÑÎ£å: {save_path}\")\n",
    "    # plt.show()\n",
    "\n",
    "# ‚úÖ Execution: Visualization for each key\n",
    "print(\"\\n========== Start visualization ==========\")\n",
    "\n",
    "# 1. Demographics and basic information\n",
    "plot_distribution('age', data_storage['age'], top_k=15, color='cornflowerblue')\n",
    "plot_distribution('gender', data_storage['gender'], top_k=5, color='lightcoral')\n",
    "plot_distribution('occupation', data_storage['occupation'], top_k=15, color='teal') # New\n",
    "plot_distribution('country', data_storage['country'], top_k=15, color='orchid')     # New\n",
    "plot_distribution('language', data_storage['language'], top_k=10, color='peru')     # New\n",
    "\n",
    "# 2. Taste information (using Underscore Key)\n",
    "plot_distribution('liked_genre', data_storage['liked_genre'], top_k=15, color='mediumseagreen')\n",
    "plot_distribution('disliked_genre', data_storage['disliked_genre'], top_k=15, color='salmon')\n",
    "\n",
    "# 3. Favorite person\n",
    "plot_distribution('liked_directors', data_storage['liked_directors'], top_k=15, color='goldenrod')\n",
    "# Print if there is actor data\n",
    "if data_storage['liked_actors']:\n",
    "    plot_distribution('liked_actors', data_storage['liked_actors'], top_k=15, color='mediumpurple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f8858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================================================\n",
    "# Paths (ML-1M)\n",
    "# =========================================================\n",
    "BASE_DIR = \"data/ml-1m/ml-1m_llmrec_format/\"\n",
    "steps = \"part5_step0\"\n",
    "\n",
    "AUG_PATH = os.path.join(BASE_DIR, f\"augmented_user_profiling_dict_{steps}_try0\")       # pickle\n",
    "USER_INFO_PATH = \"data/ml-1m/user_info.txt\"                # header available\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# Occupation mapping (code -> label)\n",
    "# =========================================================\n",
    "OCCUPATION_MAP = {\n",
    "    0:  \"other or not specified\",\n",
    "    1:  \"academic/educator\",\n",
    "    2:  \"artist\",\n",
    "    3:  \"clerical/admin\",\n",
    "    4:  \"college/grad student\",\n",
    "    5:  \"customer service\",\n",
    "    6:  \"doctor/health care\",\n",
    "    7:  \"executive/managerial\",\n",
    "    8:  \"farmer\",\n",
    "    9:  \"homemaker\",\n",
    "    10: \"K-12 student\",\n",
    "    11: \"lawyer\",\n",
    "    12: \"programmer\",\n",
    "    13: \"retired\",\n",
    "    14: \"sales/marketing\",\n",
    "    15: \"scientist\",\n",
    "    16: \"self-employed\",\n",
    "    17: \"technician/engineer\",\n",
    "    18: \"tradesman/craftsman\",\n",
    "    19: \"unemployed\",\n",
    "    20: \"writer\",\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# Age mapping (ML-1M buckets -> range label)\n",
    "# =========================================================\n",
    "AGE_RANGE_MAP = {\n",
    "    1:  \"Under 18\",\n",
    "    18: \"18-24\",\n",
    "    25: \"25-34\",\n",
    "    35: \"35-44\",\n",
    "    45: \"45-49\",\n",
    "    50: \"50-55\",\n",
    "    56: \"56+\",\n",
    "}\n",
    "\n",
    "# =========================================================\n",
    "# Load real user info\n",
    "# =========================================================\n",
    "user_df = pd.read_csv(USER_INFO_PATH, sep=r\"\\s+\", engine=\"python\")\n",
    "needed_cols = {\"new_id\", \"Gender\", \"Age\", \"Occupation\"}\n",
    "if not needed_cols.issubset(set(user_df.columns)):\n",
    "    raise ValueError(f\"Unexpected columns: {user_df.columns.tolist()}\")\n",
    "\n",
    "user_df[\"new_id\"] = user_df[\"new_id\"].astype(int)\n",
    "user_df = user_df.set_index(\"new_id\")\n",
    "\n",
    "# occupation code -> label\n",
    "user_df[\"Occupation\"] = pd.to_numeric(user_df[\"Occupation\"], errors=\"coerce\").astype(\"Int64\")\n",
    "user_df[\"OccupationLabel\"] = user_df[\"Occupation\"].map(\n",
    "    lambda x: OCCUPATION_MAP.get(int(x), \"unknown\") if pd.notna(x) else \"unknown\"\n",
    ")\n",
    "\n",
    "# age bucket code -> range label\n",
    "user_df[\"Age\"] = pd.to_numeric(user_df[\"Age\"], errors=\"coerce\").astype(\"Int64\")\n",
    "user_df[\"AgeRange\"] = user_df[\"Age\"].map(\n",
    "    lambda x: AGE_RANGE_MAP.get(int(x), \"unknown\") if pd.notna(x) else \"unknown\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded user_info: {len(user_df)} users\")\n",
    "\n",
    "# =========================================================\n",
    "# Load augmented profiling dict (pickle)\n",
    "# =========================================================\n",
    "if not os.path.exists(AUG_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {AUG_PATH}\")\n",
    "\n",
    "with open(AUG_PATH, \"rb\") as f:\n",
    "    augmented_dict = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded augmented dict: {len(augmented_dict)} users\")\n",
    "\n",
    "# =========================================================\n",
    "# Parse helpers\n",
    "# =========================================================\n",
    "def parse_profile_text(profile_text: str):\n",
    "    \"\"\"Parse and return dict format (JSON-like) from profile_text\"\"\"\n",
    "    if profile_text is None:\n",
    "        return None\n",
    "\n",
    "    cleaned = str(profile_text).strip()\n",
    "    cleaned = (\n",
    "        cleaned.replace(\"```json\", \"\")\n",
    "               .replace(\"```\", \"\")\n",
    "               .replace(\"'''\", \"\")\n",
    "               .replace(\"\\n\", \"\")\n",
    "               .replace(\"\\r\", \"\")\n",
    "    )\n",
    "\n",
    "    # Try json.loads (single quote -> double quote)\n",
    "    try:\n",
    "        cleaned_json = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        obj = json.loads(cleaned_json)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ast literal eval\n",
    "    try:\n",
    "        obj = ast.literal_eval(cleaned)\n",
    "        if isinstance(obj, dict):\n",
    "            return obj\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    return None\n",
    "\n",
    "def norm_gender(x):\n",
    "    s = str(x).strip().lower()\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    if s[0] in [\"m\", \"f\"]:\n",
    "        return s[0]\n",
    "    return s\n",
    "\n",
    "def norm_occ_text(x):\n",
    "    \"\"\"\n",
    "    Normalize predicted occupations to the OCCUPATION_MAP label space as much as possible\n",
    "    \"\"\"\n",
    "    s = str(x).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    # Check exact match (labels lower)\n",
    "    for v in OCCUPATION_MAP.values():\n",
    "        if s == v.lower():\n",
    "            return v\n",
    "\n",
    "    alias = {\n",
    "        \"other\": \"other or not specified\",\n",
    "        \"not specified\": \"other or not specified\",\n",
    "        \"unspecified\": \"other or not specified\",\n",
    "        \"educator\": \"academic/educator\",\n",
    "        \"academic\": \"academic/educator\",\n",
    "        \"teacher\": \"academic/educator\",\n",
    "        \"student\": \"college/grad student\",\n",
    "        \"grad student\": \"college/grad student\",\n",
    "        \"college student\": \"college/grad student\",\n",
    "        \"engineer\": \"technician/engineer\",\n",
    "        \"technician\": \"technician/engineer\",\n",
    "        \"healthcare\": \"doctor/health care\",\n",
    "        \"health care\": \"doctor/health care\",\n",
    "        \"doctor\": \"doctor/health care\",\n",
    "        \"physician\": \"doctor/health care\",\n",
    "        \"sales\": \"sales/marketing\",\n",
    "        \"marketing\": \"sales/marketing\",\n",
    "        \"manager\": \"executive/managerial\",\n",
    "        \"executive\": \"executive/managerial\",\n",
    "        \"self employed\": \"self-employed\",\n",
    "        \"self-employed\": \"self-employed\",\n",
    "        \"craftsman\": \"tradesman/craftsman\",\n",
    "        \"tradesman\": \"tradesman/craftsman\",\n",
    "        \"k12\": \"K-12 student\",\n",
    "        \"k-12\": \"K-12 student\",\n",
    "        \"writer\": \"writer\",\n",
    "        \"programmer\": \"programmer\",\n",
    "        \"software developer\": \"programmer\",\n",
    "        \"developer\": \"programmer\",\n",
    "        \"scientist\": \"scientist\",\n",
    "        \"lawyer\": \"lawyer\",\n",
    "        \"artist\": \"artist\",\n",
    "        \"clerical\": \"clerical/admin\",\n",
    "        \"admin\": \"clerical/admin\",\n",
    "        \"administrator\": \"clerical/admin\",\n",
    "        \"homemaker\": \"homemaker\",\n",
    "        \"unemployed\": \"unemployed\",\n",
    "        \"retired\": \"retired\",\n",
    "        \"farmer\": \"farmer\",\n",
    "        \"customer service\": \"customer service\",\n",
    "    }\n",
    "\n",
    "    if s in alias:\n",
    "        return alias[s]\n",
    "\n",
    "    # Partially inclusive matching (conservatively)\n",
    "    for key, val in alias.items():\n",
    "        if key in s:\n",
    "            return val\n",
    "\n",
    "    return s if s else \"unknown\"\n",
    "\n",
    "def map_pred_age(age_pred):\n",
    "    \"\"\"\n",
    "    Mapping the age (number/text) from LLM to ML-1M age range\n",
    "    \"\"\"\n",
    "    if age_pred is None:\n",
    "        return \"unknown\"\n",
    "    s = str(age_pred).strip().lower()\n",
    "    if not s:\n",
    "        return \"unknown\"\n",
    "\n",
    "    nums = re.findall(r\"\\d+\", s)\n",
    "    if nums:\n",
    "        age = int(nums[0])\n",
    "        if age < 18:   return \"Under 18\"\n",
    "        if age <= 24:  return \"18-24\"\n",
    "        if age <= 34:  return \"25-34\"\n",
    "        if age <= 44:  return \"35-44\"\n",
    "        if age <= 49:  return \"45-49\"\n",
    "        if age <= 55:  return \"50-55\"\n",
    "        return \"56+\"\n",
    "\n",
    "    # Based on keywords when there are no numbers\n",
    "    if any(k in s for k in [\"teen\", \"teenager\", \"under 18\", \"minor\"]):\n",
    "        return \"Under 18\"\n",
    "    if any(k in s for k in [\"18-24\", \"college age\", \"early 20\", \"twenty\"]):\n",
    "        return \"18-24\"\n",
    "    if any(k in s for k in [\"25-34\", \"late 20\", \"early 30\", \"thirty\"]):\n",
    "        return \"25-34\"\n",
    "    if any(k in s for k in [\"35-44\", \"late 30\", \"early 40\", \"forty\"]):\n",
    "        return \"35-44\"\n",
    "    if any(k in s for k in [\"45-49\", \"late 40\"]):\n",
    "        return \"45-49\"\n",
    "    if any(k in s for k in [\"50-55\", \"early 50\", \"mid 50\"]):\n",
    "        return \"50-55\"\n",
    "    if any(k in s for k in [\"56+\", \"over 55\", \"senior\", \"elder\", \"older\"]):\n",
    "        return \"56+\"\n",
    "\n",
    "    return \"unknown\"\n",
    "\n",
    "# =========================================================\n",
    "# Collect aligned real vs pred lists\n",
    "# =========================================================\n",
    "real_gender, pred_gender = [], []\n",
    "real_age, pred_age = [], []\n",
    "real_occ, pred_occ = [], []\n",
    "\n",
    "fail_parse = 0\n",
    "missing_real = 0\n",
    "\n",
    "for uid_raw, profile_text in augmented_dict.items():\n",
    "    try:\n",
    "        uid = int(uid_raw)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    if uid not in user_df.index:\n",
    "        missing_real += 1\n",
    "        continue\n",
    "\n",
    "    prof = parse_profile_text(profile_text)\n",
    "    if prof is None:\n",
    "        fail_parse += 1\n",
    "        continue\n",
    "\n",
    "    # predicted\n",
    "    p_gender = norm_gender(prof.get(\"gender\", \"\"))\n",
    "    p_age = map_pred_age(prof.get(\"age\", \"\"))\n",
    "    p_occ = norm_occ_text(prof.get(\"occupation\", \"\"))\n",
    "\n",
    "    # real\n",
    "    row = user_df.loc[uid]\n",
    "    r_gender = norm_gender(row[\"Gender\"])\n",
    "    r_age = str(row[\"AgeRange\"]).strip()\n",
    "    r_occ = str(row[\"OccupationLabel\"]).strip()\n",
    "\n",
    "    real_gender.append(r_gender); pred_gender.append(p_gender)\n",
    "    real_age.append(r_age);       pred_age.append(p_age)\n",
    "    real_occ.append(r_occ);       pred_occ.append(p_occ)\n",
    "\n",
    "print(f\"‚ö†Ô∏è parse failed users: {fail_parse}\")\n",
    "print(f\"‚ö†Ô∏è missing real users (uid not in user_info): {missing_real}\")\n",
    "print(f\"‚úÖ aligned users used: {len(real_gender)}\")\n",
    "\n",
    "# =========================================================\n",
    "# Plot function: Real vs Pred; Pred correct/incorrect stacked\n",
    "# =========================================================\n",
    "def plot_comparison_stacked(real_list, pred_list, title, top_k=10, save_dir=SAVE_DIR, filename=None):\n",
    "    real_counter = Counter(real_list)\n",
    "    pred_counter = Counter(pred_list)\n",
    "\n",
    "    correct_counter = Counter([r for r, p in zip(real_list, pred_list) if r == p])\n",
    "    incorrect_counter = {k: pred_counter.get(k, 0) - correct_counter.get(k, 0) for k in pred_counter}\n",
    "\n",
    "    all_keys = set(real_counter.keys()) | set(pred_counter.keys())\n",
    "    df = pd.DataFrame({\n",
    "        \"Category\": list(all_keys),\n",
    "        \"Real\": [real_counter.get(k, 0) for k in all_keys],\n",
    "        \"Correct\": [correct_counter.get(k, 0) for k in all_keys],\n",
    "        \"Incorrect\": [incorrect_counter.get(k, 0) for k in all_keys],\n",
    "    })\n",
    "    df[\"Predicted\"] = df[\"Correct\"] + df[\"Incorrect\"]\n",
    "    df[\"Total\"] = df[\"Real\"] + df[\"Predicted\"]\n",
    "\n",
    "    df_topk = df.sort_values(\"Correct\", ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "    categories = df_topk[\"Category\"].astype(str).tolist()\n",
    "    x = list(range(len(categories)))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(11, 5))\n",
    "\n",
    "    # üîµ Real\n",
    "    plt.bar(\n",
    "        [i - bar_width/2 for i in x],\n",
    "        df_topk[\"Real\"],\n",
    "        width=bar_width,\n",
    "        label=\"Real\",\n",
    "        color=\"#4C72B0\"   # blue\n",
    "    )\n",
    "\n",
    "    # üü¢ Predicted - Correct\n",
    "    plt.bar(\n",
    "        [i + bar_width/2 for i in x],\n",
    "        df_topk[\"Correct\"],\n",
    "        width=bar_width,\n",
    "        label=\"Predicted-Correct\",\n",
    "        color=\"#55A868\"   # green\n",
    "    )\n",
    "\n",
    "    # üü† Predicted - Incorrect\n",
    "    plt.bar(\n",
    "        [i + bar_width/2 for i in x],\n",
    "        df_topk[\"Incorrect\"],\n",
    "        bottom=df_topk[\"Correct\"],\n",
    "        width=bar_width,\n",
    "        label=\"Predicted-Incorrect\",\n",
    "        color=\"#DD8452\"   # orange\n",
    "    )\n",
    "\n",
    "    # numeric display\n",
    "    for i in x:\n",
    "        xpos_real = i - bar_width/2\n",
    "        xpos_pred = i + bar_width/2\n",
    "        real_val = int(df_topk[\"Real\"].iloc[i])\n",
    "        pred_total = int(df_topk[\"Predicted\"].iloc[i])\n",
    "        correct_val = int(df_topk[\"Correct\"].iloc[i])\n",
    "\n",
    "        plt.text(xpos_real, real_val + max(1, real_val*0.02),\n",
    "                 f\"{real_val}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "        plt.text(xpos_pred, pred_total + max(1, pred_total*0.02),\n",
    "                 f\"{pred_total}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "        # White numbers inside Correct\n",
    "        if correct_val > 0:\n",
    "            plt.text(xpos_pred, correct_val / 2,\n",
    "                     f\"{correct_val}\", ha=\"center\", va=\"center\",\n",
    "                     fontsize=9, color=\"white\", fontweight=\"bold\")\n",
    "\n",
    "    plt.xticks(ticks=x, labels=categories, rotation=45, ha=\"right\")\n",
    "    plt.title(f\"{title} Distribution (ML-1M, {steps})\")\n",
    "    plt.xlabel(title)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename is None:\n",
    "        filename = f\"{title}_{steps}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    acc = sum(1 for r, p in zip(real_list, pred_list) if r == p) / max(1, len(real_list))\n",
    "    print(f\"‚úÖ Saved: {save_path}\")\n",
    "    print(f\"[{title}] aligned N={len(real_list)}, exact-match accuracy={acc:.4f}\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# Run\n",
    "# =========================================================\n",
    "plot_comparison_stacked(real_gender, pred_gender, \"Gender\", top_k=5)\n",
    "plot_comparison_stacked(real_age, pred_age, \"AgeRange\", top_k=7)      # 7 buckets\n",
    "plot_comparison_stacked(real_occ, pred_occ, \"Occupation\", top_k=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from collections import Counter\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "TRAIN_PATH = \"data/ml-1m/train.txt\"\n",
    "ITEM_ATTR_PATH = \"data/ml-1m/ml-1m_llmrec_format/augmented_item_attribute_agg.csv\"\n",
    "\n",
    "SAVE_ITEM_PATH = os.path.join(SAVE_DIR, \"item_interaction_stats_with_meta.csv\")\n",
    "SAVE_DIRECTOR_PATH = os.path.join(SAVE_DIR, \"director_interaction_stats.csv\")\n",
    "SAVE_GENRE_PATH = os.path.join(SAVE_DIR, \"genre_interaction_stats.csv\")\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def parse_genres(genre_raw):\n",
    "    \"\"\"\n",
    "    Parse genre string into list\n",
    "    - \"Action|Comedy\" (ML-1M original)\n",
    "    - \"Action, Comedy\"\n",
    "    - \"['Action','Comedy']\"\n",
    "    \"\"\"\n",
    "    if pd.isna(genre_raw):\n",
    "        return []\n",
    "    s = str(genre_raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            return [str(g).strip() for g in ast.literal_eval(s) if str(g).strip()]\n",
    "        if \"|\" in s:\n",
    "            return [g.strip() for g in s.split(\"|\") if g.strip()]\n",
    "        if \",\" in s:\n",
    "            return [g.strip() for g in s.split(\",\") if g.strip()]\n",
    "        return [s]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "def barh_topk(df, category_col, value_col, title, save_path, top_k=20):\n",
    "    if df.empty:\n",
    "        print(f\"‚ö†Ô∏è Empty df: {title}\")\n",
    "        return\n",
    "    d = df.head(top_k).copy()\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(d[category_col][::-1], d[value_col][::-1], edgecolor=\"black\", alpha=0.9)\n",
    "\n",
    "    for bar in bars:\n",
    "        w = bar.get_width()\n",
    "        plt.text(w + (w * 0.01), bar.get_y() + bar.get_height() / 2,\n",
    "                 f\"{int(w)}\", va=\"center\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "    plt.title(title, fontsize=15, fontweight=\"bold\")\n",
    "    plt.xlabel(value_col)\n",
    "    plt.ylabel(category_col)\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"‚úÖ Saved plot: {save_path}\")\n",
    "\n",
    "# =========================\n",
    "# 1) Load train interactions (user, item, timestamp)\n",
    "# =========================\n",
    "train_df = pd.read_csv(TRAIN_PATH, sep=\"\\t\", header=None, names=[\"user\", \"item\", \"timestamp\"])\n",
    "train_df[\"item\"] = train_df[\"item\"].astype(int)\n",
    "print(f\"‚úÖ Loaded train interactions: {len(train_df)} rows\")\n",
    "\n",
    "# =========================\n",
    "# 2) Item interaction count\n",
    "# =========================\n",
    "item_inter_count = Counter(train_df[\"item\"])\n",
    "item_count_df = (\n",
    "    pd.DataFrame(item_inter_count.items(), columns=[\"item_id\", \"interaction_count\"])\n",
    "    .sort_values(\"interaction_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Top-5 items by interaction:\")\n",
    "print(item_count_df.head())\n",
    "\n",
    "# =========================\n",
    "# 3) Load item attributes (id, year, title, genre, director, country, language)\n",
    "# =========================\n",
    "item_attr_df = pd.read_csv(\n",
    "    ITEM_ATTR_PATH,\n",
    "    header=None,\n",
    "    names=[\"item_id\", \"year\", \"title\", \"genre\", \"director\", \"country\", \"language\"]\n",
    ")\n",
    "item_attr_df[\"item_id\"] = item_attr_df[\"item_id\"].astype(int)\n",
    "print(f\"‚úÖ Loaded item attributes: {len(item_attr_df)} items\")\n",
    "\n",
    "# =========================\n",
    "# 4) Add title/genre into item_count_df (request)\n",
    "# =========================\n",
    "item_with_meta_df = pd.merge(\n",
    "    item_count_df,\n",
    "    item_attr_df[[\"item_id\", \"title\", \"genre\", \"director\"]],\n",
    "    on=\"item_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "item_with_meta_df[\"title\"] = item_with_meta_df[\"title\"].fillna(\"Unknown\")\n",
    "item_with_meta_df[\"genre\"] = item_with_meta_df[\"genre\"].fillna(\"Unknown\")\n",
    "item_with_meta_df[\"director\"] = item_with_meta_df[\"director\"].fillna(\"Unknown\")\n",
    "\n",
    "item_with_meta_df.to_csv(SAVE_ITEM_PATH, index=False)\n",
    "print(f\"üíæ Saved item interaction stats with meta:\\n{SAVE_ITEM_PATH}\")\n",
    "\n",
    "# =========================\n",
    "# 5) Director-level aggregation\n",
    "# =========================\n",
    "director_stats = (\n",
    "    item_with_meta_df\n",
    "    .groupby(\"director\", as_index=False)[\"interaction_count\"]\n",
    "    .sum()\n",
    "    .sort_values(\"interaction_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nüé¨ Top-10 directors by total interactions:\")\n",
    "print(director_stats.head(10))\n",
    "\n",
    "director_stats.to_csv(SAVE_DIRECTOR_PATH, index=False)\n",
    "print(f\"\\nüíæ Saved director interaction stats:\\n{SAVE_DIRECTOR_PATH}\")\n",
    "\n",
    "# =========================\n",
    "# 6) Genre-level aggregation (item count accumulated by genre)\n",
    "# =========================\n",
    "genre_counter = Counter()\n",
    "for _, row in item_with_meta_df.iterrows():\n",
    "    cnt = int(row[\"interaction_count\"])\n",
    "    genres = parse_genres(row[\"genre\"])\n",
    "    for g in genres:\n",
    "        if g:\n",
    "            genre_counter[g] += cnt\n",
    "\n",
    "genre_stats = (\n",
    "    pd.DataFrame(genre_counter.items(), columns=[\"genre\", \"interaction_count\"])\n",
    "    .sort_values(\"interaction_count\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"\\nüéûÔ∏è Top-10 genres by total interactions:\")\n",
    "print(genre_stats.head(10))\n",
    "\n",
    "genre_stats.to_csv(SAVE_GENRE_PATH, index=False)\n",
    "print(f\"\\nüíæ Saved genre interaction stats:\\n{SAVE_GENRE_PATH}\")\n",
    "\n",
    "# =========================\n",
    "# 7) Bar graphs (Top-K)\n",
    "# =========================\n",
    "TOPK_DIRECTOR = 20\n",
    "TOPK_GENRE = 20\n",
    "\n",
    "director_plot_path = os.path.join(save_dir, f\"director_interactions_top{TOPK_DIRECTOR}.png\")\n",
    "genre_plot_path = os.path.join(save_dir, f\"genre_interactions_top{TOPK_GENRE}.png\")\n",
    "\n",
    "barh_topk(\n",
    "    director_stats,\n",
    "    category_col=\"director\",\n",
    "    value_col=\"interaction_count\",\n",
    "    title=f\"ML-1M Train Interactions by Director (Top {TOPK_DIRECTOR})\",\n",
    "    save_path=director_plot_path,\n",
    "    top_k=TOPK_DIRECTOR\n",
    ")\n",
    "\n",
    "barh_topk(\n",
    "    genre_stats,\n",
    "    category_col=\"genre\",\n",
    "    value_col=\"interaction_count\",\n",
    "    title=f\"ML-1M Train Interactions by Genre (Top {TOPK_GENRE})\",\n",
    "    save_path=genre_plot_path,\n",
    "    top_k=TOPK_GENRE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "ITEM_ATTR_PATH = \"data/ml-1m/ml-1m_llmrec_format/augmented_item_attribute_agg.csv\"\n",
    "\n",
    "OUT_MATRIX_CSV = os.path.join(SAVE_DIR, \"director_genre_movie_counts.csv\")      # director x genre (wide)\n",
    "OUT_LONG_CSV   = os.path.join(SAVE_DIR, \"director_genre_movie_counts_long.csv\") # director-genre-count (long)\n",
    "OUT_TOP_CSV    = os.path.join(SAVE_DIR, \"director_top_genres.csv\")              # Summary of top genres by director\n",
    "\n",
    "def parse_genres(genre_raw):\n",
    "    if pd.isna(genre_raw):\n",
    "        return []\n",
    "    s = str(genre_raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            return [str(g).strip() for g in ast.literal_eval(s) if str(g).strip()]\n",
    "        if \"|\" in s:\n",
    "            return [g.strip() for g in s.split(\"|\") if g.strip()]\n",
    "        if \",\" in s:\n",
    "            return [g.strip() for g in s.split(\",\") if g.strip()]\n",
    "        return [s]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "# Load item attributes\n",
    "df = pd.read_csv(\n",
    "    ITEM_ATTR_PATH,\n",
    "    header=None,\n",
    "    names=[\"item_id\", \"year\", \"title\", \"genre\", \"director\", \"country\", \"language\"]\n",
    ")\n",
    "\n",
    "df[\"director\"] = df[\"director\"].fillna(\"Unknown\")\n",
    "df[\"genre\"] = df[\"genre\"].fillna(\"Unknown\")\n",
    "\n",
    "# director -> Counter(genre -> count)\n",
    "director_genre_counter = defaultdict(Counter)\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    director = str(row[\"director\"]).strip() if str(row[\"director\"]).strip() else \"Unknown\"\n",
    "    genres = parse_genres(row[\"genre\"])\n",
    "\n",
    "    # If the genre is empty, it is treated as Unknown once (can be changed to skip if desired)\n",
    "    if not genres:\n",
    "        director_genre_counter[director][\"Unknown\"] += 1\n",
    "        continue\n",
    "\n",
    "    for g in genres:\n",
    "        if g:\n",
    "            director_genre_counter[director][g] += 1\n",
    "\n",
    "# ---- long format (director, genre, movie_count)\n",
    "rows = []\n",
    "for director, c in director_genre_counter.items():\n",
    "    for genre, cnt in c.items():\n",
    "        rows.append({\"director\": director, \"genre\": genre, \"movie_count\": int(cnt)})\n",
    "\n",
    "long_df = pd.DataFrame(rows).sort_values([\"director\", \"movie_count\"], ascending=[True, False])\n",
    "long_df.to_csv(OUT_LONG_CSV, index=False)\n",
    "print(f\"‚úÖ Saved long table: {OUT_LONG_CSV}\")\n",
    "\n",
    "# ---- wide format (pivot)\n",
    "wide_df = long_df.pivot_table(index=\"director\", columns=\"genre\", values=\"movie_count\", fill_value=0, aggfunc=\"sum\")\n",
    "wide_df.to_csv(OUT_MATRIX_CSV)\n",
    "print(f\"‚úÖ Saved wide matrix: {OUT_MATRIX_CSV}\")\n",
    "\n",
    "# ---- Summary of top genres by director\n",
    "def top_genres_str(sub_df, top_k=5):\n",
    "    sub = sub_df.sort_values(\"movie_count\", ascending=False).head(top_k)\n",
    "    return \", \".join([f\"{r.genre}:{int(r.movie_count)}\" for r in sub.itertuples(index=False)])\n",
    "\n",
    "top_summary = (\n",
    "    long_df.groupby(\"director\", as_index=False)\n",
    "    .apply(lambda g: pd.Series({\n",
    "        \"n_movies_total\": int(g[\"movie_count\"].sum()),   # Sum of genre counts of the director's films (including duplicates)\n",
    "        \"n_unique_genres\": int(g[\"genre\"].nunique()),\n",
    "        \"top_genres\": top_genres_str(g, top_k=5)\n",
    "    }))\n",
    "    .reset_index(drop=True)\n",
    "    .sort_values(\"n_movies_total\", ascending=False)\n",
    ")\n",
    "\n",
    "top_summary.to_csv(OUT_TOP_CSV, index=False)\n",
    "print(f\"‚úÖ Saved director top genres summary: {OUT_TOP_CSV}\")\n",
    "\n",
    "print(\"\\nTop 10 directors by (sum of genre-counts across movies):\")\n",
    "print(top_summary.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "FILE_PATH = \"data/ml-1m/ml-1m_llmrec_format/augmented_user_profiling_dict_part1_step0\"\n",
    "\n",
    "RUN_DIR = os.path.join(SAVE_DIR, \"group_by_all_keys\")\n",
    "os.makedirs(RUN_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "GROUP_KEYS = None  # If None: Grouping with all keys appearing in successful parsing profile\n",
    "\n",
    "AGG_KEYS = [\"age\", \"gender\", \"occupation\", \"country\", \"language\", \"liked_genre\", \"disliked_genre\"]\n",
    "TOPK_PER_KEY = 15\n",
    "\n",
    "# Specify the ‚Äúkey to be treated as a list‚Äù in the same way as your plot code.\n",
    "LIST_KEYS = {\"liked_genre\", \"disliked_genre\", \"liked_directors\", \"liked_actors\"}\n",
    "\n",
    "# Automatically skip keys that split the group too much\n",
    "SKIP_HIGH_CARDINALITY = True\n",
    "HIGH_CARDINALITY_GROUP_RATIO = 0.8\n",
    "MIN_USERS_FOR_CARDINALITY_CHECK = 100\n",
    "\n",
    "# =========================\n",
    "# Load\n",
    "# =========================\n",
    "if not os.path.exists(FILE_PATH):\n",
    "    raise FileNotFoundError(FILE_PATH)\n",
    "\n",
    "with open(FILE_PATH, \"rb\") as f:\n",
    "    augmented_dict = pickle.load(f)\n",
    "\n",
    "augmented_dict = {str(k): v for k, v in augmented_dict.items()}\n",
    "print(f\"‚úÖ Loaded: {FILE_PATH} (users={len(augmented_dict)})\")\n",
    "print(f\"‚úÖ Output root: {RUN_DIR}\")\n",
    "\n",
    "# =========================\n",
    "# Parse helpers\n",
    "# =========================\n",
    "def clean_and_parse(profile_text):\n",
    "    cleaned = str(profile_text).strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    cleaned = cleaned.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    cleaned = re.sub(r\"(\\w+)['‚Äô]s\", r\"\\1s\", cleaned)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        obj = ast.literal_eval(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        obj = json.loads(cleaned_forced)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_keys(profile):\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        nk = str(k).lower().strip().replace(\" \", \"_\")\n",
    "        if nk == \"liked_director\": nk = \"liked_directors\"\n",
    "        if nk == \"liked_genres\": nk = \"liked_genre\"\n",
    "        if nk == \"disliked__genres\": nk = \"disliked_genre\"\n",
    "        new_profile[nk] = v\n",
    "    return new_profile\n",
    "\n",
    "def is_empty_like(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        return s in [\"unknown\", \"none\", \"n/a\", \"na\", \"null\", \"\"]\n",
    "    if isinstance(x, (list, tuple, set, dict)):\n",
    "        return len(x) == 0\n",
    "    return False\n",
    "\n",
    "# Aggressive gender normalization of your code\n",
    "def normalize_gender(val):\n",
    "    if is_empty_like(val):\n",
    "        return \"Unknown\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s.startswith(\"f\") or \"female\" in s:\n",
    "        return \"Female\"\n",
    "    if s.startswith(\"m\") or \"male\" in s:\n",
    "        return \"Male\"\n",
    "    return str(val).strip().title()\n",
    "\n",
    "# Four code styles: list items are unified with title(), comma split is supported\n",
    "def extract_list(val):\n",
    "    if is_empty_like(val):\n",
    "        return []\n",
    "\n",
    "    if isinstance(val, list):\n",
    "        out = [str(x).strip() for x in val if not is_empty_like(x)]\n",
    "        out = [x for x in out if x]\n",
    "        return [x.title() for x in out]\n",
    "\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" in s:\n",
    "            out = [x.strip() for x in s.split(\",\")]\n",
    "            out = [x for x in out if x]\n",
    "            return [x.title() for x in out]\n",
    "        return [s.title()]\n",
    "\n",
    "    s = str(val).strip()\n",
    "    return [s.title()] if s else []\n",
    "\n",
    "# Scalar is also unified into title()\n",
    "def extract_scalar(val):\n",
    "    if is_empty_like(val):\n",
    "        return None\n",
    "    if isinstance(val, list):\n",
    "        out = [str(x).strip() for x in val if not is_empty_like(x)]\n",
    "        out = [x for x in out if x]\n",
    "        return \", \".join(out).title() if out else None\n",
    "    return str(val).strip().title()\n",
    "\n",
    "# =========================\n",
    "# 1) Parse once\n",
    "# =========================\n",
    "parsed_profiles = {}\n",
    "parse_ok = 0\n",
    "\n",
    "for uid, text in augmented_dict.items():\n",
    "    raw = clean_and_parse(text)\n",
    "    if not raw:\n",
    "        continue\n",
    "    prof = normalize_keys(raw)\n",
    "    parsed_profiles[uid] = prof\n",
    "    parse_ok += 1\n",
    "\n",
    "print(f\"‚úÖ parse success users: {parse_ok}\")\n",
    "if parse_ok == 0:\n",
    "    raise RuntimeError(\"No profiles parsed successfully. Check input formatting.\")\n",
    "\n",
    "# GROUP_KEYS automatic extraction\n",
    "if GROUP_KEYS is None:\n",
    "    keyset = set()\n",
    "    for prof in parsed_profiles.values():\n",
    "        keyset.update(prof.keys())\n",
    "    GROUP_KEYS = sorted(list(keyset))\n",
    "    print(f\"‚úÖ GROUP_KEYS auto: {len(GROUP_KEYS)} keys\")\n",
    "\n",
    "# =========================\n",
    "# 2) Grouping core (aggressive norm)\n",
    "# =========================\n",
    "def build_group_tables_for_key(group_key: str):\n",
    "    group_stats = {}\n",
    "\n",
    "    for uid, prof in parsed_profiles.items():\n",
    "        val = prof.get(group_key)\n",
    "\n",
    "        group_vals = extract_list(val)\n",
    "        if group_vals:\n",
    "            group_vals_norm = sorted([x.strip() for x in group_vals if x.strip()])\n",
    "            group_id = \" | \".join(group_vals_norm)\n",
    "        else:\n",
    "            v = extract_scalar(val)\n",
    "            group_id = v if v is not None else f\"Unknown_{group_key}\"\n",
    "\n",
    "        if group_id not in group_stats:\n",
    "            group_stats[group_id] = {\n",
    "                \"n_users\": 0,\n",
    "                \"counters\": {k: Counter() for k in AGG_KEYS}\n",
    "            }\n",
    "\n",
    "        group_stats[group_id][\"n_users\"] += 1\n",
    "\n",
    "        for k in AGG_KEYS:\n",
    "            raw_v = prof.get(k)\n",
    "            if is_empty_like(raw_v):\n",
    "                continue\n",
    "\n",
    "            if k == \"gender\":\n",
    "                gv = normalize_gender(raw_v)\n",
    "                group_stats[group_id][\"counters\"][k][gv] += 1\n",
    "                continue\n",
    "\n",
    "            if k in LIST_KEYS:\n",
    "                items = extract_list(raw_v)\n",
    "                for it in items:\n",
    "                    if it and not is_empty_like(it):\n",
    "                        group_stats[group_id][\"counters\"][k][it] += 1\n",
    "            else:\n",
    "                sv = extract_scalar(raw_v)\n",
    "                if sv and not is_empty_like(sv):\n",
    "                    group_stats[group_id][\"counters\"][k][sv] += 1\n",
    "\n",
    "    # long\n",
    "    long_rows = []\n",
    "    for gid, info in group_stats.items():\n",
    "        for k, counter in info[\"counters\"].items():\n",
    "            for val, cnt in counter.items():\n",
    "                long_rows.append({\n",
    "                    f\"group_{group_key}\": gid,\n",
    "                    \"n_users_in_group\": info[\"n_users\"],\n",
    "                    \"key\": k,\n",
    "                    \"value\": val,\n",
    "                    \"count\": int(cnt),\n",
    "                })\n",
    "\n",
    "    long_df = pd.DataFrame(long_rows)\n",
    "    if len(long_df) > 0:\n",
    "        long_df = long_df.sort_values(\n",
    "            [\"n_users_in_group\", f\"group_{group_key}\", \"key\", \"count\"],\n",
    "            ascending=[False, True, True, False]\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # summary\n",
    "    def topk_str(counter: Counter, topk: int):\n",
    "        items = counter.most_common(topk)\n",
    "        return \", \".join([f\"{v}:{c}\" for v, c in items])\n",
    "\n",
    "    summary_rows = []\n",
    "    for gid, info in group_stats.items():\n",
    "        row = {f\"group_{group_key}\": gid, \"n_users\": info[\"n_users\"]}\n",
    "        for k in AGG_KEYS:\n",
    "            row[f\"{k}_top{TOPK_PER_KEY}\"] = topk_str(info[\"counters\"][k], TOPK_PER_KEY)\n",
    "        summary_rows.append(row)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    if len(summary_df) > 0:\n",
    "        summary_df = summary_df.sort_values(\"n_users\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return summary_df, long_df, len(group_stats)\n",
    "\n",
    "# =========================\n",
    "# 3) Run for all GROUP_KEYS\n",
    "# =========================\n",
    "skipped = []\n",
    "processed = 0\n",
    "\n",
    "n_users_total = len(parsed_profiles)\n",
    "\n",
    "for gkey in GROUP_KEYS:\n",
    "    summary_df, long_df, n_groups = build_group_tables_for_key(gkey)\n",
    "\n",
    "    if SKIP_HIGH_CARDINALITY and n_users_total >= MIN_USERS_FOR_CARDINALITY_CHECK:\n",
    "        ratio = n_groups / max(n_users_total, 1)\n",
    "        if ratio >= HIGH_CARDINALITY_GROUP_RATIO:\n",
    "            skipped.append((gkey, n_groups, n_users_total, ratio))\n",
    "            print(f\"‚è≠Ô∏è  Skip [{gkey}] groups={n_groups}, users={n_users_total}, ratio={ratio:.3f}\")\n",
    "            continue\n",
    "\n",
    "    out_summary = os.path.join(RUN_DIR, f\"group_summary__{gkey}.csv\")\n",
    "    out_long = os.path.join(RUN_DIR, f\"group_long_counts__{gkey}.csv\")\n",
    "\n",
    "    summary_df.to_csv(out_summary, index=False)\n",
    "    long_df.to_csv(out_long, index=False)\n",
    "\n",
    "    processed += 1\n",
    "    print(f\"üíæ [{gkey}] saved: summary(groups={len(summary_df)}) long(rows={len(long_df)})\")\n",
    "\n",
    "# =========================\n",
    "# 4) Save index files\n",
    "# =========================\n",
    "index_path = os.path.join(RUN_DIR, \"index_processed_keys.txt\")\n",
    "with open(index_path, \"w\") as f:\n",
    "    for k in GROUP_KEYS:\n",
    "        f.write(f\"{k}\\n\")\n",
    "print(f\"üßæ index saved: {index_path}\")\n",
    "\n",
    "if skipped:\n",
    "    skipped_path = os.path.join(RUN_DIR, \"index_skipped_keys.csv\")\n",
    "    pd.DataFrame(skipped, columns=[\"key\", \"n_groups\", \"n_users\", \"group_ratio\"]).to_csv(skipped_path, index=False)\n",
    "    print(f\"üßæ skipped saved: {skipped_path}\")\n",
    "\n",
    "print(f\"‚úÖ Done. processed={processed}, skipped={len(skipped)}, total_keys={len(GROUP_KEYS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc08c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "FILE_PATH = \"data/ml-1m/ml-1m_llmrec_format/augmented_user_profiling_dict_part1_step0\"\n",
    "\n",
    "PLOT_DIR = os.path.join(RUN_DIR, \"plots_all_keys_by_key_folder_v2\")\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "GROUP_KEY = \"gender\"\n",
    "\n",
    "DEFAULT_TOPK = 15\n",
    "TOPK_OVERRIDE = {\n",
    "    \"age\": 10,\n",
    "    \"occupation\": 15,\n",
    "    \"country\": 15,\n",
    "    \"language\": 10,\n",
    "    \"liked_genre\": 15,\n",
    "    \"disliked_genre\": 15,\n",
    "    \"liked_directors\": 15,\n",
    "    \"liked_actors\": 15,\n",
    "}\n",
    "\n",
    "FORCE_LIST_KEYS = {\"liked_genre\", \"disliked_genre\", \"liked_directors\", \"liked_actors\"}\n",
    "LIST_DETECT_SAMPLE_N = 400\n",
    "LIST_DETECT_RATIO = 0.15\n",
    "\n",
    "# =========================\n",
    "# Parse helpers\n",
    "# =========================\n",
    "def clean_and_parse(profile_text):\n",
    "    cleaned = str(profile_text).strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    cleaned = cleaned.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    cleaned = re.sub(r\"(\\w+)['‚Äô]s\", r\"\\1s\", cleaned)\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        obj = ast.literal_eval(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        obj = json.loads(cleaned_forced)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_keys(profile):\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        nk = str(k).lower().strip().replace(\" \", \"_\")\n",
    "        if nk == \"liked_director\": nk = \"liked_directors\"\n",
    "        if nk == \"liked_actor\": nk = \"liked_actors\"\n",
    "        if nk == \"disliked_director\": nk = \"disliked_directors\"\n",
    "        new_profile[nk] = v\n",
    "    return new_profile\n",
    "\n",
    "def is_empty_like(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        return s in [\"unknown\", \"none\", \"n/a\", \"na\", \"null\", \"\"]\n",
    "    if isinstance(x, (list, tuple, set, dict)):\n",
    "        return len(x) == 0\n",
    "    return False\n",
    "\n",
    "def normalize_gender(val):\n",
    "    if is_empty_like(val):\n",
    "        return \"Unknown\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s.startswith(\"f\") or \"female\" in s:\n",
    "        return \"Female\"\n",
    "    if s.startswith(\"m\") or \"male\" in s:\n",
    "        return \"Male\"\n",
    "    return str(val).strip().title()\n",
    "\n",
    "def extract_list(val):\n",
    "    if is_empty_like(val):\n",
    "        return []\n",
    "    if isinstance(val, list):\n",
    "        out = [str(x).strip() for x in val if not is_empty_like(x)]\n",
    "        out = [x for x in out if x]\n",
    "        return [x.title() for x in out]\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" in s:\n",
    "            out = [x.strip() for x in s.split(\",\")]\n",
    "            out = [x for x in out if x]\n",
    "            return [x.title() for x in out]\n",
    "        return [s.title()]\n",
    "    s = str(val).strip()\n",
    "    return [s.title()] if s else []\n",
    "\n",
    "def extract_scalar(val):\n",
    "    if is_empty_like(val):\n",
    "        return None\n",
    "    if isinstance(val, list):\n",
    "        out = [str(x).strip() for x in val if not is_empty_like(x)]\n",
    "        out = [x for x in out if x]\n",
    "        return \", \".join(out).title() if out else None\n",
    "    return str(val).strip().title()\n",
    "\n",
    "# =========================\n",
    "# Plot helpers\n",
    "# =========================\n",
    "def plot_bar(counter: Counter, title: str, xlabel: str, save_path: str, top_k: int = 15):\n",
    "    if not counter:\n",
    "        return\n",
    "    items = counter.most_common(top_k)\n",
    "    labels = [k for k, _ in items]\n",
    "    values = [v for _, v in items]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(labels, values, edgecolor=\"black\", alpha=0.85)\n",
    "\n",
    "    for bar in bars:\n",
    "        y = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            y + (y * 0.01 if y > 0 else 0.5),\n",
    "            f\"{int(y)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    plt.title(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_compare_two(counter_a: Counter, counter_b: Counter, label_a: str, label_b: str,\n",
    "                     title: str, xlabel: str, save_path: str, top_k: int = 15):\n",
    "    # TopK criteria: Top top_k in (A+B) sum\n",
    "    union = Counter()\n",
    "    union.update(counter_a)\n",
    "    union.update(counter_b)\n",
    "    # Update is OK because it is added, not sum, not max (Counter update is added)\n",
    "    items = union.most_common(top_k)\n",
    "    if not items:\n",
    "        return\n",
    "\n",
    "    labels = [k for k, _ in items]\n",
    "    a_vals = [counter_a.get(k, 0) for k in labels]\n",
    "    b_vals = [counter_b.get(k, 0) for k in labels]\n",
    "\n",
    "    x = list(range(len(labels)))\n",
    "    width = 0.42\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars1 = plt.bar([i - width/2 for i in x], a_vals, width=width, edgecolor=\"black\", alpha=0.85, label=label_a)\n",
    "    bars2 = plt.bar([i + width/2 for i in x], b_vals, width=width, edgecolor=\"black\", alpha=0.85, label=label_b)\n",
    "\n",
    "    for bars in (bars1, bars2):\n",
    "        for bar in bars:\n",
    "            y = bar.get_height()\n",
    "            if y == 0:\n",
    "                continue\n",
    "            plt.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                y + (y * 0.01),\n",
    "                f\"{int(y)}\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=9,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    plt.title(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.xticks(x, labels, rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# Load + Parse once\n",
    "# =========================\n",
    "with open(FILE_PATH, \"rb\") as f:\n",
    "    augmented_dict = pickle.load(f)\n",
    "augmented_dict = {str(k): v for k, v in augmented_dict.items()}\n",
    "\n",
    "parsed_profiles = {}\n",
    "parse_ok = 0\n",
    "for uid, text in augmented_dict.items():\n",
    "    raw = clean_and_parse(text)\n",
    "    if not raw:\n",
    "        continue\n",
    "    prof = normalize_keys(raw)\n",
    "    parsed_profiles[uid] = prof\n",
    "    parse_ok += 1\n",
    "\n",
    "print(f\"‚úÖ parse success users: {parse_ok}\")\n",
    "if parse_ok == 0:\n",
    "    raise RuntimeError(\"No profiles parsed successfully.\")\n",
    "\n",
    "# =========================\n",
    "# Collect ALL_KEYS\n",
    "# =========================\n",
    "ALL_KEYS = set()\n",
    "for prof in parsed_profiles.values():\n",
    "    ALL_KEYS.update(prof.keys())\n",
    "ALL_KEYS = sorted(list(ALL_KEYS))\n",
    "print(f\"‚úÖ discovered keys: {len(ALL_KEYS)}\")\n",
    "\n",
    "# =========================\n",
    "# Auto-detect LIST_KEYS\n",
    "# =========================\n",
    "def detect_list_keys(all_keys, profiles, sample_n=400, ratio_th=0.15):\n",
    "    uids = list(profiles.keys())\n",
    "    sample_uids = uids[: min(sample_n, len(uids))]\n",
    "\n",
    "    list_keys = set(FORCE_LIST_KEYS)\n",
    "\n",
    "    for k in all_keys:\n",
    "        if k in list_keys:\n",
    "            continue\n",
    "        if k.startswith(\"liked_\") or k.startswith(\"disliked_\"):\n",
    "            list_keys.add(k)\n",
    "            continue\n",
    "\n",
    "        hits = 0\n",
    "        seen = 0\n",
    "        for uid in sample_uids:\n",
    "            v = profiles[uid].get(k)\n",
    "            if is_empty_like(v):\n",
    "                continue\n",
    "            seen += 1\n",
    "            if isinstance(v, list):\n",
    "                hits += 1\n",
    "            elif isinstance(v, str) and (\",\" in v):\n",
    "                hits += 1\n",
    "\n",
    "        if seen == 0:\n",
    "            continue\n",
    "        if (hits / seen) >= ratio_th:\n",
    "            list_keys.add(k)\n",
    "\n",
    "    return list_keys\n",
    "\n",
    "LIST_KEYS = detect_list_keys(ALL_KEYS, parsed_profiles, LIST_DETECT_SAMPLE_N, LIST_DETECT_RATIO)\n",
    "print(f\"‚úÖ detected LIST_KEYS: {len(LIST_KEYS)}\")\n",
    "\n",
    "# =========================\n",
    "# Aggregate: stats[key][gender] = Counter(value -> count)\n",
    "# Also: stats_all[key] = Counter(value -> count) (total sum)\n",
    "# =========================\n",
    "stats = {k: defaultdict(Counter) for k in ALL_KEYS}\n",
    "stats_all = {k: Counter() for k in ALL_KEYS}\n",
    "gender_user_counts = Counter()\n",
    "\n",
    "for uid, prof in parsed_profiles.items():\n",
    "    gender = normalize_gender(prof.get(GROUP_KEY))\n",
    "    gender_user_counts[gender] += 1\n",
    "\n",
    "    for k in ALL_KEYS:\n",
    "        v = prof.get(k)\n",
    "        if is_empty_like(v):\n",
    "            continue\n",
    "\n",
    "        if k == \"gender\":\n",
    "            gv = normalize_gender(v)\n",
    "            stats[k][gender][gv] += 1\n",
    "            stats_all[k][gv] += 1\n",
    "            continue\n",
    "\n",
    "        if k in LIST_KEYS:\n",
    "            items = extract_list(v)\n",
    "            for it in items:\n",
    "                if it and not is_empty_like(it):\n",
    "                    stats[k][gender][it] += 1\n",
    "                    stats_all[k][it] += 1\n",
    "        else:\n",
    "            sv = extract_scalar(v)\n",
    "            if sv and not is_empty_like(sv):\n",
    "                stats[k][gender][sv] += 1\n",
    "                stats_all[k][sv] += 1\n",
    "\n",
    "print(\"‚úÖ users per gender:\", dict(gender_user_counts))\n",
    "\n",
    "# =========================\n",
    "# Visualize: Create folders for each KEY\n",
    "# - Plot by gender\n",
    "# - All plot\n",
    "# - Male vs Female comparison plot\n",
    "# =========================\n",
    "for key in ALL_KEYS:\n",
    "    key_dir = os.path.join(PLOT_DIR, key)\n",
    "    os.makedirs(key_dir, exist_ok=True)\n",
    "\n",
    "    top_k = TOPK_OVERRIDE.get(key, DEFAULT_TOPK)\n",
    "    xlabel = key.replace(\"_\", \" \").title()\n",
    "\n",
    "    # (A) All plot\n",
    "    all_counter = stats_all.get(key, Counter())\n",
    "    n_all_users = sum(gender_user_counts.values())\n",
    "    if all_counter:\n",
    "        title_all = f\"All (N={n_all_users}) ‚Äî {xlabel} (Top {top_k})\"\n",
    "        save_all = os.path.join(key_dir, f\"All_top{top_k}.png\")\n",
    "        plot_bar(all_counter, title=title_all, xlabel=xlabel, save_path=save_all, top_k=top_k)\n",
    "\n",
    "    # (B) Plot by gender\n",
    "    for gender, counter in stats[key].items():\n",
    "        if not counter:\n",
    "            continue\n",
    "        n_users = gender_user_counts.get(gender, 0)\n",
    "        title = f\"{gender} (N={n_users}) ‚Äî {xlabel} (Top {top_k})\"\n",
    "        save_path = os.path.join(key_dir, f\"{gender}_top{top_k}.png\")\n",
    "        plot_bar(counter, title=title, xlabel=xlabel, save_path=save_path, top_k=top_k)\n",
    "\n",
    "    # (C) Male vs Female comparison plot\n",
    "    male_counter = stats[key].get(\"Male\", Counter())\n",
    "    female_counter = stats[key].get(\"Female\", Counter())\n",
    "    if male_counter or female_counter:\n",
    "        title_cmp = f\"Male vs Female ‚Äî {xlabel} (Top {top_k})\"\n",
    "        save_cmp = os.path.join(key_dir, f\"Male_vs_Female_top{top_k}.png\")\n",
    "        plot_compare_two(\n",
    "            male_counter, female_counter,\n",
    "            label_a=\"Male\", label_b=\"Female\",\n",
    "            title=title_cmp, xlabel=xlabel,\n",
    "            save_path=save_cmp, top_k=top_k\n",
    "        )\n",
    "\n",
    "print(f\"‚úÖ Done. Saved plots under: {PLOT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "FILE_PATH = \"data/ml-1m/ml-1m_llmrec_format/augmented_user_profiling_dict_part1_step0\"\n",
    "\n",
    "TRAIN_PATH = \"data/ml-1m/train.txt\"\n",
    "\n",
    "# Genre information can be entered in one of two ways.\n",
    "# 1) (Recommended) Include ML-1M original style genre: item_attributes.csv (no header: id,year,title,genre)\n",
    "#ITEM_ATTR_CSV = \"data/ml-1m/ml-1m_llmrec_format/item_attributes.csv\"\n",
    "\n",
    "# 2) Replace: augmented_item_attribute_agg.csv (id,year,title,genre,director,country,language)\n",
    "ITEM_ATTR_CSV = \"data/ml-1m/ml-1m_llmrec_format/augmented_item_attribute_agg.csv\"\n",
    "\n",
    "OUT_GROUP_CSV = os.path.join(SAVE_DIR, \"identical_profile_user_groups.csv\")\n",
    "OUT_GROUP_GENRE_LONG = os.path.join(SAVE_DIR, \"profile_group_genre_counts_long.csv\")\n",
    "OUT_GROUP_GENRE_TOP = os.path.join(SAVE_DIR, \"profile_group_genre_top_summary.csv\")\n",
    "\n",
    "# =========================\n",
    "# 0) Helpers\n",
    "# =========================\n",
    "def clean_and_parse(profile_text):\n",
    "    cleaned = str(profile_text).strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    cleaned = cleaned.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        obj = ast.literal_eval(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        obj = json.loads(cleaned_forced)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_keys_only(profile):\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        nk = str(k).lower().strip().replace(\" \", \"_\")\n",
    "        if nk == \"liked_director\": nk = \"liked_directors\"\n",
    "        if nk == \"liked_genres\": nk = \"liked_genre\"\n",
    "        if nk == \"disliked_genres\": nk = \"disliked_genre\"\n",
    "        new_profile[nk] = v\n",
    "    return new_profile\n",
    "\n",
    "def profile_to_canonical_str(profile):\n",
    "    try:\n",
    "        return json.dumps(profile, sort_keys=True, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        return repr(sorted(profile.items(), key=lambda x: x[0]))\n",
    "\n",
    "def parse_genres(genre_raw):\n",
    "    \"\"\"Process all 'Action|Comedy' / 'Action, Comedy' / \"['Action','Comedy']\"\"\"\"\n",
    "    if pd.isna(genre_raw):\n",
    "        return []\n",
    "    s = str(genre_raw).strip()\n",
    "    if not s:\n",
    "        return []\n",
    "    try:\n",
    "        if s.startswith(\"[\") and s.endswith(\"]\"):\n",
    "            lst = ast.literal_eval(s)\n",
    "            return [str(x).strip() for x in lst if str(x).strip()]\n",
    "        if \"|\" in s:\n",
    "            return [x.strip() for x in s.split(\"|\") if x.strip()]\n",
    "        if \",\" in s:\n",
    "            return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "        return [s]\n",
    "    except Exception:\n",
    "        return [s]\n",
    "\n",
    "# =========================\n",
    "# 1) Load profile dict and group identical profiles\n",
    "# =========================\n",
    "with open(FILE_PATH, \"rb\") as f:\n",
    "    augmented_dict = pickle.load(f)\n",
    "\n",
    "augmented_dict = {str(k): v for k, v in augmented_dict.items()}\n",
    "print(f\"‚úÖ Loaded profiles: {len(augmented_dict)} users\")\n",
    "\n",
    "profile_groups = defaultdict(list)\n",
    "parse_fail = 0\n",
    "\n",
    "for uid, text in augmented_dict.items():\n",
    "    raw = clean_and_parse(text)\n",
    "    if raw is None:\n",
    "        parse_fail += 1\n",
    "        continue\n",
    "    prof = normalize_keys_only(raw)\n",
    "    key = profile_to_canonical_str(prof)\n",
    "    profile_groups[key].append(uid)\n",
    "\n",
    "print(f\"‚ö†Ô∏è parse failed users: {parse_fail}\")\n",
    "print(f\"‚úÖ unique profile patterns: {len(profile_groups)}\")\n",
    "\n",
    "rows = []\n",
    "for idx, (profile_str, users) in enumerate(profile_groups.items()):\n",
    "    rows.append({\n",
    "        \"profile_group_id\": idx,\n",
    "        \"n_users\": len(users),\n",
    "        \"user_ids\": \",\".join(users),\n",
    "        \"profile_repr\": profile_str\n",
    "    })\n",
    "\n",
    "group_df = pd.DataFrame(rows).sort_values(\"n_users\", ascending=False).reset_index(drop=True)\n",
    "group_df.to_csv(OUT_GROUP_CSV, index=False)\n",
    "print(f\"üíæ Saved identical profile groups:\\n{OUT_GROUP_CSV}\")\n",
    "\n",
    "# group_id -> set(user_ids as int)\n",
    "group_to_users = {}\n",
    "for r in group_df.itertuples(index=False):\n",
    "    uids = [int(x) for x in str(r.user_ids).split(\",\") if str(x).strip().isdigit()]\n",
    "    group_to_users[int(r.profile_group_id)] = set(uids)\n",
    "\n",
    "# =========================\n",
    "# 2) Load train interactions and filter by group users\n",
    "# =========================\n",
    "train_df = pd.read_csv(TRAIN_PATH, sep=\"\\t\", header=None, names=[\"user\", \"item\", \"timestamp\"])\n",
    "train_df[\"user\"] = train_df[\"user\"].astype(int)\n",
    "train_df[\"item\"] = train_df[\"item\"].astype(int)\n",
    "print(f\"‚úÖ Loaded train interactions: {len(train_df)} rows\")\n",
    "\n",
    "# user -> Counter(item -> count)\n",
    "user_item_counts = defaultdict(Counter)\n",
    "for u, it in zip(train_df[\"user\"].tolist(), train_df[\"item\"].tolist()):\n",
    "    user_item_counts[int(u)][int(it)] += 1\n",
    "\n",
    "# =========================\n",
    "# 3) Load item->genres map\n",
    "# =========================\n",
    "# item_attributes.csv: id,year,title,genre (no header)\n",
    "# augmented_item_attribute_agg.csv: id,year,title,genre,director,country,language (no header)\n",
    "# Here, we assume there are at least 4 columns (id, year, title, genre).\n",
    "item_attr_df = pd.read_csv(ITEM_ATTR_CSV, header=None)\n",
    "if item_attr_df.shape[1] < 4:\n",
    "    raise ValueError(f\"Unexpected item attribute format: {ITEM_ATTR_CSV}\")\n",
    "\n",
    "item_attr_df = item_attr_df.iloc[:, :4]\n",
    "item_attr_df.columns = [\"item_id\", \"year\", \"title\", \"genre\"]\n",
    "item_attr_df[\"item_id\"] = item_attr_df[\"item_id\"].astype(int)\n",
    "\n",
    "item_to_genres = {}\n",
    "for iid, genre_raw in zip(item_attr_df[\"item_id\"].tolist(), item_attr_df[\"genre\"].tolist()):\n",
    "    item_to_genres[int(iid)] = parse_genres(genre_raw)\n",
    "\n",
    "print(f\"‚úÖ Loaded item->genres map: {len(item_to_genres)} items\")\n",
    "\n",
    "# =========================\n",
    "# 4) Group-wise genre count\n",
    "# =========================\n",
    "group_genre_counter = {}  # group_id -> Counter(genre -> count)\n",
    "\n",
    "for gid, users in group_to_users.items():\n",
    "    c = Counter()\n",
    "    for u in users:\n",
    "        # The user may not be on the train\n",
    "        if u not in user_item_counts:\n",
    "            continue\n",
    "        for item_id, cnt in user_item_counts[u].items():\n",
    "            genres = item_to_genres.get(int(item_id), [])\n",
    "            for g in genres:\n",
    "                if g:\n",
    "                    c[g] += int(cnt)\n",
    "    group_genre_counter[gid] = c\n",
    "\n",
    "# =========================\n",
    "# 4-1) Group-wise common items (>=90% users)\n",
    "# =========================\n",
    "THRESH = 0.90  # More than 90%\n",
    "\n",
    "group_common_item_count = {}  # group_id -> int\n",
    "group_common_items = {}       # group_id -> set(items)\n",
    "\n",
    "for gid, users in group_to_users.items():\n",
    "    # Use only users who appear on the train (excluding users who do not exist)\n",
    "    users_in_train = [u for u in users if u in user_item_counts]\n",
    "\n",
    "    n = len(users_in_train)\n",
    "    if n == 0:\n",
    "        group_common_items[gid] = set()\n",
    "        group_common_item_count[gid] = 0\n",
    "        continue\n",
    "\n",
    "    # item -> #users_who_interacted\n",
    "    item_user_freq = Counter()\n",
    "    for u in users_in_train:\n",
    "        # Rather than ‚Äúhow many times a user viewed the item,‚Äù\n",
    "        # ‚ÄòUser ratio‚Äô is accurate only if ‚Äúviewed or not (1)‚Äù is counted.\n",
    "        for item_id in user_item_counts[u].keys():\n",
    "            item_user_freq[int(item_id)] += 1\n",
    "\n",
    "    min_users = int((THRESH * n) + 1e-9)  # safely instead of ceil; The method below is also possible\n",
    "    # min_users = math.ceil(THRESH * n)\n",
    "\n",
    "    near_common = {it for it, cnt_users in item_user_freq.items() if cnt_users >= min_users}\n",
    "\n",
    "    group_common_items[gid] = near_common\n",
    "    group_common_item_count[gid] = len(near_common)\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Save long-format + summary\n",
    "# =========================\n",
    "long_rows = []\n",
    "summary_rows = []\n",
    "\n",
    "TOPK = 10\n",
    "\n",
    "for gid, c in group_genre_counter.items():\n",
    "    n_users = int(group_df.loc[group_df[\"profile_group_id\"] == gid, \"n_users\"].iloc[0])\\\n",
    "              if (group_df[\"profile_group_id\"] == gid).any() else 0\n",
    "\n",
    "    total_inter = int(sum(c.values()))\n",
    "    top_items = c.most_common(TOPK)\n",
    "    top_str = \", \".join([f\"{g}:{v}\" for g, v in top_items])\n",
    "\n",
    "    common_cnt = int(group_common_item_count.get(gid, 0))\n",
    "    common_items_preview = sorted(list(group_common_items.get(gid, set())))[:20]\n",
    "\n",
    "    row = {\n",
    "        \"profile_group_id\": gid,\n",
    "        \"n_users\": n_users,\n",
    "        \"total_genre_interactions\": total_inter,\n",
    "        f\"top{TOPK}_genres\": top_str,\n",
    "        \"common_items_count_all_users\": common_cnt,\n",
    "        \"common_items_preview_top20\": \",\".join(map(str, common_items_preview)),\n",
    "    }\n",
    "    summary_rows.append(row)\n",
    "\n",
    "    for g, v in c.items():\n",
    "        long_rows.append({\n",
    "            \"profile_group_id\": gid,\n",
    "            \"n_users\": n_users,\n",
    "            \"genre\": g,\n",
    "            \"interaction_count\": int(v)\n",
    "        })\n",
    "\n",
    "\n",
    "long_df = pd.DataFrame(long_rows).sort_values(\n",
    "    [\"profile_group_id\", \"interaction_count\"], ascending=[True, False]\n",
    ").reset_index(drop=True)\n",
    "long_df.to_csv(OUT_GROUP_GENRE_LONG, index=False)\n",
    "print(f\"üíæ Saved group-genre long counts:\\n{OUT_GROUP_GENRE_LONG}\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\n",
    "    [\"n_users\", \"total_genre_interactions\"], ascending=[False, False]\n",
    ").reset_index(drop=True)\n",
    "summary_df.to_csv(OUT_GROUP_GENRE_TOP, index=False)\n",
    "print(f\"üíæ Saved group-genre top summary:\\n{OUT_GROUP_GENRE_TOP}\")\n",
    "\n",
    "print(\"\\n===== Preview (top groups by size/interaction) =====\")\n",
    "print(summary_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99888435",
   "metadata": {},
   "source": [
    "### RQ3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78432cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. Preferences (ML-1M)\n",
    "# ==========================================\n",
    "base_dir = \"data/ml-1m/ml-1m_llmrec_format/\"\n",
    "save_dir = os.path.join(base_dir, \"poster/step_comparison/\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# List of files to analyze (part5_step0 ~ step4)\n",
    "file_names = [f\"augmented_user_profiling_dict_part5_step{i}\" for i in range(5)]\n",
    "\n",
    "# Define key to analyze (ML-1M domain)\n",
    "target_keys = [\n",
    "    'age', 'gender', 'occupation', \n",
    "    'country', 'language',\n",
    "    'liked_genre', 'disliked_genre', \n",
    "    'liked_directors'\n",
    "]\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data parsing and purification functions\n",
    "# ==========================================\n",
    "def clean_and_parse(profile_text):\n",
    "    cleaned = profile_text.strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace('```json', '').replace('```', '')\n",
    "    cleaned = cleaned.replace('\\n', ' ').replace('\\r', '')\n",
    "    cleaned = re.sub(r\"(\\w+)['‚Äô]s\", r\"\\1s\", cleaned) # Children's -> Childrens\n",
    "\n",
    "    try:\n",
    "        return json.loads(cleaned)\n",
    "    except:\n",
    "        try:\n",
    "            return ast.literal_eval(cleaned)\n",
    "        except:\n",
    "            try:\n",
    "                cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "                return json.loads(cleaned_forced)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "def normalize_keys(profile):\n",
    "    \"\"\" Unify key names in standard format (underbar, lowercase letters) \"\"\"\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        new_key = k.lower().strip().replace(' ', '_')\n",
    "        # Exception handling mapping\n",
    "        if new_key == 'liked_director': new_key = 'liked_directors'\n",
    "        if new_key == 'liked_actor': new_key = 'liked_actors'\n",
    "        if new_key == 'disliked_director': new_key = 'disliked_directors'\n",
    "        new_profile[new_key] = v\n",
    "    return new_profile\n",
    "\n",
    "# ==========================================\n",
    "# 3. Load full step data\n",
    "# ==========================================\n",
    "def load_all_steps_data(base_dir, file_names, target_keys):\n",
    "    all_steps_data = {} \n",
    "\n",
    "    for fname in file_names:\n",
    "        # Extract step name from file name (e.g. part5_step0 -> Step 0)\n",
    "        try:\n",
    "            step_num = fname.split('_step')[-1]\n",
    "            step_name = f\"Step {step_num}\"\n",
    "        except:\n",
    "            step_name = fname\n",
    "\n",
    "        file_path = os.path.join(base_dir, fname)\n",
    "        step_data = {key: [] for key in target_keys}\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"üìÇ Î°úÎìú Ï§ë: {fname} ...\", end=\" \")\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                augmented_dict = pickle.load(f)\n",
    "            print(f\"ÏôÑÎ£å ({len(augmented_dict)}Î™Ö)\")\n",
    "            \n",
    "            for _, profile_text in augmented_dict.items():\n",
    "                raw_profile = clean_and_parse(profile_text)\n",
    "                if not raw_profile: continue\n",
    "                \n",
    "                # Apply key normalization\n",
    "                profile = normalize_keys(raw_profile)\n",
    "\n",
    "                for key in target_keys:\n",
    "                    val = profile.get(key, None)\n",
    "                    if val:\n",
    "                        # List processing\n",
    "                        if isinstance(val, list):\n",
    "                            cleaned_items = [str(item).strip().title() for item in val if item]\n",
    "                            step_data[key].extend(cleaned_items)\n",
    "                        # string processing\n",
    "                        elif isinstance(val, str):\n",
    "                            val = val.strip()\n",
    "                            if val.lower() in ['unknown', 'none', 'n/a', '', 'null']: continue\n",
    "                            \n",
    "                            if ',' in val:\n",
    "                                cleaned_items = [item.strip().title() for item in val.split(',')]\n",
    "                                step_data[key].extend(cleaned_items)\n",
    "                            else:\n",
    "                                if key == 'gender':\n",
    "                                    g = val.lower()\n",
    "                                    if g.startswith('m'): step_data[key].append('Male')\n",
    "                                    elif g.startswith('f'): step_data[key].append('Female')\n",
    "                                    else: step_data[key].append(val.title())\n",
    "                                else:\n",
    "                                    step_data[key].append(val.title())\n",
    "        else:\n",
    "            print(f\"‚ùå ÌååÏùº ÏóÜÏùå: {file_path}\")\n",
    "        \n",
    "        all_steps_data[step_name] = step_data\n",
    "        \n",
    "    return all_steps_data\n",
    "\n",
    "# ==========================================\n",
    "# 4. Integrated visualization function (2x3 grid)\n",
    "# ==========================================\n",
    "def plot_grid_distribution(target_key, all_steps_data, top_k=10, base_color='skyblue'):\n",
    "    steps = sorted(all_steps_data.keys()) # Step 0 ~ Step 4 order guaranteed\n",
    "    \n",
    "    # Create a 2-row, 3-column subplot (use 5 out of 6 spaces)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten() \n",
    "    \n",
    "    display_title = target_key.replace('_', ' ').title()\n",
    "    print(f\"üé® Í∑∏Î¶¨Îäî Ï§ë: {display_title}\")\n",
    "\n",
    "    for i, step_name in enumerate(steps):\n",
    "        if i >= 6: break\n",
    "        \n",
    "        data_list = all_steps_data[step_name][target_key]\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if not data_list:\n",
    "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center')\n",
    "            ax.set_title(f\"{step_name}\", fontsize=12, fontweight='bold')\n",
    "            continue\n",
    "\n",
    "        # frequency calculation\n",
    "        counter = Counter(data_list)\n",
    "        df = pd.DataFrame.from_dict(counter, orient='index', columns=['Count']).reset_index()\n",
    "        df.rename(columns={'index': 'Category'}, inplace=True)\n",
    "        df_topk = df.sort_values(by='Count', ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "        # bar graph\n",
    "        bars = ax.bar(df_topk['Category'], df_topk['Count'], color=base_color, edgecolor='black', alpha=0.8)\n",
    "\n",
    "        # Numerical display\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, yval + (yval * 0.01), int(yval), \n",
    "                     ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "        # Subplot Styling\n",
    "        ax.set_title(f\"{step_name} (N={len(data_list)})\", fontsize=12, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Handle remaining blank spaces\n",
    "    for j in range(len(steps), 6):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    # full title\n",
    "    plt.suptitle(f\"Changes in {display_title} Distribution (Step 0 - 4)\", fontsize=20, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
    "\n",
    "    # save\n",
    "    filename = f\"Comparison_{target_key}.png\"\n",
    "    save_path = os.path.join(save_dir, filename)\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {save_path}\")\n",
    "    plt.close()\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ Main Run\n",
    "# ==========================================\n",
    "print(\"========== Start loading data ==========\")\n",
    "db = load_all_steps_data(base_dir, file_names, target_keys)\n",
    "\n",
    "print(\"\\n========== Start creating graph ==========\")\n",
    "\n",
    "# 1. Demographics\n",
    "plot_grid_distribution('age', db, top_k=15, base_color='cornflowerblue')\n",
    "plot_grid_distribution('gender', db, top_k=5, base_color='lightcoral')\n",
    "plot_grid_distribution('occupation', db, top_k=10, base_color='teal')\n",
    "plot_grid_distribution('country', db, top_k=10, base_color='orchid')\n",
    "plot_grid_distribution('language', db, top_k=10, base_color='peru')\n",
    "\n",
    "# 2. Taste information\n",
    "plot_grid_distribution('liked_genre', db, top_k=15, base_color='mediumseagreen')\n",
    "plot_grid_distribution('disliked_genre', db, top_k=15, base_color='salmon')\n",
    "plot_grid_distribution('liked_directors', db, top_k=15, base_color='goldenrod')\n",
    "\n",
    "print(\"\\n\\U0001f389 All comparative analysis graphs have been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e74eae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. Preferences (ML-1M)\n",
    "# ==========================================\n",
    "base_dir = \"data/ml-1m/ml-1m_llmrec_format/\"\n",
    "save_dir = os.path.join(base_dir, \"poster/step_comparison_gender/\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "file_names = [f\"augmented_user_profiling_dict_part5_step{i}\" for i in range(5)]\n",
    "\n",
    "target_keys = [\n",
    "    'age', 'gender', 'occupation',\n",
    "    'country', 'language',\n",
    "    'liked_genre', 'disliked_genre',\n",
    "    'liked_directors'\n",
    "]\n",
    "\n",
    "LIST_KEYS = {\"liked_genre\", \"disliked_genre\", \"liked_directors\"}  # list-like keys to process\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data parsing and purification functions\n",
    "# ==========================================\n",
    "def clean_and_parse(profile_text):\n",
    "    cleaned = str(profile_text).strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace('```json', '').replace('```', '')\n",
    "    cleaned = cleaned.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    cleaned = re.sub(r\"(\\w+)['‚Äô]s\", r\"\\1s\", cleaned)  # Children's -> Childrens\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        obj = ast.literal_eval(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        obj = json.loads(cleaned_forced)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalize_keys(profile):\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        nk = str(k).lower().strip().replace(' ', '_')\n",
    "        if nk == 'liked_director': nk = 'liked_directors'\n",
    "        if nk == 'liked_actor': nk = 'liked_actors'\n",
    "        if nk == 'disliked_director': nk = 'disliked_directors'\n",
    "        new_profile[nk] = v\n",
    "    return new_profile\n",
    "\n",
    "\n",
    "def is_empty_like(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        return s in [\"unknown\", \"none\", \"n/a\", \"na\", \"null\", \"\"]\n",
    "    if isinstance(x, (list, tuple, set, dict)):\n",
    "        return len(x) == 0\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize_gender(val):\n",
    "    if is_empty_like(val):\n",
    "        return \"Unknown\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s.startswith(\"f\") or \"female\" in s:\n",
    "        return \"Female\"\n",
    "    if s.startswith(\"m\") or \"male\" in s:\n",
    "        return \"Male\"\n",
    "    return str(val).strip().title()\n",
    "\n",
    "\n",
    "def extract_list(val):\n",
    "    \"\"\"list/str -> list[str], comma split support\"\"\"\n",
    "    if is_empty_like(val):\n",
    "        return []\n",
    "    if isinstance(val, list):\n",
    "        out = [str(x).strip() for x in val if not is_empty_like(x)]\n",
    "        out = [x for x in out if x]\n",
    "        return [x.title() for x in out]\n",
    "    if isinstance(val, str):\n",
    "        s = val.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        if \",\" in s:\n",
    "            out = [x.strip() for x in s.split(\",\")]\n",
    "            out = [x for x in out if x]\n",
    "            return [x.title() for x in out]\n",
    "        return [s.title()]\n",
    "    s = str(val).strip()\n",
    "    return [s.title()] if s else []\n",
    "\n",
    "\n",
    "def extract_scalar(val, key=None):\n",
    "    if is_empty_like(val):\n",
    "        return None\n",
    "\n",
    "    if isinstance(val, str):\n",
    "        v = val.strip()\n",
    "        if not v:\n",
    "            return None\n",
    "        if key == \"gender\":\n",
    "            return normalize_gender(v)\n",
    "        return v.title()\n",
    "\n",
    "    if isinstance(val, list):\n",
    "        out = [str(x).strip() for x in val if not is_empty_like(x)]\n",
    "        out = [x for x in out if x]\n",
    "        return \", \".join(out).title() if out else None\n",
    "\n",
    "    return str(val).strip().title()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. Load step-by-step data divided by gender\n",
    "#    all_steps_data[gender][step_name][key] = list(values)\n",
    "# ==========================================\n",
    "def load_all_steps_data_by_gender(base_dir, file_names, target_keys):\n",
    "    genders = [\"Male\", \"Female\", \"Unknown\"]\n",
    "    all_steps_data = {g: {} for g in genders}\n",
    "    users_per_gender_step = {g: Counter() for g in genders}  # users_per_gender_step[g][step]=count\n",
    "\n",
    "    for fname in file_names:\n",
    "        step_num = fname.split('_step')[-1] if \"_step\" in fname else fname\n",
    "        step_name = f\"Step {step_num}\"\n",
    "\n",
    "        file_path = os.path.join(base_dir, fname)\n",
    "        step_bucket = {g: {k: [] for k in target_keys} for g in genders}\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ùå ÌååÏùº ÏóÜÏùå: {file_path}\")\n",
    "            for g in genders:\n",
    "                all_steps_data[g][step_name] = step_bucket[g]\n",
    "            continue\n",
    "\n",
    "        print(f\"üìÇ Î°úÎìú Ï§ë: {fname} ...\", end=\" \")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            augmented_dict = pickle.load(f)\n",
    "        print(f\"ÏôÑÎ£å ({len(augmented_dict)}Î™Ö)\")\n",
    "\n",
    "        parse_ok = 0\n",
    "        for _, profile_text in augmented_dict.items():\n",
    "            raw_profile = clean_and_parse(profile_text)\n",
    "            if not raw_profile:\n",
    "                continue\n",
    "\n",
    "            profile = normalize_keys(raw_profile)\n",
    "            parse_ok += 1\n",
    "\n",
    "            g = normalize_gender(profile.get(\"gender\", None))\n",
    "            if g not in step_bucket:\n",
    "                g = \"Unknown\"\n",
    "            users_per_gender_step[g][step_name] += 1\n",
    "\n",
    "            for key in target_keys:\n",
    "                v = profile.get(key, None)\n",
    "                if is_empty_like(v):\n",
    "                    continue\n",
    "\n",
    "                if key in LIST_KEYS:\n",
    "                    items = extract_list(v)\n",
    "                    step_bucket[g][key].extend(items)\n",
    "                else:\n",
    "                    sv = extract_scalar(v, key=key)\n",
    "                    if sv and not is_empty_like(sv):\n",
    "                        step_bucket[g][key].append(sv)\n",
    "\n",
    "        print(f\"  ‚Ü≥ parse success: {parse_ok}\")\n",
    "\n",
    "        for g in genders:\n",
    "            all_steps_data[g][step_name] = step_bucket[g]\n",
    "\n",
    "    return all_steps_data, users_per_gender_step\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. Step comparison grid plot (gender version)\n",
    "# ==========================================\n",
    "def plot_grid_distribution_by_gender(\n",
    "    gender: str,\n",
    "    target_key: str,\n",
    "    all_steps_data_gender: dict,\n",
    "    users_per_gender_step: Counter,\n",
    "    top_k=10,\n",
    "    base_color='skyblue',\n",
    "    save_dir=None\n",
    "):\n",
    "    steps = sorted(all_steps_data_gender.keys(), key=lambda x: int(x.split()[-1]) if x.split()[-1].isdigit() else x)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    display_title = target_key.replace('_', ' ').title()\n",
    "    print(f\"üé® [{gender}] Í∑∏Î¶¨Îäî Ï§ë: {display_title}\")\n",
    "\n",
    "    for i, step_name in enumerate(steps):\n",
    "        if i >= 6:\n",
    "            break\n",
    "\n",
    "        data_list = all_steps_data_gender[step_name].get(target_key, [])\n",
    "        ax = axes[i]\n",
    "        n_users = int(users_per_gender_step.get(step_name, 0))\n",
    "\n",
    "        if not data_list:\n",
    "            ax.text(0.5, 0.5, 'No Data', ha='center', va='center')\n",
    "            ax.set_title(f\"{step_name} (N={n_users})\", fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        counter = Counter(data_list)\n",
    "        df = pd.DataFrame(counter.items(), columns=[\"Category\", \"Count\"])\n",
    "        df_topk = df.sort_values(by='Count', ascending=False).head(top_k).reset_index(drop=True)\n",
    "\n",
    "        bars = ax.bar(df_topk['Category'], df_topk['Count'], edgecolor='black', alpha=0.85)\n",
    "\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                yval + (yval * 0.01 if yval > 0 else 0.5),\n",
    "                int(yval),\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=9,\n",
    "                fontweight='bold'\n",
    "            )\n",
    "\n",
    "        ax.set_title(f\"{step_name} (Users={n_users})\", fontsize=12, fontweight='bold')\n",
    "        ax.tick_params(axis='x', rotation=45, labelsize=9)\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    for j in range(len(steps), 6):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.suptitle(f\"{gender}: Changes in {display_title} Distribution (Step 0 - 4)\", fontsize=20, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    if save_dir is not None:\n",
    "        gender_dir = os.path.join(save_dir, gender)\n",
    "        os.makedirs(gender_dir, exist_ok=True)\n",
    "        filename = f\"Comparison_{gender}_{target_key}.png\"\n",
    "        save_path = os.path.join(gender_dir, filename)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å: {save_path}\")\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ Main Run\n",
    "# ==========================================\n",
    "print(\"========== Start loading data ==========\")\n",
    "db_by_gender, users_per_gender_step = load_all_steps_data_by_gender(base_dir, file_names, target_keys)\n",
    "\n",
    "print(\"\\n========== Start creating graph ==========\")\n",
    "\n",
    "# Save only Male/Female (add Unknown if desired)\n",
    "GENDER_TO_PLOT = [\"Male\", \"Female\"]\n",
    "\n",
    "# Top_k / color settings per key (maintain the original four settings)\n",
    "PLOT_CONF = {\n",
    "    'age': (15, 'cornflowerblue'),\n",
    "    'gender': (5, 'lightcoral'),\n",
    "    'occupation': (10, 'teal'),\n",
    "    'country': (10, 'orchid'),\n",
    "    'language': (10, 'peru'),\n",
    "    'liked_genre': (15, 'mediumseagreen'),\n",
    "    'disliked_genre': (15, 'salmon'),\n",
    "    'liked_directors': (15, 'goldenrod'),\n",
    "}\n",
    "\n",
    "for gender in GENDER_TO_PLOT:\n",
    "    for key, (topk, color) in PLOT_CONF.items():\n",
    "        plot_grid_distribution_by_gender(\n",
    "            gender=gender,\n",
    "            target_key=key,\n",
    "            all_steps_data_gender=db_by_gender[gender],\n",
    "            users_per_gender_step=users_per_gender_step[gender],\n",
    "            top_k=topk,\n",
    "            base_color=color,\n",
    "            save_dir=save_dir\n",
    "        )\n",
    "\n",
    "print(\"\\n\\U0001f389 Male/Female step comparison graph created\")\n",
    "print(\"saved at:\", save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3539f622",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72743eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import ast\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "TRY1_PATH = \"data/ml-1m/ml-1m_llmrec_format/augmented_user_profiling_dict_part5_step0_try0\"\n",
    "TRY2_PATH = \"data/ml-1m/ml-1m_llmrec_format/augmented_user_profiling_dict_part5_step0_try1\"\n",
    "OUT_DIR = \"data/ml-1m/ml-1m_llmrec_format/poster/\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "OUT_SUMMARY_CSV = os.path.join(OUT_DIR, \"profile_rawstring_consistency_try1_vs_try2_summary.csv\")\n",
    "OUT_COUNTS_CSV  = os.path.join(OUT_DIR, \"profile_rawstring_consistency_try1_vs_try2_counts.csv\")\n",
    "OUT_USERS_TXT   = os.path.join(OUT_DIR, \"profile_rawstring_consistency_try1_vs_try2_mismatch_users.txt\")\n",
    "\n",
    "# =========================\n",
    "# Target keys\n",
    "# =========================\n",
    "TARGET_KEYS = [\n",
    "    \"age\", \"gender\", \"occupation\",\n",
    "    \"country\", \"language\",\n",
    "    \"liked_genre\", \"disliked_genre\",\n",
    "    \"liked_directors\", \"liked_actors\",\n",
    "]\n",
    "\n",
    "# =========================\n",
    "# Load pickles\n",
    "# =========================\n",
    "def load_pickle(path: str) -> Dict[str, Any]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"File not found: {path}\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        obj = pickle.load(f)\n",
    "    return {str(k): v for k, v in obj.items()}\n",
    "\n",
    "try1 = load_pickle(TRY1_PATH)\n",
    "try2 = load_pickle(TRY2_PATH)\n",
    "\n",
    "common_users = sorted(set(try1.keys()) & set(try2.keys()), key=lambda x: int(x) if x.isdigit() else x)\n",
    "print(f\"‚úÖ try1 users: {len(try1)}\")\n",
    "print(f\"‚úÖ try2 users: {len(try2)}\")\n",
    "print(f\"‚úÖ common users: {len(common_users)}\")\n",
    "\n",
    "# =========================\n",
    "# Parse + key normalization only\n",
    "# (No normalization of values)\n",
    "# =========================\n",
    "def clean_and_parse(profile_text: Any) -> Optional[Dict[str, Any]]:\n",
    "    if profile_text is None:\n",
    "        return None\n",
    "\n",
    "    cleaned = str(profile_text).strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    cleaned = cleaned.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "    # json\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # python literal\n",
    "    try:\n",
    "        obj = ast.literal_eval(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # json after single quote -> double quote\n",
    "    try:\n",
    "        cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        obj = json.loads(cleaned_forced)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_keys(profile: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        nk = str(k).lower().strip().replace(\" \", \"_\")\n",
    "        # Singular/plural unification only (values ‚Äã‚Äãremain the same)\n",
    "        if nk == \"liked_director\": nk = \"liked_directors\"\n",
    "        if nk == \"liked_actor\": nk = \"liked_actors\"\n",
    "        if nk == \"disliked_director\": nk = \"disliked_directors\"\n",
    "        new_profile[nk] = v\n",
    "    return new_profile\n",
    "\n",
    "# =========================\n",
    "# Raw-string comparison\n",
    "# =========================\n",
    "def val_to_raw(v: Any, sort_list: bool = False) -> Any:\n",
    "    \"\"\"\n",
    "    A conversion to compare the value itself to \"a form closer to the original string\".\n",
    "    - str: strip only\n",
    "    - list: list with elements converted to str (if sort_list=True, sort to remove order effect)\n",
    "    - Others: str(v).strip()\n",
    "    \"\"\"\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, str):\n",
    "        return v.strip()\n",
    "    if isinstance(v, list):\n",
    "        lst = [str(x) for x in v]\n",
    "        return sorted(lst) if sort_list else lst\n",
    "    return str(v).strip()\n",
    "\n",
    "def raw_equal(a: Any, b: Any) -> bool:\n",
    "    return a == b\n",
    "\n",
    "# The order of the list must be the same to determine whether it is considered ‚Äúequal.‚Äù\n",
    "# - True: Ignore order (compare after sorting)\n",
    "# - False: Includes order (compare as is)\n",
    "SORT_LIST_BEFORE_COMPARE = False\n",
    "\n",
    "rows = []\n",
    "mismatch_users = []\n",
    "\n",
    "for uid in common_users:\n",
    "    p1_raw = clean_and_parse(try1[uid])\n",
    "    p2_raw = clean_and_parse(try2[uid])\n",
    "\n",
    "    if not p1_raw or not p2_raw:\n",
    "        rows.append({\n",
    "            \"user_id\": uid,\n",
    "            \"parse_ok_try1\": bool(p1_raw),\n",
    "            \"parse_ok_try2\": bool(p2_raw),\n",
    "            \"mismatch_count\": None,\n",
    "            \"mismatch_keys\": \"PARSE_FAIL\",\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    p1 = normalize_keys(p1_raw)\n",
    "    p2 = normalize_keys(p2_raw)\n",
    "\n",
    "    mism_keys = []\n",
    "    detail = {}\n",
    "\n",
    "    for key in TARGET_KEYS:\n",
    "        v1 = val_to_raw(p1.get(key, None), sort_list=SORT_LIST_BEFORE_COMPARE)\n",
    "        v2 = val_to_raw(p2.get(key, None), sort_list=SORT_LIST_BEFORE_COMPARE)\n",
    "\n",
    "        if not raw_equal(v1, v2):\n",
    "            mism_keys.append(key)\n",
    "\n",
    "        # Save original for debugging/review\n",
    "        detail[f\"{key}_try1\"] = v1\n",
    "        detail[f\"{key}_try2\"] = v2\n",
    "\n",
    "    mismatch_count = len(mism_keys)\n",
    "    if mismatch_count > 0:\n",
    "        mismatch_users.append(uid)\n",
    "\n",
    "    row = {\n",
    "        \"user_id\": uid,\n",
    "        \"parse_ok_try1\": True,\n",
    "        \"parse_ok_try2\": True,\n",
    "        \"mismatch_count\": mismatch_count,\n",
    "        \"mismatch_keys\": \",\".join(mism_keys) if mism_keys else \"\",\n",
    "    }\n",
    "    row.update(detail)\n",
    "    rows.append(row)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# =========================\n",
    "# Save outputs\n",
    "# =========================\n",
    "df.to_csv(OUT_SUMMARY_CSV, index=False)\n",
    "print(f\"‚úÖ Saved summary CSV: {OUT_SUMMARY_CSV}\")\n",
    "\n",
    "# Mismatch count by field (parse ok only)\n",
    "df_ok = df[(df[\"parse_ok_try1\"] == True) & (df[\"parse_ok_try2\"] == True)].copy()\n",
    "field_counts = Counter()\n",
    "for keys in df_ok[\"mismatch_keys\"].fillna(\"\").tolist():\n",
    "    if not keys:\n",
    "        continue\n",
    "    for k in keys.split(\",\"):\n",
    "        if k.strip():\n",
    "            field_counts[k.strip()] += 1\n",
    "\n",
    "counts_df = pd.DataFrame(\n",
    "    sorted(field_counts.items(), key=lambda x: x[1], reverse=True),\n",
    "    columns=[\"field\", \"mismatch_user_count\"]\n",
    ")\n",
    "counts_df.to_csv(OUT_COUNTS_CSV, index=False)\n",
    "print(f\"‚úÖ Saved mismatch counts CSV: {OUT_COUNTS_CSV}\")\n",
    "\n",
    "with open(OUT_USERS_TXT, \"w\") as f:\n",
    "    for uid in mismatch_users:\n",
    "        f.write(f\"{uid}\\n\")\n",
    "print(f\"‚úÖ Saved mismatch users list: {OUT_USERS_TXT}\")\n",
    "\n",
    "print(\"\\n===== QUICK STATS =====\")\n",
    "print(\"parsed OK users:\", len(df_ok))\n",
    "print(\"users with >=1 mismatch:\", (df_ok[\"mismatch_count\"].fillna(0) >= 0).sum())\n",
    "if (df_ok[\"mismatch_count\"].fillna(0) > 0).any():\n",
    "    print(\"avg mismatch_count among mismatched:\",\n",
    "          df_ok.loc[df_ok[\"mismatch_count\"].fillna(0) > 0, \"mismatch_count\"].mean())\n",
    "print(\"\\nTop mismatch fields:\")\n",
    "print(counts_df.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31e869",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25d659",
   "metadata": {},
   "source": [
    "### Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08e29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "ITEM_ATTR_PATH = \"data/ml-1m/item_attribute.csv\"\n",
    "PKL_PATH = \"data/ml-1m/Augmentation_format/aug_triplets_part1_step0.pkl\"\n",
    "\n",
    "\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def load_ml1m_item_attribute(item_attribute_csv: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    item_attribute.csv example:\n",
    "    0,1995,Toy Story (1995),\"Animation, Children's, Comedy\"\n",
    "\n",
    "    Return df column:\n",
    "    - item_id (int)\n",
    "    - year (int/nullable)\n",
    "    - title (str)\n",
    "    - genres_raw (str)\n",
    "    \"\"\"\n",
    "    # engine=\"python\" is safe because of quoting/comma handling\n",
    "    df = pd.read_csv(\n",
    "        item_attribute_csv,\n",
    "        header=None,\n",
    "        names=[\"item_id\", \"year\", \"title\", \"genres_raw\"],\n",
    "        engine=\"python\",\n",
    "    )\n",
    "    df[\"item_id\"] = df[\"item_id\"].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_genre_long_table(item_attr_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create (item_id, genre) long-form\n",
    "    - genres_raw: Split strings like ‚ÄúAnimation, Children‚Äôs, Comedy‚Äù with ‚Äò,‚Äô\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for _, r in item_attr_df.iterrows():\n",
    "        item_id = int(r[\"item_id\"])\n",
    "        raw = r[\"genres_raw\"]\n",
    "        if pd.isna(raw):\n",
    "            continue\n",
    "\n",
    "        # \"Animation, Children's, Comedy\" -> [\"Animation\", \"Children's\", \"Comedy\"]\n",
    "        genres = [g.strip() for g in str(raw).split(\",\") if g and str(g).strip()]\n",
    "        for g in genres:\n",
    "            rows.append((item_id, g))\n",
    "\n",
    "    gdf = pd.DataFrame(rows, columns=[\"item_id\", \"genre\"])\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def analyze_pos_genre_distribution(\n",
    "    triplets_pkl_path: str,\n",
    "    genre_long_df: pd.DataFrame,\n",
    "    topk: int = 30,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    return:\n",
    "    - counts: Genre distribution based on pos selection frequency (count, prob)\n",
    "    - genre_item_count_df: Genre distribution based on unique pos items (item_count)\n",
    "    \"\"\"\n",
    "    triplets = load_pickle(triplets_pkl_path)\n",
    "\n",
    "    pos_items = []\n",
    "    for t in triplets:\n",
    "        if not isinstance(t, (list, tuple)) or len(t) < 3:\n",
    "            continue\n",
    "        _, pos, _ = t[0], t[1], t[2]\n",
    "        try:\n",
    "            pos_items.append(int(pos))\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(pos_items) == 0:\n",
    "        empty1 = pd.DataFrame(columns=[\"genre\", \"count\", \"prob\"])\n",
    "        empty2 = pd.DataFrame(columns=[\"genre\", \"item_count\"])\n",
    "        return empty1, empty2\n",
    "\n",
    "    # === Number of selections for each unique item in pos_items ===\n",
    "    pos_item_counter = Counter(pos_items)\n",
    "    pos_item_count_df = (\n",
    "        pd.DataFrame(pos_item_counter.items(), columns=[\"item_id\", \"pos_count\"])\n",
    "        .sort_values(\"pos_count\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"[POS ITEM COUNTS] unique items: {len(pos_item_count_df)}\")\n",
    "    print(pos_item_count_df.head(20))\n",
    "    print(\"count stats:\", pos_item_count_df[\"pos_count\"].describe())\n",
    "\n",
    "    # === Genre count based on unique items ===\n",
    "    unique_pos_items = pos_item_count_df[\"item_id\"].unique()\n",
    "\n",
    "    unique_item_genres = (\n",
    "        pd.DataFrame({\"item_id\": unique_pos_items})\n",
    "        .merge(genre_long_df, on=\"item_id\", how=\"left\")\n",
    "        .dropna(subset=[\"genre\"])\n",
    "        .drop_duplicates(subset=[\"item_id\", \"genre\"])\n",
    "    )\n",
    "\n",
    "    genre_item_count_df = (\n",
    "        unique_item_genres.groupby(\"genre\")\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "        .rename(\"item_count\")\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    print(\"[UNIQUE POS ITEMS] genre count (item-based)\")\n",
    "    print(genre_item_count_df.head(20))\n",
    "\n",
    "    # === Existing: Frequency-based genre distribution ===\n",
    "    pos_df = pd.DataFrame({\"item_id\": pos_items})\n",
    "    merged = pos_df.merge(genre_long_df, on=\"item_id\", how=\"left\").dropna(subset=[\"genre\"])\n",
    "\n",
    "    counts = (\n",
    "        merged.groupby(\"genre\")\n",
    "        .size()\n",
    "        .sort_values(ascending=False)\n",
    "        .rename(\"count\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    total = int(counts[\"count\"].sum()) if len(counts) > 0 else 0\n",
    "    counts[\"prob\"] = counts[\"count\"] / max(1, total)\n",
    "\n",
    "    if topk is not None:\n",
    "        counts = counts.head(topk)\n",
    "\n",
    "    return counts, genre_item_count_df\n",
    "\n",
    "\n",
    "\n",
    "# === Run ===\n",
    "item_attr_df = load_ml1m_item_attribute(ITEM_ATTR_PATH)\n",
    "genre_long_df = build_genre_long_table(item_attr_df)\n",
    "\n",
    "print(\"item_attr_df:\", item_attr_df.shape)\n",
    "print(\"genre_long_df:\", genre_long_df.shape)\n",
    "print(genre_long_df.head(5))\n",
    "\n",
    "pos_genre_df, unique_pos_genre_df = analyze_pos_genre_distribution(PKL_PATH, genre_long_df, topk=30)\n",
    "out_path = \"data/ml-1m/Augmentation_format/results\"\n",
    "\n",
    "# CSV save (unique item standard)\n",
    "unique_pos_genre_df.to_csv(f\"{out_path}/unique_pos_item_genre_dist_part1_step0.csv\", index=False)\n",
    "print(\"saved unique item genre dist:\", f\"{out_path}/unique_pos_item_genre_dist_part1_step0.csv\")\n",
    "\n",
    "print(\"\\n[aug_triplets_part1_step0.pkl] positive(pos) genre distribution (top 30)\")\n",
    "print(pos_genre_df)\n",
    "\n",
    "pos_genre_df.to_csv(f\"{out_path}/pos_genre_dist_part1_step0.csv\", index=False)\n",
    "print(\"saved:\", out_path)\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_genre_bar_vertical(\n",
    "    df: pd.DataFrame,\n",
    "    title: str,\n",
    "    topk: int = 10,\n",
    "    save_path: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Vertical bar plot\n",
    "    - Display count value above each bar\n",
    "    - Use only up to topk\n",
    "    - Save as file when save_path is specified\n",
    "    \"\"\"\n",
    "    if df is None or df.empty:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "\n",
    "    d = df.sort_values(\"count\", ascending=False).head(topk).copy()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(d[\"genre\"], d[\"count\"])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"genre\")\n",
    "    plt.ylabel(\"count\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    # Show numbers above bar\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            h,\n",
    "            f\"{int(h)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        save_path = Path(save_path)\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "        print(f\"saved figure to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Traditional frequency-based graph\n",
    "plot_genre_bar_vertical(\n",
    "    pos_genre_df,\n",
    "    title=\"[POS] Genre distribution (frequency-based, Top 10)\",\n",
    "    topk=10,\n",
    "    save_path=f\"{out_path}/pos_genre_bar_part1_step0.png\",\n",
    ")\n",
    "\n",
    "# Graph based on unique items (save with different file name)\n",
    "unique_plot_df = unique_pos_genre_df.rename(columns={\"item_count\": \"count\"})\n",
    "plot_genre_bar_vertical(\n",
    "    unique_plot_df,\n",
    "    title=\"[POS UNIQUE ITEMS] Genre distribution (item-based, Top 10)\",\n",
    "    topk=10,\n",
    "    save_path=f\"{out_path}/pos_unique_item_genre_bar_part1_step0.png\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d746f",
   "metadata": {},
   "source": [
    "### Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "AB_PATH = \"data/ml-1m/Augmentation_format/aug_triplets_part5_step0_try1.pkl\"\n",
    "BA_PATH = \"data/ml-1m/Augmentation_format/aug_triplets_part5_step0_try2.pkl\"\n",
    "\n",
    "def load_triplets(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)  # List[Tuple[int,int,int]]\n",
    "\n",
    "def build_pos_counter(triplets):\n",
    "    \"\"\"\n",
    "    key = (u, a, b) where a<b are the unordered pair\n",
    "    value = Counter({pos_item: count})\n",
    "    \"\"\"\n",
    "    d = defaultdict(Counter)\n",
    "    for u, pos, neg in triplets:\n",
    "        a, b = sorted((int(pos), int(neg)))\n",
    "        key = (int(u), a, b)\n",
    "        d[key][int(pos)] += 1\n",
    "    return d\n",
    "\n",
    "ab = load_triplets(AB_PATH)\n",
    "ba = load_triplets(BA_PATH)\n",
    "\n",
    "# sort ab, ba\n",
    "ab = sorted(ab, key=lambda x: (x[0], x[1], x[2]))\n",
    "ba = sorted(ba, key=lambda x: (x[0], x[1], x[2]))\n",
    "\n",
    "print(\"AB triplets:\", len(ab))\n",
    "print(\"BA triplets:\", len(ba))\n",
    "\n",
    "ab_map = build_pos_counter(ab)\n",
    "ba_map = build_pos_counter(ba)\n",
    "\n",
    "common_keys = set(ab_map.keys()) & set(ba_map.keys())\n",
    "only_ab = set(ab_map.keys()) - set(ba_map.keys())\n",
    "only_ba = set(ba_map.keys()) - set(ab_map.keys())\n",
    "\n",
    "if only_ab or only_ba:\n",
    "    print(f\"WARNING: key mismatch. only_ab={len(only_ab)}, only_ba={len(only_ba)}\")\n",
    "\n",
    "mismatch_total = 0\n",
    "match_total = 0\n",
    "total_compared = 0\n",
    "\n",
    "# optional: mismatch examples\n",
    "examples = []\n",
    "\n",
    "for key in common_keys:\n",
    "    # key=(u,a,b)\n",
    "    u, a, b = key\n",
    "\n",
    "    ab_counts = ab_map[key]  # Counter(pos)\n",
    "    ba_counts = ba_map[key]\n",
    "\n",
    "    total_ab = sum(ab_counts.values())\n",
    "    total_ba = sum(ba_counts.values())\n",
    "    if total_ab != total_ba:\n",
    "        # Comparison is ambiguous if the number of AB/BA samples for the same key is different.\n",
    "        # Still, you can only compare to the possible range (min) or just skip it.\n",
    "        # Here, it is calculated based on ‚Äúpossible matches‚Äù.\n",
    "        pass\n",
    "\n",
    "    # Number of matches possible with the same pos\n",
    "    match_a = min(ab_counts.get(a, 0), ba_counts.get(a, 0))\n",
    "    match_b = min(ab_counts.get(b, 0), ba_counts.get(b, 0))\n",
    "    matches = match_a + match_b\n",
    "\n",
    "    # Total number of comparables (since it is a multiset, whichever is smaller)\n",
    "    total = min(total_ab, total_ba)\n",
    "\n",
    "    mismatches = total - matches\n",
    "\n",
    "    match_total += matches\n",
    "    mismatch_total += mismatches\n",
    "    total_compared += total\n",
    "\n",
    "    if mismatches > 0 and len(examples) < 20:\n",
    "        examples.append({\n",
    "            \"user\": u, \"pair\": (a, b),\n",
    "            \"AB\": dict(ab_counts), \"BA\": dict(ba_counts),\n",
    "            \"mismatches\": mismatches, \"total\": total\n",
    "        })\n",
    "\n",
    "print(\"\\n===== RESULT =====\")\n",
    "print(\"Compared pairs(keys):\", len(common_keys))\n",
    "print(\"Total comparable samples:\", total_compared)\n",
    "print(\"Match count:\", match_total)\n",
    "print(\"Mismatch count (pos differs AB vs BA):\", mismatch_total)\n",
    "if total_compared > 0:\n",
    "    print(\"Mismatch rate:\", mismatch_total / total_compared)\n",
    "\n",
    "print(\"\\n===== mismatch examples (up to 20) =====\")\n",
    "for ex in examples:\n",
    "    print(ex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc50df",
   "metadata": {},
   "source": [
    "# RQ2 & RQ3\n",
    "Analysis of the results (after feedback loop) of repeating recommendations with data generated by LLM (Part 1: RQ2, Part 5: RQ3)\n",
    "\n",
    "Includes: LLMRec, A-LLMRec, Augmentation, TR_CF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c7aed",
   "metadata": {},
   "source": [
    "## LLMRec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757d910",
   "metadata": {},
   "source": [
    "### Bias\n",
    "Comparison of recommendation results by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209dbb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pickle, re, ast\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================================================\n",
    "# 0) Paths\n",
    "# =========================================================\n",
    "BASE_DIR = \"data/ml-1m/ml-1m_llmrec_format\"\n",
    "PRED_PATH = os.path.join(BASE_DIR, \"predict_label_part5.json\")\n",
    "PROFILE_PATH = os.path.join(BASE_DIR, \"augmented_user_profiling_dict_part1_step0\")\n",
    "ITEM_ATTR_PATH = \"data/ml-1m/item_attribute.csv\"\n",
    "\n",
    "SAVE_PLOT_DIR = os.path.join(BASE_DIR, \"poster/gender_recommendation_genre\")\n",
    "os.makedirs(SAVE_PLOT_DIR, exist_ok=True)\n",
    "\n",
    "TOPK_GENRE = 15  # bar plot top-k\n",
    "\n",
    "# =========================================================\n",
    "# 1) Profile parsing helpers\n",
    "# =========================================================\n",
    "def clean_and_parse(profile_text):\n",
    "    cleaned = str(profile_text).strip()\n",
    "    cleaned = cleaned.replace(\"'''\", \"\").replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "    cleaned = cleaned.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    cleaned = re.sub(r\"(\\w+)['‚Äô]s\", r\"\\1s\", cleaned)  # Children's -> Childrens\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()\n",
    "\n",
    "    # json.loads\n",
    "    try:\n",
    "        obj = json.loads(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ast.literal_eval\n",
    "    try:\n",
    "        obj = ast.literal_eval(cleaned)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # last coercion\n",
    "    try:\n",
    "        cleaned_forced = re.sub(r\"(?<!\\\\)'\", '\"', cleaned)\n",
    "        obj = json.loads(cleaned_forced)\n",
    "        return obj if isinstance(obj, dict) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalize_keys(profile: dict) -> dict:\n",
    "    new_profile = {}\n",
    "    for k, v in profile.items():\n",
    "        nk = str(k).lower().strip().replace(\" \", \"_\")\n",
    "        if nk == \"liked_director\": nk = \"liked_directors\"\n",
    "        if nk == \"liked_actor\": nk = \"liked_actors\"\n",
    "        if nk == \"disliked_director\": nk = \"disliked_directors\"\n",
    "        new_profile[nk] = v\n",
    "    return new_profile\n",
    "\n",
    "def is_empty_like(x):\n",
    "    if x is None:\n",
    "        return True\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip().lower()\n",
    "        return s in [\"unknown\", \"none\", \"n/a\", \"na\", \"null\", \"\"]\n",
    "    if isinstance(x, (list, tuple, set, dict)):\n",
    "        return len(x) == 0\n",
    "    return False\n",
    "\n",
    "def normalize_gender(val):\n",
    "    if is_empty_like(val):\n",
    "        return \"Unknown\"\n",
    "    s = str(val).strip().lower()\n",
    "    if s.startswith(\"f\") or \"female\" in s:\n",
    "        return \"Female\"\n",
    "    if s.startswith(\"m\") or \"male\" in s:\n",
    "        return \"Male\"\n",
    "    return \"Unknown\"\n",
    "\n",
    "# =========================================================\n",
    "# 2) Load user -> gender from profiling dict\n",
    "# =========================================================\n",
    "with open(PROFILE_PATH, \"rb\") as f:\n",
    "    profile_dict = pickle.load(f)\n",
    "\n",
    "user_gender: dict[int, str] = {}\n",
    "parse_ok = 0\n",
    "for uid, text in profile_dict.items():\n",
    "    raw = clean_and_parse(text)\n",
    "    if not raw:\n",
    "        continue\n",
    "    prof = normalize_keys(raw)\n",
    "    g = normalize_gender(prof.get(\"gender\"))\n",
    "    try:\n",
    "        user_gender[int(uid)] = g\n",
    "        parse_ok += 1\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "gender_user_count = Counter(user_gender.values())\n",
    "\n",
    "print(f\"‚úÖ profile parse success users: {parse_ok}\")\n",
    "print(\"‚úÖ users per gender:\", dict(gender_user_count))\n",
    "\n",
    "# =========================================================\n",
    "# 3) Load predictions: user -> items\n",
    "# =========================================================\n",
    "with open(PRED_PATH, \"r\") as f:\n",
    "    pred_raw = json.load(f)\n",
    "\n",
    "pred_dict: dict[int, list[int]] = {}\n",
    "for u, items in pred_raw.items():\n",
    "    try:\n",
    "        ui = int(u)\n",
    "    except Exception:\n",
    "        continue\n",
    "    if isinstance(items, list):\n",
    "        pred_dict[ui] = [int(x) for x in items]\n",
    "    else:\n",
    "        try:\n",
    "            pred_dict[ui] = [int(items)]\n",
    "        except Exception:\n",
    "            pred_dict[ui] = []\n",
    "\n",
    "print(\"‚úÖ users in pred_dict:\", len(pred_dict))\n",
    "\n",
    "# =========================================================\n",
    "# 4) Group predicted items by gender\n",
    "# =========================================================\n",
    "gender_pred_items = defaultdict(list)  # gender -> list of predicted items (with repetition)\n",
    "interaction_count = 0\n",
    "for u, items in pred_dict.items():\n",
    "    interaction_count += len(items)\n",
    "    g = user_gender.get(u, \"Unknown\")\n",
    "    gender_pred_items[g].extend(items)\n",
    "\n",
    "gender_pred_item_count = {g: len(items) for g, items in gender_pred_items.items()}\n",
    "print(\"‚úÖ predicted items per gender:\", dict(gender_pred_item_count))\n",
    "print(f\"‚úÖ total interactions (user-item pairs): {interaction_count}\")\n",
    "\n",
    "# =========================================================\n",
    "# 5) Load item -> genres from ML-1M item_attribute.csv\n",
    "# =========================================================\n",
    "item_df = pd.read_csv(\n",
    "    ITEM_ATTR_PATH,\n",
    "    header=None,\n",
    "    names=[\"item_id\", \"year\", \"title\", \"genres_raw\"],\n",
    "    engine=\"python\",\n",
    ")\n",
    "item_df[\"item_id\"] = item_df[\"item_id\"].astype(int)\n",
    "\n",
    "item2genres: dict[int, list[str]] = {}\n",
    "for _, r in item_df.iterrows():\n",
    "    raw = r[\"genres_raw\"]\n",
    "    if pd.isna(raw):\n",
    "        continue\n",
    "    genres = [g.strip().title() for g in str(raw).split(\",\") if g.strip()]\n",
    "    item2genres[int(r[\"item_id\"])] = genres\n",
    "\n",
    "print(\"‚úÖ loaded item genres:\", len(item2genres))\n",
    "\n",
    "# =========================================================\n",
    "# 6) Gender-wise genre analysis\n",
    "#    (A) frequency-based: count by recommended frequency\n",
    "# =========================================================\n",
    "gender_genre_count = {}\n",
    "for gender, items in gender_pred_items.items():\n",
    "    c = Counter()\n",
    "    for it in items:\n",
    "        genres = item2genres.get(int(it), [])\n",
    "        for gg in genres:\n",
    "            c[gg] += 1\n",
    "    gender_genre_count[gender] = c\n",
    "\n",
    "# =========================================================\n",
    "# 7) Plot helper (vertical bar) + save\n",
    "# =========================================================\n",
    "def plot_gender_genre_bar(\n",
    "    gender: str,\n",
    "    genre_counter: Counter,\n",
    "    user_count: int,\n",
    "    pred_item_count: int,\n",
    "    top_k: int = 15,\n",
    "    save_dir: str | None = None,\n",
    "):\n",
    "    if not genre_counter:\n",
    "        print(f\"‚ö†Ô∏è no genre data: {gender}\")\n",
    "        return\n",
    "\n",
    "    items = genre_counter.most_common(top_k)\n",
    "    labels = [k for k, _ in items]\n",
    "    values = [v for _, v in items]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(labels, values, edgecolor=\"black\", alpha=0.85)\n",
    "\n",
    "    for bar in bars:\n",
    "        h = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            h + (h * 0.01 if h > 0 else 0.5),\n",
    "            f\"{int(h)}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=10,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "\n",
    "    title = (\n",
    "        f\"{gender} ‚Äî Recommended Genre Distribution\\n\"\n",
    "        f\"(Users={user_count}, Predicted Items={pred_item_count})\"\n",
    "    )\n",
    "    plt.title(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Genre\", fontsize=12)\n",
    "    plt.ylabel(\"Count\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_dir is not None:\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "        save_path = os.path.join(save_dir, f\"{gender}_predicted_genre_bar_top{top_k}.png\")\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"‚úÖ saved: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# =========================================================\n",
    "# 8) Make plots (Male / Female)\n",
    "# =========================================================\n",
    "for gender in [\"Male\", \"Female\"]:\n",
    "    plot_gender_genre_bar(\n",
    "        gender=gender,\n",
    "        genre_counter=gender_genre_count.get(gender, Counter()),\n",
    "        user_count=gender_user_count.get(gender, 0),\n",
    "        pred_item_count=gender_pred_item_count.get(gender, 0),\n",
    "        top_k=TOPK_GENRE,\n",
    "        save_dir=SAVE_PLOT_DIR,\n",
    "    )\n",
    "\n",
    "print(\"üéâ Done. Plots saved at:\", SAVE_PLOT_DIR)\n",
    "\n",
    "# =========================================================\n",
    "# 9) Count recommended items by group (All / Male / Female)\n",
    "# =========================================================\n",
    "\n",
    "def print_top_recommended_items(\n",
    "    item_counter: Counter,\n",
    "    title: str,\n",
    "    top_k: int = 20,\n",
    "):\n",
    "    print(f\"\\n===== {title} (Top {top_k}) =====\")\n",
    "    for rank, (item_id, cnt) in enumerate(item_counter.most_common(top_k), 1):\n",
    "        print(f\"{rank:02d}. Item {item_id:<6d}  |  count = {cnt}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# (A) All users\n",
    "# -------------------------\n",
    "all_items = []\n",
    "for items in pred_dict.values():\n",
    "    all_items.extend(items)\n",
    "\n",
    "all_item_counter = Counter(all_items)\n",
    "\n",
    "print_top_recommended_items(\n",
    "    all_item_counter,\n",
    "    title=\"ALL USERS ‚Äî Recommended Items\",\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "print(f\"\\n[ALL] total predicted items: {len(all_items)}\")\n",
    "print(f\"[ALL] unique predicted items: {len(all_item_counter)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# (B) Male users\n",
    "# -------------------------\n",
    "male_items = gender_pred_items.get(\"Male\", [])\n",
    "male_item_counter = Counter(male_items)\n",
    "\n",
    "print_top_recommended_items(\n",
    "    male_item_counter,\n",
    "    title=\"MALE USERS ‚Äî Recommended Items\",\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "print(f\"\\n[Male] total predicted items: {len(male_items)}\")\n",
    "print(f\"[Male] unique predicted items: {len(male_item_counter)}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# (C) Female users\n",
    "# -------------------------\n",
    "female_items = gender_pred_items.get(\"Female\", [])\n",
    "female_item_counter = Counter(female_items)\n",
    "\n",
    "print_top_recommended_items(\n",
    "    female_item_counter,\n",
    "    title=\"FEMALE USERS ‚Äî Recommended Items\",\n",
    "    top_k=30,\n",
    ")\n",
    "\n",
    "print(f\"\\n[Female] total predicted items: {len(female_items)}\")\n",
    "print(f\"[Female] unique predicted items: {len(female_item_counter)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 10) Items recommended only to one gender (set difference)\n",
    "# =========================================================\n",
    "\n",
    "def print_items_with_counts(item_ids, counter: Counter, title: str, top_k: int = 30):\n",
    "    print(f\"\\n===== {title} (Top {top_k}) =====\")\n",
    "    rows = [(it, counter.get(it, 0)) for it in item_ids]\n",
    "    rows.sort(key=lambda x: x[1], reverse=True)\n",
    "    for i, (it, cnt) in enumerate(rows[:top_k], 1):\n",
    "        print(f\"{i:02d}. Item {it:<6d} | count = {cnt}\")\n",
    "    print(f\"Total items: {len(rows)}\")\n",
    "\n",
    "\n",
    "male_counter = male_item_counter\n",
    "female_counter = female_item_counter\n",
    "\n",
    "male_set = set(male_counter.keys())\n",
    "female_set = set(female_counter.keys())\n",
    "\n",
    "# Male only / Female only\n",
    "male_only_items = male_set - female_set\n",
    "female_only_items = female_set - male_set\n",
    "\n",
    "print(f\"\\n[Male only] unique items: {len(male_only_items)}\")\n",
    "print(f\"[Female only] unique items: {len(female_only_items)}\")\n",
    "\n",
    "print_items_with_counts(male_only_items, male_counter, \"MALE ONLY recommended items\", top_k=30)\n",
    "print_items_with_counts(female_only_items, female_counter, \"FEMALE ONLY recommended items\", top_k=30)\n",
    "\n",
    "# -------------------------\n",
    "# (Optional) Set a minimum recommendation count threshold\n",
    "# -------------------------\n",
    "MIN_COUNT = 5  # Adjust if necessary\n",
    "\n",
    "male_only_strong = [it for it in male_only_items if male_counter[it] >= MIN_COUNT]\n",
    "female_only_strong = [it for it in female_only_items if female_counter[it] >= MIN_COUNT]\n",
    "\n",
    "print(f\"\\n[Male only | count >= {MIN_COUNT}] items: {len(male_only_strong)}\")\n",
    "print(f\"[Female only | count >= {MIN_COUNT}] items: {len(female_only_strong)}\")\n",
    "\n",
    "print_items_with_counts(male_only_strong, male_counter, f\"MALE ONLY (count >= {MIN_COUNT})\", top_k=30)\n",
    "print_items_with_counts(female_only_strong, female_counter, f\"FEMALE ONLY (count >= {MIN_COUNT})\", top_k=30)\n",
    "\n",
    "# =========================================================\n",
    "# 11) Genre distribution for MALE/FEMALE-only recommended items\n",
    "# =========================================================\n",
    "\n",
    "def genre_counter_for_itemset(\n",
    "    item_ids,\n",
    "    item_counter: Counter,          # Counter for the number of item recommendations for that gender\n",
    "    item2genres: dict[int, list[str]],\n",
    "    weighted: bool = True,          # If True, the weight is based on the number of recommendations, if False, 1 vote per item.\n",
    "):\n",
    "    gc = Counter()\n",
    "    for it in item_ids:\n",
    "        genres = item2genres.get(int(it), [])\n",
    "        if not genres:\n",
    "            continue\n",
    "        w = int(item_counter.get(it, 1)) if weighted else 1\n",
    "        for g in genres:\n",
    "            gc[g] += w\n",
    "    return gc\n",
    "\n",
    "def print_genre_summary(gc: Counter, title: str, top_k: int = 20):\n",
    "    total = sum(gc.values())\n",
    "    print(f\"\\n===== {title} =====\")\n",
    "    print(f\"Total genre counts: {total}, Unique genres: {len(gc)}\")\n",
    "    for i, (g, c) in enumerate(gc.most_common(top_k), 1):\n",
    "        pct = (c / total * 100) if total > 0 else 0.0\n",
    "        print(f\"{i:02d}. {g:<15s} | count={c:<6d} | {pct:5.1f}%\")\n",
    "\n",
    "# (A) MALE ONLY: Version weighted by ‚Äúnumber of recommendations‚Äù\n",
    "male_only_genre_weighted = genre_counter_for_itemset(\n",
    "    male_only_items, male_item_counter, item2genres, weighted=True\n",
    ")\n",
    "print_genre_summary(male_only_genre_weighted, \"MALE ONLY ‚Äî Genre Sum (weighted by recommendation count)\", top_k=20)\n",
    "\n",
    "# (B) FEMALE ONLY: Version weighted by ‚Äúnumber of recommendations‚Äù\n",
    "female_only_genre_weighted = genre_counter_for_itemset(\n",
    "    female_only_items, female_item_counter, item2genres, weighted=True\n",
    ")\n",
    "print_genre_summary(female_only_genre_weighted, \"FEMALE ONLY ‚Äî Genre Sum (weighted by recommendation count)\", top_k=20)\n",
    "\n",
    "# (Optional) If you also want to view summation on an item-by-item basis (ignoring duplicate recommendations):\n",
    "male_only_genre_unweighted = genre_counter_for_itemset(\n",
    "    male_only_items, male_item_counter, item2genres, weighted=False\n",
    ")\n",
    "female_only_genre_unweighted = genre_counter_for_itemset(\n",
    "    female_only_items, female_item_counter, item2genres, weighted=False\n",
    ")\n",
    "print_genre_summary(male_only_genre_unweighted, \"MALE ONLY ‚Äî Genre Sum (unweighted, per-item once)\", top_k=20)\n",
    "print_genre_summary(female_only_genre_unweighted, \"FEMALE ONLY ‚Äî Genre Sum (unweighted, per-item once)\", top_k=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed917c",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# 2) Collect pred_count and user set for each scenario\n",
    "scenarios = [\n",
    "    ('case2','part1'),\n",
    "    ('case2','part5'),\n",
    "]\n",
    "pred_counts = {}\n",
    "pred_user_sets = {}\n",
    "for case, part in scenarios:\n",
    "    path = f\"{BASE_DIR}/predict_label_{part}.json\"\n",
    "    with open(path) as f:\n",
    "        raw = json.load(f)\n",
    "    # String key ‚Üí integer, predicted number\n",
    "    cnt_dict = {int(u): len(titles) for u, titles in raw.items()}\n",
    "    key = f\"{case}_{part}\"\n",
    "    pred_counts[key] = cnt_dict\n",
    "    pred_user_sets[key] = set(cnt_dict.keys())\n",
    "\n",
    "# 3) Common user set: train ‚à© label (requested method), converted to int type \n",
    "common_users = sorted(set(train_data) & set(ground_truth))\n",
    "\n",
    "# 4) Calculate total actual (interactions) total (common users only)\n",
    "actual_total = sum(len(ground_truth[u]) for u in common_users)\n",
    "print(f\"Actual total interactions (common users): {actual_total}, {len(common_users)}Î™Ö\\n\")\n",
    "\n",
    "# 5) Calculation and output by scenario + missing user count by case\n",
    "case_missing_users = {'case1': set(), 'case2': set()}\n",
    "common_users = [int(u) for u in common_users]  # Convert to integer type\n",
    "\n",
    "for case, part in scenarios:\n",
    "    key = f\"{case}_{part}\"\n",
    "    pred_users = pred_user_sets[key]\n",
    "    missing_users = set(common_users) - pred_users  # Common user not in prediction\n",
    "    case_missing_users[case].update(missing_users)  # Cumulative by case\n",
    "\n",
    "    pred_total = sum(pred_counts[key].get(u, 0) for u in common_users)\n",
    "    missing    = actual_total - pred_total\n",
    "    missing_pct= missing / actual_total * 100\n",
    "\n",
    "    print(f\"{key}:\")\n",
    "    print(f\"  Predicted total (common users) = {pred_total}\")\n",
    "    print(f\"  Missing interactions           = {missing} ({missing_pct:.2f}%)\")\n",
    "    print(f\"  Missing users (count)          = {len(missing_users)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffbb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_userlist_counts(path: str) -> dict[int, int]:\n",
    "    if not os.path.exists(path):\n",
    "        return {}\n",
    "    with open(path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    out = {}\n",
    "    if isinstance(data, dict):\n",
    "        for u, arr in data.items():\n",
    "            try:\n",
    "                uid = int(u)\n",
    "            except:\n",
    "                continue\n",
    "            if isinstance(arr, list):\n",
    "                out[uid] = len(arr)\n",
    "            elif isinstance(arr, int):\n",
    "                out[uid] = arr\n",
    "            else:\n",
    "                out[uid] = 0\n",
    "    return out\n",
    "\n",
    "def summarize_case2_part(part: int, max_step: int) -> pd.DataFrame:\n",
    "    rows, all_users = [], set()\n",
    "\n",
    "    # Collect user ID union at every step\n",
    "    for step in range(1, max_step+1):\n",
    "        miss_path = os.path.join(BASE_DIR, f\"missing_titles_ml-1m_case2_part{part}_step{step}.json\")\n",
    "        dup_path  = os.path.join(BASE_DIR, f\"skipped_duplicates_ml-1m_case2_part{part}_step{step}.json\")\n",
    "        miss = load_userlist_counts(miss_path)\n",
    "        dup  = load_userlist_counts(dup_path)\n",
    "        all_users |= set(miss.keys()) | set(dup.keys())\n",
    "\n",
    "    # Count by step (0 if none)\n",
    "    for step in range(1, max_step+1):\n",
    "        miss_path = os.path.join(BASE_DIR, f\"missing_titles_ml-1m_case2_part{part}_step{step}.json\")\n",
    "        dup_path  = os.path.join(BASE_DIR, f\"skipped_duplicates_ml-1m_case2_part{part}_step{step}.json\")\n",
    "        miss = load_userlist_counts(miss_path)\n",
    "        dup  = load_userlist_counts(dup_path)\n",
    "        for u in sorted(all_users):\n",
    "            rows.append({\n",
    "                \"user_id\": u,\n",
    "                \"step\": step,\n",
    "                \"missing_title\": int(miss.get(u, 0)),\n",
    "                \"skipped_duplicates\": int(dup.get(u, 0)),\n",
    "            })\n",
    "    return pd.DataFrame(rows).sort_values([\"user_id\",\"step\"]).reset_index(drop=True)\n",
    "\n",
    "# Generate summary\n",
    "df1  = summarize_case2_part(part=1,  max_step=1)\n",
    "df5  = summarize_case2_part(part=5,  max_step=5)\n",
    "\n",
    "# (1) part5: Print total table for each step\n",
    "step_totals1 = (\n",
    "    df1.groupby(\"step\")[[\"missing_title\",\"skipped_duplicates\"]]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .astype({\"step\": int, \"missing_title\": int, \"skipped_duplicates\": int})\n",
    ")\n",
    "print(step_totals1.to_string(index=False))\n",
    "\n",
    "step_totals5 = (\n",
    "    df5.groupby(\"step\")[[\"missing_title\",\"skipped_duplicates\"]]\n",
    "      .sum()\n",
    "      .reset_index()\n",
    "      .astype({\"step\": int, \"missing_title\": int, \"skipped_duplicates\": int})\n",
    ")\n",
    "print(step_totals5.to_string(index=False))\n",
    "\n",
    "\n",
    "# (2) Output parts overall totals (part5/part10 total total)\n",
    "overall1  = df1[[\"missing_title\",\"skipped_duplicates\"]].sum().astype(int).to_dict()\n",
    "overall5  = df5[[\"missing_title\",\"skipped_duplicates\"]].sum().astype(int).to_dict()\n",
    "\n",
    "parts_overall = pd.DataFrame([\n",
    "    {\"part\": 1,  **overall1},\n",
    "    {\"part\": 5,  **overall5},\n",
    "])[[\"part\",\"missing_title\",\"skipped_duplicates\"]]\n",
    "\n",
    "print(\"\\n[parts overall totals]\")\n",
    "print(parts_overall.to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af7422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "PARTS = 5\n",
    "PREFIX = \"predict_label_part5_step\"   # If necessary, change to predict_label\n",
    "TRY0, TRY1 = 0, 1\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        d = json.load(f)\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        uid = int(k)\n",
    "        if v is None:\n",
    "            out[uid] = []\n",
    "        elif isinstance(v, list):\n",
    "            out[uid] = [int(x) for x in v]\n",
    "        else:\n",
    "            out[uid] = [int(v)]\n",
    "    return out\n",
    "\n",
    "def compare_step(d0, d1, step):\n",
    "    all_users = sorted(set(d0.keys()) | set(d1.keys()))\n",
    "    rows = []\n",
    "    changed = 0\n",
    "    for u in all_users:\n",
    "        a = d0.get(u, [])\n",
    "        b = d1.get(u, [])\n",
    "        if a == b:\n",
    "            continue\n",
    "        changed += 1\n",
    "        sa, sb = set(a), set(b)\n",
    "        rows.append({\n",
    "            \"step\": step,\n",
    "            \"user_id\": u,\n",
    "            \"try0\": a,\n",
    "            \"try1\": b,\n",
    "            \"len_try0\": len(a),\n",
    "            \"len_try1\": len(b),\n",
    "            \"only_in_try0\": sorted(list(sa - sb)),\n",
    "            \"only_in_try1\": sorted(list(sb - sa)),\n",
    "            \"set_equal\": (sa == sb),\n",
    "        })\n",
    "    return changed, pd.DataFrame(rows)\n",
    "\n",
    "# ---- run ----\n",
    "summary_rows = []\n",
    "detail_dfs = []\n",
    "\n",
    "for step in range(1, PARTS+1):\n",
    "    p0 = os.path.join(BASE_DIR, f\"{PREFIX}{step}_try{TRY0}.json\")\n",
    "    p1 = os.path.join(BASE_DIR, f\"{PREFIX}{step}_try{TRY1}.json\")\n",
    "\n",
    "    d0 = load_json(p0)\n",
    "    d1 = load_json(p1)\n",
    "\n",
    "    changed, df_detail = compare_step(d0, d1, step)\n",
    "    union_users = len(set(d0.keys()) | set(d1.keys()))\n",
    "    exact_equal = union_users - changed\n",
    "\n",
    "    # Case where the set is the same but the order is different\n",
    "    order_only = int(df_detail[\"set_equal\"].sum()) if len(df_detail) else 0\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"step\": step,\n",
    "        \"users_try0\": len(d0),\n",
    "        \"users_try1\": len(d1),\n",
    "        \"users_union\": union_users,\n",
    "        \"users_changed(+1 per user)\": changed,\n",
    "        \"users_exact_equal\": exact_equal,\n",
    "        \"changed_set_equal_but_order_diff\": order_only,\n",
    "        \"file_try0\": p0,\n",
    "        \"file_try1\": p1,\n",
    "    })\n",
    "\n",
    "    detail_dfs.append(df_detail)\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows)\n",
    "df_detail_all = pd.concat(detail_dfs, ignore_index=True) if detail_dfs else pd.DataFrame()\n",
    "\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9141fb2",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7e33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Performance Check\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_prediction(pred_dict, ground_truth, common_users):\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    hit_ratio_list = []\n",
    "    ndcg_list = []\n",
    "    hit_item_list = []\n",
    "\n",
    "    for user_str, pred_items in pred_dict.items():\n",
    "        if user_str in common_users:\n",
    "            user = int(user_str)\n",
    "            if str(user) not in ground_truth:\n",
    "                continue\n",
    "\n",
    "            gt_items = set(ground_truth[str(user)])  # flat list of item_ids\n",
    "            if not gt_items:\n",
    "                continue\n",
    "\n",
    "            k = len(gt_items)\n",
    "            pred_topk = pred_items[:k]\n",
    "            hit_items = set(pred_topk) & gt_items\n",
    "\n",
    "            precision = len(hit_items) / k\n",
    "            recall = len(hit_items) / len(gt_items)\n",
    "            hit_ratio = 1.0 if len(hit_items) > 0 else 0.0\n",
    "\n",
    "            hit_item_list.append((user, k, len(hit_items)))\n",
    "\n",
    "            dcg = sum([1.0 / np.log2(i + 2) for i, item in enumerate(pred_topk) if item in gt_items])\n",
    "            idcg = sum([1.0 / np.log2(i + 2) for i in range(min(len(gt_items), k))])\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "            precision_list.append(precision)\n",
    "            recall_list.append(recall)\n",
    "            hit_ratio_list.append(hit_ratio)\n",
    "            ndcg_list.append(ndcg)\n",
    "\n",
    "    return {\n",
    "        \"precision\": np.mean(precision_list),\n",
    "        \"recall\": np.mean(recall_list),\n",
    "        \"hit_ratio\": np.mean(hit_ratio_list),\n",
    "        \"ndcg\": np.mean(ndcg_list),\n",
    "        \"hit_details\": hit_item_list\n",
    "    }\n",
    "\n",
    "with open(PRED_P1) as f:\n",
    "    pred_dict  = json.load(f)\n",
    "\n",
    "# evaluation\n",
    "result = evaluate_prediction(pred_dict, ground_truth, common_users)\n",
    "\n",
    "print(f\"Part1 Results:\")\n",
    "print(f\"precision: {result['precision']:.4f}, recall: {result['recall']:.4f}, \"\n",
    "        f\"hit_ratio: {result['hit_ratio']:.4f}, ndcg: {result['ndcg']:.4f}\")\n",
    "\n",
    "with open(PRED_P5) as f:\n",
    "    pred_dict  = json.load(f)\n",
    "\n",
    "# evaluation\n",
    "result = evaluate_prediction(pred_dict, ground_truth, common_users)\n",
    "\n",
    "print(f\"Part5 Results:\")\n",
    "print(f\"precision: {result['precision']:.4f}, recall: {result['recall']:.4f}, \"\n",
    "        f\"hit_ratio: {result['hit_ratio']:.4f}, ndcg: {result['ndcg']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c1569",
   "metadata": {},
   "source": [
    "## Popularity & Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139029d0",
   "metadata": {},
   "source": [
    "### LLMRec, A-LLMRec, Augmentation, TR CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55576f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ML-1M: Label/Part1/Part5 Popularity Change Boxplot (genre refactor, no diversity) =====\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Popularity from train_data\n",
    "# ---------------------------\n",
    "item_pop = Counter()\n",
    "for items in train_data.values():\n",
    "    for it in items:\n",
    "        item_pop[int(it)] += 1\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Load predictions\n",
    "# ---------------------------\n",
    "with open(PRED_P1) as f:\n",
    "    pred_p1 = json.load(f)\n",
    "with open(PRED_P5) as f:\n",
    "    pred_p5 = json.load(f)\n",
    "\n",
    "print(len(train_data), \"users in train_data\")\n",
    "print(len(ground_truth), \"users in ground_truth (Label)\")\n",
    "print(len(pred_p1), \"users in pred_p1\")\n",
    "print(len(pred_p5), \"users in pred_p5\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Common users\n",
    "#   - assume common_users is (train ‚à© gt) (already calculated in previous code)\n",
    "#   - Limited to users with both pred\n",
    "# ---------------------------\n",
    "common_users_pred = set(pred_p1.keys()) & set(pred_p5.keys())\n",
    "common_users_label = set(ground_truth.keys())\n",
    "common_users_final = set(map(str, common_users)) & common_users_pred & common_users_label\n",
    "print(\"len(common_users_final):\", len(common_users_final))\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Popularity metric\n",
    "# ---------------------------\n",
    "def avg_popularity_from_item_ids(user_items: dict[str, list[int]]):\n",
    "    \"\"\"Return: dict[uid(str)] -> avg_popularity\"\"\"\n",
    "    out = {}\n",
    "    for uid, items in user_items.items():\n",
    "        ids = [int(x) for x in items]\n",
    "        pops = [item_pop.get(it, 0) for it in ids]\n",
    "        out[str(uid)] = float(np.mean(pops)) if pops else 0.0\n",
    "    return out\n",
    "\n",
    "# Filter only common users\n",
    "label_common = {u: ground_truth[u] for u in common_users_final}\n",
    "p1_common    = {u: pred_p1[u]     for u in common_users_final}\n",
    "p5_common    = {u: pred_p5[u]     for u in common_users_final}\n",
    "\n",
    "m_label = avg_popularity_from_item_ids(label_common)\n",
    "m_p1    = avg_popularity_from_item_ids(p1_common)\n",
    "m_p5    = avg_popularity_from_item_ids(p5_common)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Œî transitions: Label->P1, Label->P5, P1->P5 (Popularity only)\n",
    "# ---------------------------\n",
    "d_label_p1 = []\n",
    "d_label_p5 = []\n",
    "d_p1_p5    = []\n",
    "\n",
    "for u in common_users_final:\n",
    "    lp = m_label.get(u, 0.0)\n",
    "    p1 = m_p1.get(u, 0.0)\n",
    "    p5 = m_p5.get(u, 0.0)\n",
    "\n",
    "    d_label_p1.append(p1 - lp)\n",
    "    d_label_p5.append(p5 - lp)\n",
    "    d_p1_p5.append(p5 - p1)\n",
    "\n",
    "df_delta = pd.DataFrame({\n",
    "    \"delta\": d_label_p1 + d_label_p5 + d_p1_p5,\n",
    "    \"transition\": ([\"Label ‚Üí Part1\"] * len(d_label_p1))\n",
    "                + ([\"Label ‚Üí Part5\"] * len(d_label_p5))\n",
    "                + ([\"Part1 ‚Üí Part5\"] * len(d_p1_p5)),\n",
    "    \"metric\": [\"Popularity\"] * (len(d_label_p1) + len(d_label_p5) + len(d_p1_p5)),\n",
    "    \"case\": [\"ML-1M\"] * (len(d_label_p1) + len(d_label_p5) + len(d_p1_p5)),\n",
    "})\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Plot\n",
    "# ---------------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(9, 6))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=df_delta,\n",
    "    x=\"transition\",\n",
    "    y=\"delta\",\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "\n",
    "ax.set_title(\"Popularity Change ‚Äî Label / Part1 / Part5 (ML-1M)\")\n",
    "ax.set_xlabel(\"Transition\")\n",
    "ax.set_ylabel(\"Œî Average Popularity (train-based counts)\")\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Summary (Popularity Œî)\")\n",
    "print(df_delta.groupby(\"transition\")[\"delta\"].describe().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee494e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ML-1M: Label vs Part1 (4 baselines) Popularity Change Boxplot =====\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Paths (Part1 predictions of 4 baselines)\n",
    "# ---------------------------\n",
    "BASELINES_P1 = {\n",
    "    \"A-LLMRec\": \"data/ml-1m/A-LLMRec_format/A-LLMRec_results/predict_label_part1.json\",\n",
    "    \"LLMRec\": \"data/ml-1m/ml-1m_llmrec_format/predict_label_part1.json\",\n",
    "    \"Augmentation\": \"data/ml-1m/Augmentation_format/predict_label_part1.json\",\n",
    "    #\"TraditionalCF\": \"data/ml-1m/traditionalCF/predict_label_part1.json\",\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Popularity from train_data\n",
    "# ---------------------------\n",
    "def build_item_popularity(train_data: dict) -> Counter:\n",
    "    item_pop = Counter()\n",
    "    for items in train_data.values():\n",
    "        for it in items:\n",
    "            item_pop[int(it)] += 1\n",
    "    return item_pop\n",
    "\n",
    "item_pop = build_item_popularity(train_data)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Utilities\n",
    "# ---------------------------\n",
    "def load_json(path: str) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def avg_popularity_from_item_ids(user_items: dict, item_pop: Counter) -> dict[str, float]:\n",
    "    \"\"\"user_items: dict[uid]->list[item_id], return dict[str(uid)]->avg_popularity\"\"\"\n",
    "    out = {}\n",
    "    for uid, items in user_items.items():\n",
    "        if items is None:\n",
    "            out[str(uid)] = 0.0\n",
    "            continue\n",
    "        ids = [int(x) for x in items]\n",
    "        pops = [item_pop.get(it, 0) for it in ids]\n",
    "        out[str(uid)] = float(np.mean(pops)) if len(pops) > 0 else 0.0\n",
    "    return out\n",
    "\n",
    "def filter_dict_by_users(d: dict, users: set[str]) -> dict:\n",
    "    return {u: d[u] for u in users if u in d}\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Common users base\n",
    "#   - common_users: Assume (train ‚à© gt) (use existing code as is)\n",
    "# ---------------------------\n",
    "common_users_base = set(map(str, common_users))  # Unify uid into str\n",
    "label_users = set(map(str, ground_truth.keys()))\n",
    "common_users_base = common_users_base & label_users\n",
    "\n",
    "print(\"len(common_users_base):\", len(common_users_base))\n",
    "\n",
    "# Label popularity (fixed)\n",
    "label_common = filter_dict_by_users({str(k): v for k, v in ground_truth.items()}, common_users_base)\n",
    "m_label = avg_popularity_from_item_ids(label_common, item_pop)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) For each baseline: compute Œî (Part1 - Label)\n",
    "# ---------------------------\n",
    "rows = []\n",
    "baseline_pred = {}\n",
    "\n",
    "for bname, ppath in BASELINES_P1.items():\n",
    "    pred = load_json(ppath)\n",
    "    pred = {str(k): v for k, v in pred.items()}  # uid str unity\n",
    "    baseline_pred[bname] = pred\n",
    "\n",
    "    # Only users who actually have pred in this baseline (must also have Label)\n",
    "    users_b = common_users_base & set(pred.keys())\n",
    "\n",
    "    # user filter\n",
    "    pred_common = filter_dict_by_users(pred, users_b)\n",
    "\n",
    "    # popularity metric\n",
    "    m_pred = avg_popularity_from_item_ids(pred_common, item_pop)\n",
    "\n",
    "    # delta\n",
    "    for u in users_b:\n",
    "        lp = m_label.get(u, 0.0)\n",
    "        pp = m_pred.get(u, 0.0)\n",
    "        rows.append({\n",
    "            \"uid\": u,\n",
    "            \"baseline\": bname,\n",
    "            \"delta\": pp - lp,\n",
    "            \"metric\": \"Popularity\",\n",
    "            \"case\": \"ML-1M\",\n",
    "        })\n",
    "\n",
    "    print(f\"{bname}: users used = {len(users_b)}\")\n",
    "\n",
    "df_delta = pd.DataFrame(rows)\n",
    "print(\"df_delta shape:\", df_delta.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Plot (single plot comparing 4 baselines)\n",
    "# ---------------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=df_delta,\n",
    "    x=\"baseline\",\n",
    "    y=\"delta\",\n",
    "    order=list(BASELINES_P1.keys()),\n",
    ")\n",
    "\n",
    "ax.set_title(\"Popularity Change ‚Äî GroundTruth ‚Üí Simulation Data (4 baselines, ML-1M)\")\n",
    "ax.set_xlabel(\"Baseline (Part1 prediction)\")\n",
    "ax.set_ylabel(\"Œî Average Popularity (Part1 - Label, train-based counts)\")\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt save\n",
    "plt.savefig(\"data/ml-1m/plots/popularity.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Summary\n",
    "# ---------------------------\n",
    "print(\"Summary (Œî Popularity = Part1 - Label)\")\n",
    "print(df_delta.groupby(\"baseline\")[\"delta\"].describe().round(3))\n",
    "print(\"plot is saved -> data/ml-1m/plots/popularity.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== ML-1M: Part1 -> Part5 (4 baselines) Popularity Change Boxplot =====\n",
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ---------------------------\n",
    "# 0) Paths (Part1 predictions of 4 baselines)\n",
    "# ---------------------------\n",
    "BASELINES_P1 = {\n",
    "    \"A-LLMRec\": \"data/ml-1m/A-LLMRec_format/A-LLMRec_results/predict_label_part1.json\",\n",
    "    \"LLMRec\": \"data/ml-1m/ml-1m_llmrec_format/predict_label_part1.json\",\n",
    "    \"Augmentation\": \"data/ml-1m/Augmentation_format/predict_label_part1.json\",\n",
    "    #\"TraditionalCF\": \"data/ml-1m/traditionalCF/predict_label_part1.json\",\n",
    "}\n",
    "\n",
    "def part1_to_part5_path(p1_path: str) -> str:\n",
    "    return p1_path.replace(\"predict_label_part1.json\", \"predict_label_part5.json\")\n",
    "\n",
    "BASELINES_P5 = {k: part1_to_part5_path(v) for k, v in BASELINES_P1.items()}\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Popularity from train_data (same)\n",
    "# ---------------------------\n",
    "def build_item_popularity(train_data: dict) -> Counter:\n",
    "    item_pop = Counter()\n",
    "    for items in train_data.values():\n",
    "        for it in items:\n",
    "            item_pop[int(it)] += 1\n",
    "    return item_pop\n",
    "\n",
    "item_pop = build_item_popularity(train_data)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Utilities\n",
    "# ---------------------------\n",
    "def load_json(path: str) -> dict:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(path)\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def avg_popularity_from_item_ids(user_items: dict, item_pop: Counter) -> dict[str, float]:\n",
    "    \"\"\"user_items: dict[uid]->list[item_id], return dict[str(uid)]->avg_popularity\"\"\"\n",
    "    out = {}\n",
    "    for uid, items in user_items.items():\n",
    "        if items is None:\n",
    "            out[str(uid)] = 0.0\n",
    "            continue\n",
    "        if isinstance(items, int):\n",
    "            items = [items]\n",
    "        ids = [int(x) for x in items]\n",
    "        pops = [item_pop.get(it, 0) for it in ids]\n",
    "        out[str(uid)] = float(np.mean(pops)) if len(pops) > 0 else 0.0\n",
    "    return out\n",
    "\n",
    "def filter_dict_by_users(d: dict, users: set[str]) -> dict:\n",
    "    return {u: d[u] for u in users if u in d}\n",
    "\n",
    "# ---------------------------\n",
    "# 3) For each baseline: compute Œî (Part5 - Part1)\n",
    "#   - Assume common_users uses your existing definition as is.\n",
    "# ---------------------------\n",
    "common_users_base = set(map(str, common_users))  # uid str unity\n",
    "print(\"len(common_users_base):\", len(common_users_base))\n",
    "\n",
    "rows = []\n",
    "\n",
    "for bname in BASELINES_P1.keys():\n",
    "    p1_path = BASELINES_P1[bname]\n",
    "    p5_path = BASELINES_P5[bname]\n",
    "\n",
    "    pred_p1 = load_json(p1_path)\n",
    "    pred_p5 = load_json(p5_path)\n",
    "\n",
    "    pred_p1 = {str(k): v for k, v in pred_p1.items()}\n",
    "    pred_p5 = {str(k): v for k, v in pred_p5.items()}\n",
    "\n",
    "    # Users that exist in both Part1 and Part5 + common_users\n",
    "    users_b = common_users_base & set(pred_p1.keys()) & set(pred_p5.keys())\n",
    "\n",
    "    pred_p1_common = filter_dict_by_users(pred_p1, users_b)\n",
    "    pred_p5_common = filter_dict_by_users(pred_p5, users_b)\n",
    "\n",
    "    m_p1 = avg_popularity_from_item_ids(pred_p1_common, item_pop)\n",
    "    m_p5 = avg_popularity_from_item_ids(pred_p5_common, item_pop)\n",
    "\n",
    "    for u in users_b:\n",
    "        p1v = m_p1.get(u, 0.0)\n",
    "        p5v = m_p5.get(u, 0.0)\n",
    "        rows.append({\n",
    "            \"uid\": u,\n",
    "            \"baseline\": bname,\n",
    "            \"delta\": p5v - p1v,   # Core: Part5 - Part1\n",
    "            \"metric\": \"Popularity\",\n",
    "            \"case\": \"ML-1M\",\n",
    "        })\n",
    "\n",
    "    print(f\"{bname}: users used = {len(users_b)}\")\n",
    "    print(f\"  - p1: {p1_path}\")\n",
    "    print(f\"  - p5: {p5_path}\")\n",
    "\n",
    "df_delta = pd.DataFrame(rows)\n",
    "print(\"df_delta shape:\", df_delta.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Plot (single plot comparing 4 baselines)\n",
    "# ---------------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = sns.boxplot(\n",
    "    data=df_delta,\n",
    "    x=\"baseline\",\n",
    "    y=\"delta\",\n",
    "    order=list(BASELINES_P1.keys()),\n",
    ")\n",
    "\n",
    "ax.set_title(\"Popularity Change ‚Äî Recommendation ‚Üí FeedbackLoop (3 baselines, ML-1M)\")\n",
    "ax.set_xlabel(\"Baseline\")\n",
    "ax.set_ylabel(\"Œî Average Popularity (Part5 - Part1, train-based counts)\")\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "out_path = \"data/ml-1m/plots/popularity_p1_to_p5.png\"\n",
    "plt.savefig(out_path, dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Summary\n",
    "# ---------------------------\n",
    "print(\"Summary (Œî Popularity = Part5 - Part1)\")\n",
    "print(df_delta.groupby(\"baseline\")[\"delta\"].describe().round(3))\n",
    "print(\"plot is saved ->\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3353dc",
   "metadata": {},
   "source": [
    "# RQ4\n",
    "Analysis of risks that may arise from embeddings created through feedback loops (polarization, filter bubbles, etc.)\n",
    "\n",
    "Includes: LLMRec, A-LLMRec, Augmentation, TR_CF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f224d5",
   "metadata": {},
   "source": [
    "## Polarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49d313",
   "metadata": {},
   "source": [
    "### Save 5 steps at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc18e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, json, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# =========================\n",
    "# Path/Settings\n",
    "# =========================\n",
    "STEPS    = list(range(PARTS))\n",
    "CLUSTER_STEP = STEPS[-1]              # Perform KMeans in the final step\n",
    "OUT_DIR  = os.path.join(BASE_DIR, \"figs_tsne_cluster_progress\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TSNE_PERPLEXITY      = 30\n",
    "TSNE_RANDOM_STATE    = 42\n",
    "KMEANS_RANDOM_STATE  = 42\n",
    "K_USERS = 2\n",
    "K_ITEMS = 2\n",
    "\n",
    "# (Optional) t-SNE accelerated sampling\n",
    "SUBSAMPLE_USERS_FOR_TSNE = None  # Example: 5000\n",
    "SUBSAMPLE_ITEMS_FOR_TSNE = None  # Example: 5000\n",
    "\n",
    "# =========================\n",
    "# utility\n",
    "# =========================\n",
    "def load_users_items_for_step(base_dir: str, parts: int, step: int):\n",
    "    # upath = os.path.join(base_dir, f\"u_i_embedding_in_sasrec/ml-1m_case2_parts{parts}/user_emb_part_step0{step}.npy\")\n",
    "    # ipath = os.path.join(base_dir, f\"u_i_embedding_in_sasrec/ml-1m_case2_parts{parts}/item_emb_part0{step}.npy\")\n",
    "    upath = os.path.join(base_dir, f\"user_emb_part{parts}_step{step}.npy\")\n",
    "    ipath = os.path.join(base_dir, f\"item_emb_part{parts}_step{step}.npy\")\n",
    "    if not os.path.exists(upath):\n",
    "        raise FileNotFoundError(upath)\n",
    "    U = np.load(upath)  # shape: (num_users_step, d)\n",
    "    I = np.load(ipath) if os.path.exists(ipath) else None  # shape: (num_items_step, d) or None\n",
    "    if U.ndim != 2:\n",
    "        raise ValueError(f\"user_emb shape invalid at step{step}: {U.shape}\")\n",
    "    if I is not None and I.ndim != 2:\n",
    "        raise ValueError(f\"item_emb shape invalid at step{step}: {I.shape}\")\n",
    "    return U, I\n",
    "\n",
    "\n",
    "def align_users_for_step(common_users, num_users_step: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    common_users: Allow all set/list/ndarray.\n",
    "    num_users_step: User axis length of this step (ex. user_by_step[s].shape[0])\n",
    "    return: [N_u'] Sorted array of effective user ids\n",
    "    \"\"\"\n",
    "    # 1) Unify input as NumPy int64 array\n",
    "    if isinstance(common_users, (set, list, tuple)):\n",
    "        cu = np.asarray(sorted(common_users), dtype=np.int64)  # Alignment recommended for reproducibility\n",
    "    else:\n",
    "        cu = np.asarray(common_users, dtype=np.int64)\n",
    "\n",
    "    if cu.size == 0:\n",
    "        return cu\n",
    "\n",
    "    # 2) Effective range masking\n",
    "    mask = (cu >= 0) & (cu < num_users_step)\n",
    "    return cu[mask]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Load each step embedding\n",
    "# =========================\n",
    "print(\"Loading augmentation embeddings per step...\")\n",
    "user_by_step = {}\n",
    "item_by_step = {}\n",
    "dims = set()\n",
    "\n",
    "for s in STEPS:\n",
    "    U, I = load_users_items_for_step(BASE_DIR, PARTS, s)\n",
    "    user_by_step[s] = U\n",
    "    item_by_step[s] = I\n",
    "    dims.add(U.shape[1])\n",
    "    if I is not None:\n",
    "        dims.add(I.shape[1])\n",
    "    print(f\"  - step{s}: users={U.shape}, items={None if I is None else I.shape}\")\n",
    "\n",
    "if len(dims) != 1:\n",
    "    raise RuntimeError(f\"ÏûÑÎ≤†Îî© Ï∞®ÏõêÏù¥ stepÎ≥ÑÎ°ú Îã§Î¶ÖÎãàÎã§: {dims}\")\n",
    "D = dims.pop()\n",
    "\n",
    "# =========================\n",
    "# 2) train‚à©label common user calculation + sorting by step\n",
    "# =========================\n",
    "print(f\"common users: {len(common_users)}\")\n",
    "\n",
    "aligned_users_by_step = {s: align_users_for_step(common_users, user_by_step[s].shape[0])\n",
    "                         for s in STEPS}\n",
    "print({s: len(aligned_users_by_step[s]) for s in STEPS})\n",
    "\n",
    "# =========================\n",
    "# 3) Sort common parts of items (minimum length)\n",
    "# =========================\n",
    "item_lengths = [item_by_step[s].shape[0] for s in STEPS if item_by_step[s] is not None]\n",
    "if len(item_lengths) == 0:\n",
    "    I_base = 0\n",
    "else:\n",
    "    I_base = min(item_lengths)\n",
    "    uniq = set(item_lengths)\n",
    "    if len(uniq) != 1:\n",
    "        print(f\"[warn] stepÎ≥Ñ ÏïÑÏù¥ÌÖú ÏàòÍ∞Ä Îã§Î¶ÖÎãàÎã§: {uniq} ‚Üí ÏïûÏóêÏÑú {I_base}Í∞úÎßå Í≥µÌÜµ ÏÇ¨Ïö©\")\n",
    "    for s in STEPS:\n",
    "        I = item_by_step[s]\n",
    "        item_by_step[s] = None if I is None or I.shape[0] < I_base else I[:I_base]\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "if I_base > 0:\n",
    "    if SUBSAMPLE_ITEMS_FOR_TSNE is not None and SUBSAMPLE_ITEMS_FOR_TSNE < I_base:\n",
    "        keep_items = np.sort(rng.choice(I_base, size=SUBSAMPLE_ITEMS_FOR_TSNE, replace=False))\n",
    "    else:\n",
    "        keep_items = np.arange(I_base)\n",
    "else:\n",
    "    keep_items = np.array([], dtype=int)\n",
    "\n",
    "# =========================\n",
    "# 4) In the final step, KMeans ‚Üí label creation\n",
    "# =========================\n",
    "print(f\"KMeans on final step: step{CLUSTER_STEP}\")\n",
    "U_final = user_by_step[CLUSTER_STEP]\n",
    "ids_final = aligned_users_by_step[CLUSTER_STEP]\n",
    "if len(ids_final) < K_USERS:\n",
    "    raise RuntimeError(f\"ÏµúÏ¢Ö stepÏóêÏÑú ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ Í∞ÄÎä•Ìïú Í≥µÌÜµ Ïú†Ï†ÄÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§: {len(ids_final)}\")\n",
    "\n",
    "emb_final_users = U_final[ids_final, :]  # (|ids_final|, D)\n",
    "user_kmeans = KMeans(n_clusters=K_USERS, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "user_labels_final = user_kmeans.fit_predict(emb_final_users)\n",
    "user_center_idx = {k: user_labels_final == k for k in range(K_USERS)}\n",
    "\n",
    "# Item KMeans (final step common item)\n",
    "if item_by_step[CLUSTER_STEP] is not None and len(keep_items) > 0:\n",
    "    I_final = item_by_step[CLUSTER_STEP][keep_items]  # (|keep_items|, D)\n",
    "    item_kmeans = KMeans(n_clusters=K_ITEMS, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "    item_labels_final = item_kmeans.fit_predict(I_final)\n",
    "else:\n",
    "    item_kmeans = None\n",
    "    item_labels_final = None\n",
    "    print(\"[warn] Omitted item clustering as there are no final step items\")\n",
    "\n",
    "# =========================\n",
    "# 5) Configuring data blocks to be put into t-SNE (users + items across steps)\n",
    "# =========================\n",
    "print(\"Preparing t-SNE blocks...\")\n",
    "ids_for_tsne = {}      # step -> user_id array (reflects sampling)\n",
    "users_for_tsne = {}    # step -> user_emb array\n",
    "item_for_tsne  = {}    # step -> item_emb array (corresponds to common keep_items)\n",
    "\n",
    "# User sampling criteria: ids_final set of final step\n",
    "if SUBSAMPLE_USERS_FOR_TSNE is not None and SUBSAMPLE_USERS_FOR_TSNE < len(ids_final):\n",
    "    sample_users = np.sort(rng.choice(ids_final, size=SUBSAMPLE_USERS_FOR_TSNE, replace=False))\n",
    "else:\n",
    "    sample_users = ids_final\n",
    "\n",
    "for s in STEPS:\n",
    "    ids_s = aligned_users_by_step[s]\n",
    "    if len(ids_s) == 0:\n",
    "        ids_for_tsne[s] = np.array([], dtype=np.int64)\n",
    "        users_for_tsne[s] = np.empty((0, D), dtype=np.float32)\n",
    "    else:\n",
    "        mask = np.isin(ids_s, sample_users)\n",
    "        ids_for_tsne[s] = ids_s[mask]\n",
    "        users_for_tsne[s] = user_by_step[s][ids_for_tsne[s], :]\n",
    "\n",
    "    I = item_by_step[s]\n",
    "    item_for_tsne[s] = None if I is None or len(keep_items) == 0 else I[keep_items]\n",
    "\n",
    "# =========================\n",
    "# 6) Perform t-SNE (summing users/items of all steps)\n",
    "# =========================\n",
    "print(\"Fitting joint t-SNE on users + items across steps\")\n",
    "blocks = []\n",
    "slices = {}  # step -> {\"user\": (st,en), \"item\": (st,en) or None}\n",
    "cursor = 0\n",
    "\n",
    "for s in STEPS:\n",
    "    Ublk = users_for_tsne[s]\n",
    "    blocks.append(Ublk)\n",
    "    u_st, u_en = cursor, cursor + len(Ublk)\n",
    "    cursor = u_en\n",
    "\n",
    "    Iblk = item_for_tsne[s]\n",
    "    if Iblk is not None and len(Iblk) > 0:\n",
    "        blocks.append(Iblk)\n",
    "        i_st, i_en = cursor, cursor + len(Iblk)\n",
    "        cursor = i_en\n",
    "        slices[s] = {\"user\": (u_st, u_en), \"item\": (i_st, i_en)}\n",
    "    else:\n",
    "        slices[s] = {\"user\": (u_st, u_en), \"item\": None}\n",
    "\n",
    "if len(blocks) == 0 or sum(len(b) for b in blocks) < 3:\n",
    "    raise RuntimeError(\"There are too few samples for t-SNE.\")\n",
    "\n",
    "X_all = np.vstack(blocks)\n",
    "perp = min(TSNE_PERPLEXITY, max(5, (len(X_all) - 1)//3))\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=perp,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    "    random_state=TSNE_RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    verbose=1\n",
    ")\n",
    "X_tsne = tsne.fit_transform(X_all)\n",
    "\n",
    "# Coordinate range (common axis)\n",
    "xmin, ymin = X_tsne.min(axis=0)\n",
    "xmax, ymax = X_tsne.max(axis=0)\n",
    "xpad = 0.05 * (xmax - xmin) if xmax > xmin else 0.5\n",
    "ypad = 0.05 * (ymax - ymin) if ymax > ymin else 0.5\n",
    "\n",
    "# Final step center coordinates (user/item)\n",
    "palette = [\"tab:blue\",\"tab:orange\",\"tab:green\",\"tab:red\",\"tab:purple\",\n",
    "           \"tab:brown\",\"tab:pink\",\"tab:gray\",\"tab:olive\",\"tab:cyan\"]\n",
    "\n",
    "def compute_centers(coords: np.ndarray, labels: np.ndarray, K: int):\n",
    "    centers = {}\n",
    "    for k in range(K):\n",
    "        mask = (labels == k)\n",
    "        if np.any(mask):\n",
    "            centers[k] = coords[mask].mean(axis=0)\n",
    "    return centers\n",
    "\n",
    "# Final step user/item coordinate slice\n",
    "u_st_f, u_en_f = slices[CLUSTER_STEP][\"user\"]\n",
    "coords_u_final = X_tsne[u_st_f:u_en_f]\n",
    "# user_labels_final is a label for ids_final (=common user in the final step before sampling)\n",
    "# Since t-SNE may only contain sample_users, reorder the labels according to the order ids_for_tsne[CLUSTER_STEP]\n",
    "ids_tsne_final = ids_for_tsne[CLUSTER_STEP]\n",
    "# Each user_id in ids_tsne_final finds and maps the index within ids_final.\n",
    "idx_map_final = {int(u): i for i, u in enumerate(ids_final.tolist())}\n",
    "labels_u_for_plot = np.array([user_labels_final[idx_map_final[int(u)]] for u in ids_tsne_final], dtype=int)\n",
    "user_centers = compute_centers(coords_u_final, labels_u_for_plot, K_USERS)\n",
    "\n",
    "if slices[CLUSTER_STEP][\"item\"] is not None and item_labels_final is not None:\n",
    "    i_st_f, i_en_f = slices[CLUSTER_STEP][\"item\"]\n",
    "    coords_i_final = X_tsne[i_st_f:i_en_f]\n",
    "    item_centers = compute_centers(coords_i_final, item_labels_final, K_ITEMS)\n",
    "else:\n",
    "    item_centers = {}\n",
    "\n",
    "# =========================\n",
    "# 7) Save picture ‚Äî Users only / Items only / Users+Items\n",
    "# =========================\n",
    "def plot_users_only():\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        u_st, u_en = slices[s][\"user\"]\n",
    "        coords_u = X_tsne[u_st:u_en]\n",
    "        # Color with label based on final step\n",
    "        if s == CLUSTER_STEP:\n",
    "            labels_plot = labels_u_for_plot\n",
    "        else:\n",
    "            # Different steps use the same label for the same user (sampled ids)\n",
    "            # ids_for_tsne[s] -> maps to ids_tsne_final\n",
    "            ids_tsne_s = ids_for_tsne[s]\n",
    "            labels_plot = np.empty((len(ids_tsne_s),), dtype=int)\n",
    "            for i, uid in enumerate(ids_tsne_s):\n",
    "                labels_plot[i] = labels_u_for_plot[np.where(ids_tsne_final == uid)[0][0]]\n",
    "        for k in range(K_USERS):\n",
    "            mk = (labels_plot == k)\n",
    "            if np.any(mk):\n",
    "                ax.scatter(coords_u[mk,0], coords_u[mk,1], s=8, alpha=0.7,\n",
    "                           c=palette[k % len(palette)], label=f\"Users: C{k}\")\n",
    "        if s == CLUSTER_STEP:\n",
    "            for k, ctr in user_centers.items():\n",
    "                ax.scatter(ctr[0], ctr[1], s=120, marker=\"X\",\n",
    "                           c=palette[k % len(palette)], edgecolor=\"k\")\n",
    "        ax.set_title(f\"step{s} (Users only, colored by step{CLUSTER_STEP})\")\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        # Uncomment if you want to use a fixed axis\n",
    "        # ax.set_xlim(xmin - xpad, xmax + xpad)\n",
    "        # ax.set_ylim(ymin - ypad, ymax + ypad)\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_USERS_ONLY.png\")\n",
    "    plt.savefig(out, dpi=220); plt.close()\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "def plot_items_only():\n",
    "    has_any = any(item_for_tsne[s] is not None for s in STEPS) and (item_labels_final is not None)\n",
    "    if not has_any:\n",
    "        print(\"  - Item only plot omitted (no item embedding/label)\")\n",
    "        return\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        if slices[s][\"item\"] is not None:\n",
    "            i_st, i_en = slices[s][\"item\"]\n",
    "            coords_i = X_tsne[i_st:i_en]\n",
    "            # Use item_labels_final of the final step as is (corresponds to common keep_items)\n",
    "            for k in range(K_ITEMS):\n",
    "                mk = (item_labels_final == k)\n",
    "                if np.any(mk):\n",
    "                    ax.scatter(coords_i[mk,0], coords_i[mk,1], s=5, alpha=0.35,\n",
    "                               c=palette[(k+2) % len(palette)], label=f\"Items: C{k}\")\n",
    "            if s == CLUSTER_STEP and len(item_centers) > 0:\n",
    "                for k, ctr in item_centers.items():\n",
    "                    ax.scatter(ctr[0], ctr[1], s=110, marker=\"D\",\n",
    "                               c=palette[(k+2) % len(palette)], edgecolor=\"k\")\n",
    "        ax.set_title(f\"step{s} (Items only, colored by step{CLUSTER_STEP})\")\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_ITEMS_ONLY.png\")\n",
    "    plt.savefig(out, dpi=220); plt.close()\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "def plot_users_items():\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        # item\n",
    "        if slices[s][\"item\"] is not None and item_labels_final is not None:\n",
    "            i_st, i_en = slices[s][\"item\"]\n",
    "            coords_i = X_tsne[i_st:i_en]\n",
    "            for k in range(K_ITEMS):\n",
    "                mk = (item_labels_final == k)\n",
    "                if np.any(mk):\n",
    "                    ax.scatter(coords_i[mk,0], coords_i[mk,1], s=5, alpha=0.35,\n",
    "                               c=palette[(k+2) % len(palette)], label=f\"Items: C{k}\")\n",
    "            if s == CLUSTER_STEP and len(item_centers) > 0:\n",
    "                for k, ctr in item_centers.items():\n",
    "                    ax.scatter(ctr[0], ctr[1], s=110, marker=\"D\",\n",
    "                               c=palette[(k+2) % len(palette)], edgecolor=\"k\")\n",
    "        # posthumous work\n",
    "        u_st, u_en = slices[s][\"user\"]\n",
    "        coords_u = X_tsne[u_st:u_en]\n",
    "        # User label configuration for each step\n",
    "        if s == CLUSTER_STEP:\n",
    "            labels_plot = labels_u_for_plot\n",
    "        else:\n",
    "            ids_tsne_s = ids_for_tsne[s]\n",
    "            labels_plot = np.empty((len(ids_tsne_s),), dtype=int)\n",
    "            for i, uid in enumerate(ids_tsne_s):\n",
    "                labels_plot[i] = labels_u_for_plot[np.where(ids_tsne_final == uid)[0][0]]\n",
    "        for k in range(K_USERS):\n",
    "            mk = (labels_plot == k)\n",
    "            if np.any(mk):\n",
    "                ax.scatter(coords_u[mk,0], coords_u[mk,1], s=8, alpha=0.7,\n",
    "                           c=palette[k % len(palette)], label=f\"Users: C{k}\")\n",
    "        if s == CLUSTER_STEP:\n",
    "            for k, ctr in user_centers.items():\n",
    "                ax.scatter(ctr[0], ctr[1], s=120, marker=\"X\",\n",
    "                           c=palette[k % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        ax.set_title(f\"step{s} (Users & Items, colored by step{CLUSTER_STEP})\")\n",
    "        ax.set_xlim(xmin - xpad, xmax + xpad)\n",
    "        ax.set_ylim(ymin - ypad, ymax + ypad)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "    plt.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_USERS_ITEMS.png\")\n",
    "    plt.savefig(out, dpi=220); plt.close()\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "print(\"Plotting...\")\n",
    "plot_users_only()\n",
    "plot_items_only()\n",
    "plot_users_items()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dabf8c0",
   "metadata": {},
   "source": [
    "### Save each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c29c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# ============================================================\n",
    "# Values ‚Äã‚Äãthat must be predefined in the user environment\n",
    "#   - BASE_DIR: Directory with embeddings (.npy)\n",
    "#   - PARTS: Number of steps (e.g. 5)\n",
    "#   - common_users: train‚à©label common user id set/list/ndarray\n",
    "# ============================================================\n",
    "# BASE_DIR = \"...\"\n",
    "# PARTS = 5\n",
    "# common_users = ...\n",
    "\n",
    "# =========================\n",
    "# Path/Settings\n",
    "# =========================\n",
    "STEPS = list(range(PARTS))\n",
    "CLUSTER_STEP = STEPS[-1]  # Perform KMeans in the final step\n",
    "OUT_DIR = os.path.join(BASE_DIR, \"figs_tsne_cluster_progress\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TSNE_PERPLEXITY = 30\n",
    "TSNE_RANDOM_STATE = 42\n",
    "KMEANS_RANDOM_STATE = 42\n",
    "K_USERS = 2\n",
    "K_ITEMS = 2\n",
    "\n",
    "# (Optional) t-SNE accelerated sampling\n",
    "SUBSAMPLE_USERS_FOR_TSNE = None  # Example: 5000\n",
    "SUBSAMPLE_ITEMS_FOR_TSNE = None  # Example: 5000\n",
    "\n",
    "# (Global) Remove top p% of distances from centroid in t-SNE 2D ‚Äúplotting only‚Äù\n",
    "GLOBAL_TRIM_TOP_PCT = 0.0   # If you want global trimming, set it to 0.005, etc. (if 0, global trim off)\n",
    "\n",
    "# (Key) Step-by-step (local) trimming: Remove isolated points in step 1.\n",
    "STEP_TRIM_TOP_PCT = 0.01        # Basic: Remove top 0.2% at each step\n",
    "STEP_TRIM_TOP_PCT_STEP1 = 0.01  # step==remove 10,000 times harder (0.5%)\n",
    "\n",
    "# =========================\n",
    "# utility\n",
    "# =========================\n",
    "def load_users_items_for_step(base_dir: str, parts: int, step: int):\n",
    "    upath = os.path.join(base_dir, f\"user_emb_part{parts}_step{step}.npy\")\n",
    "    ipath = os.path.join(base_dir, f\"item_emb_part{parts}_step{step}.npy\")\n",
    "    if not os.path.exists(upath):\n",
    "        raise FileNotFoundError(upath)\n",
    "    U = np.load(upath)  # (num_users_step, d)\n",
    "    I = np.load(ipath) if os.path.exists(ipath) else None  # (num_items_step, d) or None\n",
    "    if U.ndim != 2:\n",
    "        raise ValueError(f\"user_emb shape invalid at step{step}: {U.shape}\")\n",
    "    if I is not None and I.ndim != 2:\n",
    "        raise ValueError(f\"item_emb shape invalid at step{step}: {I.shape}\")\n",
    "    return U, I\n",
    "\n",
    "\n",
    "def align_users_for_step(common_users, num_users_step: int) -> np.ndarray:\n",
    "    if isinstance(common_users, (set, list, tuple)):\n",
    "        cu = np.asarray(sorted(common_users), dtype=np.int64)\n",
    "    else:\n",
    "        cu = np.asarray(common_users, dtype=np.int64)\n",
    "\n",
    "    if cu.size == 0:\n",
    "        return cu\n",
    "\n",
    "    mask = (cu >= 0) & (cu < num_users_step)\n",
    "    return cu[mask]\n",
    "\n",
    "\n",
    "def compute_centers(coords: np.ndarray, labels: np.ndarray, K: int):\n",
    "    centers = {}\n",
    "    for k in range(K):\n",
    "        mk = (labels == k)\n",
    "        if np.any(mk):\n",
    "            centers[k] = coords[mk].mean(axis=0)\n",
    "    return centers\n",
    "\n",
    "\n",
    "def stepwise_trim_mask(coords2d: np.ndarray, top_pct: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    coords2d: (N,2)\n",
    "    top_pct: Remove top_pct distance from center in this step (plot-only)\n",
    "    return: keep mask (True=keep)\n",
    "    \"\"\"\n",
    "    n = 0 if coords2d is None else len(coords2d)\n",
    "    if n == 0:\n",
    "        return np.zeros((0,), dtype=bool)\n",
    "    if top_pct is None or top_pct <= 0:\n",
    "        return np.ones((n,), dtype=bool)\n",
    "    if top_pct >= 1.0:\n",
    "        return np.zeros((n,), dtype=bool)\n",
    "\n",
    "    ctr = np.median(coords2d, axis=0)\n",
    "    dist = np.linalg.norm(coords2d - ctr, axis=1)\n",
    "\n",
    "    q = 1.0 - top_pct\n",
    "    q = min(max(q, 0.0), 1.0)\n",
    "    thr = np.quantile(dist, q)\n",
    "    return dist <= thr\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Load each step embedding\n",
    "# =========================\n",
    "print(\"Loading augmentation embeddings per step...\")\n",
    "user_by_step = {}\n",
    "item_by_step = {}\n",
    "dims = set()\n",
    "\n",
    "for s in STEPS:\n",
    "    U, I = load_users_items_for_step(BASE_DIR, PARTS, s)\n",
    "    user_by_step[s] = U\n",
    "    item_by_step[s] = I\n",
    "    dims.add(U.shape[1])\n",
    "    if I is not None:\n",
    "        dims.add(I.shape[1])\n",
    "    print(f\"  - step{s}: users={U.shape}, items={None if I is None else I.shape}\")\n",
    "\n",
    "if len(dims) != 1:\n",
    "    raise RuntimeError(f\"ÏûÑÎ≤†Îî© Ï∞®ÏõêÏù¥ stepÎ≥ÑÎ°ú Îã§Î¶ÖÎãàÎã§: {dims}\")\n",
    "D = dims.pop()\n",
    "\n",
    "# =========================\n",
    "# 2) train‚à©label common user calculation + sorting by step\n",
    "# =========================\n",
    "print(f\"common users: {len(common_users)}\")\n",
    "aligned_users_by_step = {\n",
    "    s: align_users_for_step(common_users, user_by_step[s].shape[0])\n",
    "    for s in STEPS\n",
    "}\n",
    "print({s: len(aligned_users_by_step[s]) for s in STEPS})\n",
    "\n",
    "# =========================\n",
    "# 3) Sort common parts of items (minimum length)\n",
    "# =========================\n",
    "item_lengths = [item_by_step[s].shape[0] for s in STEPS if item_by_step[s] is not None]\n",
    "if len(item_lengths) == 0:\n",
    "    I_base = 0\n",
    "else:\n",
    "    I_base = min(item_lengths)\n",
    "    uniq = set(item_lengths)\n",
    "    if len(uniq) != 1:\n",
    "        print(f\"[warn] stepÎ≥Ñ ÏïÑÏù¥ÌÖú ÏàòÍ∞Ä Îã§Î¶ÖÎãàÎã§: {uniq} ‚Üí ÏïûÏóêÏÑú {I_base}Í∞úÎßå Í≥µÌÜµ ÏÇ¨Ïö©\")\n",
    "    for s in STEPS:\n",
    "        I = item_by_step[s]\n",
    "        item_by_step[s] = None if (I is None or I.shape[0] < I_base) else I[:I_base]\n",
    "\n",
    "rng = np.random.RandomState(0)\n",
    "if I_base > 0:\n",
    "    if SUBSAMPLE_ITEMS_FOR_TSNE is not None and SUBSAMPLE_ITEMS_FOR_TSNE < I_base:\n",
    "        keep_items = np.sort(rng.choice(I_base, size=SUBSAMPLE_ITEMS_FOR_TSNE, replace=False))\n",
    "    else:\n",
    "        keep_items = np.arange(I_base, dtype=int)\n",
    "else:\n",
    "    keep_items = np.array([], dtype=int)\n",
    "\n",
    "# =========================\n",
    "# 4) In the final step, KMeans ‚Üí label creation\n",
    "# =========================\n",
    "print(f\"KMeans on final step: step{CLUSTER_STEP}\")\n",
    "U_final = user_by_step[CLUSTER_STEP]\n",
    "ids_final = aligned_users_by_step[CLUSTER_STEP]\n",
    "if len(ids_final) < K_USERS:\n",
    "    raise RuntimeError(f\"ÏµúÏ¢Ö stepÏóêÏÑú ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ Í∞ÄÎä•Ìïú Í≥µÌÜµ Ïú†Ï†ÄÍ∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§: {len(ids_final)}\")\n",
    "\n",
    "emb_final_users = U_final[ids_final, :]  # (|ids_final|, D)\n",
    "user_kmeans = KMeans(n_clusters=K_USERS, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "user_labels_final_full = user_kmeans.fit_predict(emb_final_users)  # ids_final order basis\n",
    "\n",
    "# Item KMeans (final step common item)\n",
    "if item_by_step[CLUSTER_STEP] is not None and len(keep_items) > 0:\n",
    "    I_final = item_by_step[CLUSTER_STEP][keep_items, :]  # (|keep_items|, D)\n",
    "    item_kmeans = KMeans(n_clusters=K_ITEMS, random_state=KMEANS_RANDOM_STATE, n_init=10)\n",
    "    item_labels_final_full = item_kmeans.fit_predict(I_final)  # keep_items order basis\n",
    "else:\n",
    "    item_kmeans = None\n",
    "    item_labels_final_full = None\n",
    "    print(\"[warn] Omitted item clustering as there are no final step items\")\n",
    "\n",
    "# ids_final uid -> label mapping (based on final step)\n",
    "uid2label = {int(uid): int(lb) for uid, lb in zip(ids_final.tolist(), user_labels_final_full.tolist())}\n",
    "\n",
    "# =========================\n",
    "# 5) Configuring data blocks to be put into t-SNE (users + items across steps)\n",
    "# =========================\n",
    "print(\"Preparing t-SNE blocks...\")\n",
    "ids_for_tsne = {}      # step -> user_id array (reflects sampling)\n",
    "users_for_tsne = {}    # step -> user_emb array\n",
    "item_for_tsne  = {}    # step -> item_emb array (corresponds to common keep_items)\n",
    "\n",
    "# User sampling criteria: ids_final set of final step\n",
    "if SUBSAMPLE_USERS_FOR_TSNE is not None and SUBSAMPLE_USERS_FOR_TSNE < len(ids_final):\n",
    "    sample_users = np.sort(rng.choice(ids_final, size=SUBSAMPLE_USERS_FOR_TSNE, replace=False))\n",
    "else:\n",
    "    sample_users = ids_final\n",
    "\n",
    "for s in STEPS:\n",
    "    ids_s = aligned_users_by_step[s]\n",
    "    if len(ids_s) == 0:\n",
    "        ids_for_tsne[s] = np.array([], dtype=np.int64)\n",
    "        users_for_tsne[s] = np.empty((0, D), dtype=np.float32)\n",
    "    else:\n",
    "        mask = np.isin(ids_s, sample_users)\n",
    "        ids_for_tsne[s] = ids_s[mask]\n",
    "        users_for_tsne[s] = user_by_step[s][ids_for_tsne[s], :]\n",
    "\n",
    "    I = item_by_step[s]\n",
    "    item_for_tsne[s] = None if (I is None or len(keep_items) == 0) else I[keep_items, :]\n",
    "\n",
    "# =========================\n",
    "# 6) Perform t-SNE (summing users/items of all steps)\n",
    "# =========================\n",
    "print(\"Fitting joint t-SNE on users + items across steps\")\n",
    "blocks = []\n",
    "slices = {}  # step -> {\"user\": (st,en), \"item\": (st,en) or None}\n",
    "cursor = 0\n",
    "\n",
    "for s in STEPS:\n",
    "    Ublk = users_for_tsne[s]\n",
    "    blocks.append(Ublk)\n",
    "    u_st, u_en = cursor, cursor + len(Ublk)\n",
    "    cursor = u_en\n",
    "\n",
    "    Iblk = item_for_tsne[s]\n",
    "    if Iblk is not None and len(Iblk) > 0:\n",
    "        blocks.append(Iblk)\n",
    "        i_st, i_en = cursor, cursor + len(Iblk)\n",
    "        cursor = i_en\n",
    "        slices[s] = {\"user\": (u_st, u_en), \"item\": (i_st, i_en)}\n",
    "    else:\n",
    "        slices[s] = {\"user\": (u_st, u_en), \"item\": None}\n",
    "\n",
    "if len(blocks) == 0 or sum(len(b) for b in blocks) < 3:\n",
    "    raise RuntimeError(\"There are too few samples for t-SNE.\")\n",
    "\n",
    "X_all = np.vstack(blocks)\n",
    "perp = min(TSNE_PERPLEXITY, max(5, (len(X_all) - 1)//3))\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=perp,\n",
    "    init=\"pca\",\n",
    "    learning_rate=\"auto\",\n",
    "    random_state=TSNE_RANDOM_STATE,\n",
    "    max_iter=1000,\n",
    "    verbose=1\n",
    ")\n",
    "X_tsne = tsne.fit_transform(X_all)\n",
    "\n",
    "# =========================\n",
    "# (Optional) Global trimming (plot-only)\n",
    "# =========================\n",
    "if GLOBAL_TRIM_TOP_PCT is not None and GLOBAL_TRIM_TOP_PCT > 0:\n",
    "    ctr2d = np.median(X_tsne, axis=0)\n",
    "    dist2d = np.linalg.norm(X_tsne - ctr2d, axis=1)\n",
    "    thr = np.quantile(dist2d, 1.0 - GLOBAL_TRIM_TOP_PCT)\n",
    "    global_keep_mask = dist2d <= thr\n",
    "    print(f\"[global-trim] keep {global_keep_mask.sum()}/{len(global_keep_mask)} \"\n",
    "          f\"({100.0 * global_keep_mask.mean():.2f}%) points\")\n",
    "else:\n",
    "    global_keep_mask = np.ones((X_tsne.shape[0],), dtype=bool)\n",
    "\n",
    "# Axis range: Based on global keep (if none, all)\n",
    "X_plot = X_tsne[global_keep_mask]\n",
    "xmin, ymin = X_plot.min(axis=0)\n",
    "xmax, ymax = X_plot.max(axis=0)\n",
    "xpad = 0.05 * (xmax - xmin) if xmax > xmin else 0.5\n",
    "ypad = 0.05 * (ymax - ymin) if ymax > ymin else 0.5\n",
    "\n",
    "palette = [\"tab:blue\",\"tab:orange\",\"tab:green\",\"tab:red\",\"tab:purple\",\n",
    "           \"tab:brown\",\"tab:pink\",\"tab:gray\",\"tab:olive\",\"tab:cyan\"]\n",
    "\n",
    "# =========================\n",
    "# 6.5) Final step: Preparation of labels/centers (for plot) of users/items\n",
    "#     - Center calculations are also consistently applied by trimming (global/local)\n",
    "# =========================\n",
    "# final users slice\n",
    "u_st_f, u_en_f = slices[CLUSTER_STEP][\"user\"]\n",
    "coords_u_final_all = X_tsne[u_st_f:u_en_f]\n",
    "ids_tsne_final_all = ids_for_tsne[CLUSTER_STEP]\n",
    "labels_u_final_all = np.array([uid2label[int(uid)] for uid in ids_tsne_final_all], dtype=int)\n",
    "\n",
    "# Apply global trim\n",
    "g_keep_u_f = global_keep_mask[u_st_f:u_en_f]\n",
    "coords_u_final_g = coords_u_final_all[g_keep_u_f]\n",
    "ids_tsne_final_g = ids_tsne_final_all[g_keep_u_f]\n",
    "labels_u_final_g = labels_u_final_all[g_keep_u_f]\n",
    "\n",
    "# Local trim is also applied to the final step (default step rule)\n",
    "local_pct_f = STEP_TRIM_TOP_PCT_STEP1 if CLUSTER_STEP == 1 else STEP_TRIM_TOP_PCT\n",
    "l_keep_u_f = stepwise_trim_mask(coords_u_final_g, top_pct=local_pct_f)\n",
    "coords_u_final = coords_u_final_g[l_keep_u_f]\n",
    "ids_tsne_final = ids_tsne_final_g[l_keep_u_f]\n",
    "labels_u_for_plot = labels_u_final_g[l_keep_u_f]\n",
    "\n",
    "user_centers = compute_centers(coords_u_final, labels_u_for_plot, K_USERS)\n",
    "\n",
    "# final items slice\n",
    "if slices[CLUSTER_STEP][\"item\"] is not None and item_labels_final_full is not None:\n",
    "    i_st_f, i_en_f = slices[CLUSTER_STEP][\"item\"]\n",
    "    coords_i_final_all = X_tsne[i_st_f:i_en_f]\n",
    "    labels_i_final_all = item_labels_final_full\n",
    "\n",
    "    # global trim\n",
    "    g_keep_i_f = global_keep_mask[i_st_f:i_en_f]\n",
    "    coords_i_final_g = coords_i_final_all[g_keep_i_f]\n",
    "    labels_i_final_g = labels_i_final_all[g_keep_i_f]\n",
    "\n",
    "    # Local trim (final step)\n",
    "    local_pct_i_f = STEP_TRIM_TOP_PCT_STEP1 if CLUSTER_STEP == 1 else STEP_TRIM_TOP_PCT\n",
    "    l_keep_i_f = stepwise_trim_mask(coords_i_final_g, top_pct=local_pct_i_f)\n",
    "    coords_i_final = coords_i_final_g[l_keep_i_f]\n",
    "    labels_i_final = labels_i_final_g[l_keep_i_f]\n",
    "\n",
    "    item_centers = compute_centers(coords_i_final, labels_i_final, K_ITEMS)\n",
    "else:\n",
    "    item_centers = {}\n",
    "\n",
    "# final uid -> position (based on trimmed final)\n",
    "pos_map_final = {int(uid): i for i, uid in enumerate(ids_tsne_final.tolist())}\n",
    "\n",
    "# =========================\n",
    "# 7) Save picture ‚Äî Users only / Items only / Users+Items\n",
    "# =========================\n",
    "def plot_users_only():\n",
    "    step_dir = os.path.join(OUT_DIR, \"step_by_step\")\n",
    "    os.makedirs(step_dir, exist_ok=True)\n",
    "\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        fig_s, ax_s = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "        u_st, u_en = slices[s][\"user\"]\n",
    "        coords_u_all = X_tsne[u_st:u_en]\n",
    "        ids_tsne_s_all = ids_for_tsne[s]\n",
    "\n",
    "        if len(ids_tsne_s_all) == 0:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            plt.close(fig_s)\n",
    "            continue\n",
    "\n",
    "        # (1) Global trim\n",
    "        g_keep = global_keep_mask[u_st:u_en]\n",
    "        coords_u_g = coords_u_all[g_keep]\n",
    "        ids_u_g = ids_tsne_s_all[g_keep]\n",
    "\n",
    "        if len(ids_u_g) == 0:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            plt.close(fig_s)\n",
    "            continue\n",
    "\n",
    "        # (2) Local trim by step\n",
    "        local_pct = STEP_TRIM_TOP_PCT_STEP1 if s == 1 else STEP_TRIM_TOP_PCT\n",
    "        l_keep = stepwise_trim_mask(coords_u_g, top_pct=local_pct)\n",
    "        coords_u = coords_u_g[l_keep]\n",
    "        ids_tsne_s = ids_u_g[l_keep]\n",
    "\n",
    "        if len(ids_tsne_s) == 0:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            plt.close(fig_s)\n",
    "            continue\n",
    "\n",
    "        # Color by label based on final: uid not in final is discarded\n",
    "        valid_mask = np.array([int(uid) in pos_map_final for uid in ids_tsne_s], dtype=bool)\n",
    "        coords_valid = coords_u[valid_mask]\n",
    "        ids_valid = ids_tsne_s[valid_mask]\n",
    "\n",
    "        if len(ids_valid) == 0:\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            plt.close(fig_s)\n",
    "            continue\n",
    "\n",
    "        labels_plot = np.array([labels_u_for_plot[pos_map_final[int(uid)]] for uid in ids_valid], dtype=int)\n",
    "\n",
    "        for k in range(K_USERS):\n",
    "            mk = (labels_plot == k)\n",
    "            if np.any(mk):\n",
    "                ax.scatter(coords_valid[mk, 0], coords_valid[mk, 1],\n",
    "                           s=8, alpha=0.7, c=palette[k % len(palette)],\n",
    "                           label=f\"Users: C{k}\")\n",
    "                ax_s.scatter(coords_valid[mk, 0], coords_valid[mk, 1],\n",
    "                             s=10, alpha=0.7, c=palette[k % len(palette)],\n",
    "                             label=f\"Users: C{k}\")\n",
    "\n",
    "        if s == CLUSTER_STEP:\n",
    "            for k, ctr in user_centers.items():\n",
    "                ax.scatter(ctr[0], ctr[1], s=120, marker=\"X\",\n",
    "                           c=palette[k % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        for a in (ax, ax_s):\n",
    "            #a.set_xlim(xmin - xpad, xmax + xpad)\n",
    "            #a.set_ylim(ymin - ypad, ymax + ypad)\n",
    "            a.set_xticks([]); a.set_yticks([])\n",
    "\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "        ax_s.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "        fig_s.tight_layout()\n",
    "        out_s = os.path.join(step_dir, f\"step{s}_tsne2d_USERS_ONLY.png\")\n",
    "        fig_s.savefig(out_s, dpi=220)\n",
    "        plt.close(fig_s)\n",
    "        print(\"  - saved:\", out_s)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_USERS_ONLY.png\")\n",
    "    fig.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "\n",
    "def plot_items_only():\n",
    "    has_any = any(item_for_tsne[s] is not None for s in STEPS) and (item_labels_final_full is not None)\n",
    "    if not has_any:\n",
    "        print(\"  - Item only plot omitted (no item embedding/label)\")\n",
    "        return\n",
    "\n",
    "    step_dir = os.path.join(OUT_DIR, \"step_by_step\")\n",
    "    os.makedirs(step_dir, exist_ok=True)\n",
    "\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "        fig_s, ax_s = plt.subplots(figsize=(5, 4))\n",
    "\n",
    "        if slices[s][\"item\"] is not None:\n",
    "            i_st, i_en = slices[s][\"item\"]\n",
    "            coords_i_all = X_tsne[i_st:i_en]\n",
    "            labels_i_all = item_labels_final_full\n",
    "\n",
    "            # (1) Global trim\n",
    "            g_keep = global_keep_mask[i_st:i_en]\n",
    "            coords_i_g = coords_i_all[g_keep]\n",
    "            labels_i_g = labels_i_all[g_keep]\n",
    "\n",
    "            # (2) Local trim by step\n",
    "            local_pct = STEP_TRIM_TOP_PCT_STEP1 if s == 1 else STEP_TRIM_TOP_PCT\n",
    "            l_keep = stepwise_trim_mask(coords_i_g, top_pct=local_pct)\n",
    "            coords_i = coords_i_g[l_keep]\n",
    "            labels_i = labels_i_g[l_keep]\n",
    "\n",
    "            if len(coords_i) > 0:\n",
    "                for k in range(K_ITEMS):\n",
    "                    mk = (labels_i == k)\n",
    "                    if np.any(mk):\n",
    "                        ax.scatter(coords_i[mk, 0], coords_i[mk, 1],\n",
    "                                   s=5, alpha=0.35, c=palette[(k+2) % len(palette)],\n",
    "                                   label=f\"Items: C{k}\")\n",
    "                        ax_s.scatter(coords_i[mk, 0], coords_i[mk, 1],\n",
    "                                     s=5, alpha=0.35, c=palette[(k+2) % len(palette)],\n",
    "                                     label=f\"Items: C{k}\")\n",
    "\n",
    "                if s == CLUSTER_STEP and len(item_centers) > 0:\n",
    "                    for k, ctr in item_centers.items():\n",
    "                        ax.scatter(ctr[0], ctr[1], s=110, marker=\"D\",\n",
    "                                   c=palette[(k+2) % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        for a in (ax, ax_s):\n",
    "            #a.set_xlim(xmin - xpad, xmax + xpad)\n",
    "            #a.set_ylim(ymin - ypad, ymax + ypad)\n",
    "            a.set_xticks([]); a.set_yticks([])\n",
    "\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "        ax_s.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "        fig_s.tight_layout()\n",
    "        out_s = os.path.join(step_dir, f\"step{s}_tsne2d_ITEMS_ONLY.png\")\n",
    "        fig_s.savefig(out_s, dpi=220)\n",
    "        plt.close(fig_s)\n",
    "        print(\"  - saved:\", out_s)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_ITEMS_ONLY.png\")\n",
    "    fig.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "\n",
    "def plot_users_items():\n",
    "    step_dir = os.path.join(OUT_DIR, \"step_by_step\")\n",
    "    os.makedirs(step_dir, exist_ok=True)\n",
    "\n",
    "    S = len(STEPS)\n",
    "    cols = min(3, S)\n",
    "    rows = math.ceil(S / cols)\n",
    "    fig = plt.figure(figsize=(5*cols, 4*rows))\n",
    "\n",
    "    for idx, s in enumerate(STEPS, start=1):\n",
    "        ax = fig.add_subplot(rows, cols, idx)\n",
    "\n",
    "        # item\n",
    "        if slices[s][\"item\"] is not None and item_labels_final_full is not None:\n",
    "            i_st, i_en = slices[s][\"item\"]\n",
    "            coords_i_all = X_tsne[i_st:i_en]\n",
    "            labels_i_all = item_labels_final_full\n",
    "\n",
    "            # (1) Global trim\n",
    "            g_keep = global_keep_mask[i_st:i_en]\n",
    "            coords_i_g = coords_i_all[g_keep]\n",
    "            labels_i_g = labels_i_all[g_keep]\n",
    "\n",
    "            # (2) Local trim by step\n",
    "            local_pct = STEP_TRIM_TOP_PCT_STEP1 if s == 1 else STEP_TRIM_TOP_PCT\n",
    "            l_keep = stepwise_trim_mask(coords_i_g, top_pct=local_pct)\n",
    "            coords_i = coords_i_g[l_keep]\n",
    "            labels_i = labels_i_g[l_keep]\n",
    "\n",
    "            if len(coords_i) > 0:\n",
    "                for k in range(K_ITEMS):\n",
    "                    mk = (labels_i == k)\n",
    "                    if np.any(mk):\n",
    "                        ax.scatter(coords_i[mk, 0], coords_i[mk, 1],\n",
    "                                   s=5, alpha=0.35, c=palette[(k+2) % len(palette)],\n",
    "                                   label=f\"Items: C{k}\")\n",
    "                if s == CLUSTER_STEP and len(item_centers) > 0:\n",
    "                    for k, ctr in item_centers.items():\n",
    "                        ax.scatter(ctr[0], ctr[1], s=110, marker=\"D\",\n",
    "                                   c=palette[(k+2) % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        # posthumous work\n",
    "        u_st, u_en = slices[s][\"user\"]\n",
    "        coords_u_all = X_tsne[u_st:u_en]\n",
    "        ids_tsne_s_all = ids_for_tsne[s]\n",
    "\n",
    "        if len(ids_tsne_s_all) > 0:\n",
    "            # (1) Global trim\n",
    "            g_keep = global_keep_mask[u_st:u_en]\n",
    "            coords_u_g = coords_u_all[g_keep]\n",
    "            ids_u_g = ids_tsne_s_all[g_keep]\n",
    "\n",
    "            # (2) Local trim by step\n",
    "            local_pct = STEP_TRIM_TOP_PCT_STEP1 if s == 1 else STEP_TRIM_TOP_PCT\n",
    "            l_keep = stepwise_trim_mask(coords_u_g, top_pct=local_pct)\n",
    "            coords_u = coords_u_g[l_keep]\n",
    "            ids_tsne_s = ids_u_g[l_keep]\n",
    "\n",
    "            # Color with labels based on final (uids not in final are discarded)\n",
    "            valid_mask = np.array([int(uid) in pos_map_final for uid in ids_tsne_s], dtype=bool)\n",
    "            coords_valid = coords_u[valid_mask]\n",
    "            ids_valid = ids_tsne_s[valid_mask]\n",
    "\n",
    "            if len(ids_valid) > 0:\n",
    "                labels_plot = np.array(\n",
    "                    [labels_u_for_plot[pos_map_final[int(uid)]] for uid in ids_valid],\n",
    "                    dtype=int\n",
    "                )\n",
    "\n",
    "                for k in range(K_USERS):\n",
    "                    mk = (labels_plot == k)\n",
    "                    if np.any(mk):\n",
    "                        ax.scatter(coords_valid[mk, 0], coords_valid[mk, 1],\n",
    "                                   s=8, alpha=0.7, c=palette[k % len(palette)],\n",
    "                                   label=f\"Users: C{k}\")\n",
    "\n",
    "                if s == CLUSTER_STEP:\n",
    "                    for k, ctr in user_centers.items():\n",
    "                        ax.scatter(ctr[0], ctr[1], s=120, marker=\"X\",\n",
    "                                   c=palette[k % len(palette)], edgecolor=\"k\")\n",
    "\n",
    "        ax.set_title(f\"step{s} (Users & Items, colored by step{CLUSTER_STEP})\")\n",
    "        #ax.set_xlim(xmin - xpad, xmax + xpad)\n",
    "        #ax.set_ylim(ymin - ypad, ymax + ypad)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "        if idx == 1:\n",
    "            ax.legend(loc=\"best\", frameon=True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_tsne2d_USERS_ITEMS.png\")\n",
    "    fig.savefig(out, dpi=220)\n",
    "    plt.close(fig)\n",
    "    print(\"  - saved:\", out)\n",
    "\n",
    "\n",
    "print(\"Plotting...\")\n",
    "plot_users_only()\n",
    "plot_items_only()\n",
    "plot_users_items()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda2aecc",
   "metadata": {},
   "source": [
    "### Cluster Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3462e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from itertools import combinations\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_user_label_map(user_ids: np.ndarray, user_labels: np.ndarray) -> Dict[int, int]:\n",
    "    return {int(u): int(l) for u, l in zip(user_ids.tolist(), user_labels.tolist())}\n",
    "\n",
    "def get_labels_for_ids(target_ids: np.ndarray, label_map: Dict[int, int]) -> np.ndarray:\n",
    "    if len(target_ids) == 0:\n",
    "        return np.empty((0,), dtype=int)\n",
    "    return np.array([label_map.get(int(u), -1) for u in target_ids.tolist()], dtype=int)\n",
    "\n",
    "def calculate_centers(points: np.ndarray, labels: np.ndarray, k: int) -> Dict[int, Optional[np.ndarray]]:\n",
    "    centers = {}\n",
    "    for cluster_id in range(k):\n",
    "        mask = (labels == cluster_id)\n",
    "        centers[cluster_id] = points[mask].mean(axis=0) if np.any(mask) else None\n",
    "    return centers\n",
    "\n",
    "def calculate_distance(point_a: Optional[np.ndarray], point_b: Optional[np.ndarray]) -> float:\n",
    "    if point_a is None or point_b is None:\n",
    "        return np.nan\n",
    "    return float(np.linalg.norm(point_a - point_b))\n",
    "\n",
    "def get_pairwise_stats(centers: Dict[int, Optional[np.ndarray]]) -> Tuple[float, float, float]:\n",
    "    valid = [v for v in centers.values() if v is not None]\n",
    "    if len(valid) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dists = [np.linalg.norm(a - b) for a, b in combinations(valid, 2)]\n",
    "    return float(np.mean(dists)), float(np.min(dists)), float(np.max(dists))\n",
    "\n",
    "def get_user_item_stats(centers_u: Dict[int, Optional[np.ndarray]],\n",
    "                        centers_i: Dict[int, Optional[np.ndarray]]) -> Tuple[float, float, float]:\n",
    "    vec_u = [v for v in centers_u.values() if v is not None]\n",
    "    vec_i = [v for v in centers_i.values() if v is not None]\n",
    "    if not vec_u or not vec_i:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dists = [np.linalg.norm(u - i) for u in vec_u for i in vec_i]\n",
    "    return float(np.mean(dists)), float(np.min(dists)), float(np.max(dists))\n",
    "\n",
    "# =========================\n",
    "# Main (RAW ONLY)\n",
    "# =========================\n",
    "\n",
    "# 1) Prepare label map from final step clustering\n",
    "user_label_map_final = get_user_label_map(ids_final, user_labels_final)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for s in STEPS:\n",
    "    step_data = {\"step\": s}\n",
    "\n",
    "    # --- A) RAW: User centers ---\n",
    "    ids_u_raw = aligned_users_by_step[s]\n",
    "    emb_u_raw = user_by_step[s][ids_u_raw, :] if len(ids_u_raw) > 0 else np.empty((0, user_by_step[s].shape[1]))\n",
    "    labels_u_raw = get_labels_for_ids(ids_u_raw, user_label_map_final)\n",
    "    centers_u_raw = calculate_centers(emb_u_raw, labels_u_raw, K_USERS)\n",
    "\n",
    "    # --- B) RAW: Item centers ---\n",
    "    # Since item_for_tsne[s] was being used as raw item embedding, use it as is.\n",
    "    if item_for_tsne[s] is not None and item_labels_final is not None:\n",
    "        emb_i_raw = item_for_tsne[s]\n",
    "        centers_i_raw = calculate_centers(emb_i_raw, item_labels_final, K_ITEMS)\n",
    "    else:\n",
    "        centers_i_raw = {k: None for k in range(K_ITEMS)}\n",
    "\n",
    "    # --- C) Stats (RAW) ---\n",
    "    # User-User / Item-Item (raw)\n",
    "    step_data[\"user_dist_raw_mean\"], step_data[\"user_dist_raw_min\"], step_data[\"user_dist_raw_max\"] = get_pairwise_stats(centers_u_raw)\n",
    "    step_data[\"item_dist_raw_mean\"], step_data[\"item_dist_raw_min\"], step_data[\"item_dist_raw_max\"] = get_pairwise_stats(centers_i_raw)\n",
    "\n",
    "    # Detailed stats when K=2\n",
    "    if K_USERS == 2:\n",
    "        step_data[\"user_dist_raw_U0U1\"] = calculate_distance(centers_u_raw.get(0), centers_u_raw.get(1))\n",
    "    if K_ITEMS == 2:\n",
    "        step_data[\"item_dist_raw_I0I1\"] = calculate_distance(centers_i_raw.get(0), centers_i_raw.get(1))\n",
    "\n",
    "    # User-Item (raw)\n",
    "    if K_USERS == 2 and K_ITEMS == 2:\n",
    "        for u_idx in range(2):\n",
    "            for i_idx in range(2):\n",
    "                key_suffix = f\"U{u_idx}_I{i_idx}\"\n",
    "                step_data[f\"ui_raw_{key_suffix}\"] = calculate_distance(\n",
    "                    centers_u_raw.get(u_idx), centers_i_raw.get(i_idx)\n",
    "                )\n",
    "    else:\n",
    "        step_data[\"ui_raw_mean\"], step_data[\"ui_raw_min\"], step_data[\"ui_raw_max\"] = get_user_item_stats(centers_u_raw, centers_i_raw)\n",
    "\n",
    "    rows.append(step_data)\n",
    "\n",
    "# --- Save CSV ---\n",
    "csv_filename = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_RAW_center_distances_over_steps.csv\")\n",
    "if rows:\n",
    "    with open(csv_filename, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"  - saved: {csv_filename}\")\n",
    "\n",
    "# =========================\n",
    "# Visualization (RAW, combined)\n",
    "# =========================\n",
    "\n",
    "steps_ax = [r[\"step\"] for r in rows]\n",
    "\n",
    "fig, (ax_top, ax_bottom) = plt.subplots(\n",
    "    nrows=2, ncols=1, figsize=(8, 8), sharex=True\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# (Top) User-User & Item-Item (RAW)\n",
    "# -------------------------\n",
    "ax_top.plot(\n",
    "    steps_ax,\n",
    "    [r[\"user_dist_raw_mean\"] for r in rows],\n",
    "    marker=\"o\",\n",
    "    label=\"Users (raw, mean)\"\n",
    ")\n",
    "ax_top.plot(\n",
    "    steps_ax,\n",
    "    [r[\"item_dist_raw_mean\"] for r in rows],\n",
    "    marker=\"s\",\n",
    "    label=\"Items (raw, mean)\"\n",
    ")\n",
    "ax_top.set_ylabel(\"Center distance\")\n",
    "ax_top.set_title(\"User/User & Item/Item Center Distance (RAW)\")\n",
    "ax_top.legend()\n",
    "\n",
    "# -------------------------\n",
    "# (Bottom) User-Item (RAW)\n",
    "# -------------------------\n",
    "if K_USERS == 2 and K_ITEMS == 2:\n",
    "    keys = [\"U0_I0\", \"U0_I1\", \"U1_I0\", \"U1_I1\"]\n",
    "    for key_suffix in keys:\n",
    "        ax_bottom.plot(\n",
    "            steps_ax,\n",
    "            [r.get(f\"ui_raw_{key_suffix}\", np.nan) for r in rows],\n",
    "            marker=\"o\",\n",
    "            label=key_suffix.replace(\"_\", \"‚Äì\")\n",
    "        )\n",
    "    ax_bottom.legend(ncol=2)\n",
    "    title_suffix = \"(Detailed K=2)\"\n",
    "else:\n",
    "    for stat, mk in [(\"mean\", \"o\"), (\"min\", \"s\"), (\"max\", \"^\")]:\n",
    "        ax_bottom.plot(\n",
    "            steps_ax,\n",
    "            [r.get(f\"ui_raw_{stat}\", np.nan) for r in rows],\n",
    "            marker=mk,\n",
    "            label=stat\n",
    "        )\n",
    "    ax_bottom.legend()\n",
    "    title_suffix = \"(Summary)\"\n",
    "\n",
    "ax_bottom.set_xlabel(\"Step\")\n",
    "ax_bottom.set_ylabel(\"Center distance\")\n",
    "ax_bottom.set_title(f\"User‚ÄìItem Center Distances (RAW) {title_suffix}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "out = os.path.join(\n",
    "    OUT_DIR,\n",
    "    f\"step{CLUSTER_STEP}_RAW_center_distances_over_steps_combined.png\"\n",
    ")\n",
    "plt.savefig(out, dpi=220)\n",
    "plt.close()\n",
    "\n",
    "print(f\"  - saved: {out}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdabbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from itertools import combinations\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_user_label_map(user_ids: np.ndarray, user_labels: np.ndarray) -> Dict[int, int]:\n",
    "    return {int(u): int(l) for u, l in zip(user_ids.tolist(), user_labels.tolist())}\n",
    "\n",
    "def get_labels_for_ids(target_ids: np.ndarray, label_map: Dict[int, int]) -> np.ndarray:\n",
    "    if len(target_ids) == 0:\n",
    "        return np.empty((0,), dtype=int)\n",
    "    return np.array([label_map.get(int(u), -1) for u in target_ids.tolist()], dtype=int)\n",
    "\n",
    "def calculate_centers(points: np.ndarray, labels: np.ndarray, k: int) -> Dict[int, Optional[np.ndarray]]:\n",
    "    centers = {}\n",
    "    for cluster_id in range(k):\n",
    "        mask = (labels == cluster_id)\n",
    "        centers[cluster_id] = points[mask].mean(axis=0) if np.any(mask) else None\n",
    "    return centers\n",
    "\n",
    "def get_pairwise_stats(centers: Dict[int, Optional[np.ndarray]]) -> Tuple[float, float, float]:\n",
    "    valid = [v for v in centers.values() if v is not None]\n",
    "    if len(valid) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dists = [np.linalg.norm(a - b) for a, b in combinations(valid, 2)]\n",
    "    return float(np.mean(dists)), float(np.min(dists)), float(np.max(dists))\n",
    "\n",
    "def calculate_distance(point_a: Optional[np.ndarray], point_b: Optional[np.ndarray]) -> float:\n",
    "    if point_a is None or point_b is None:\n",
    "        return np.nan\n",
    "    return float(np.linalg.norm(point_a - point_b))\n",
    "\n",
    "# =========================\n",
    "# Main (RAW ONLY) - First & Last step only\n",
    "#   - Only User-User and Item-Item distances\n",
    "# =========================\n",
    "\n",
    "FIRST_STEP = STEPS[0]\n",
    "LAST_STEP = STEPS[-1]\n",
    "TARGET_STEPS = [FIRST_STEP, LAST_STEP]\n",
    "\n",
    "# final step user clustering label map\n",
    "user_label_map_final = get_user_label_map(ids_final, user_labels_final)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for s in TARGET_STEPS:\n",
    "    step_data = {\"step\": s}\n",
    "\n",
    "    # --- A) RAW: User centers (embedding space) ---\n",
    "    ids_u_raw = aligned_users_by_step[s]\n",
    "    emb_u_raw = user_by_step[s][ids_u_raw, :] if len(ids_u_raw) > 0 else np.empty((0, user_by_step[s].shape[1]))\n",
    "    labels_u_raw = get_labels_for_ids(ids_u_raw, user_label_map_final)\n",
    "    centers_u_raw = calculate_centers(emb_u_raw, labels_u_raw, K_USERS)\n",
    "\n",
    "    # --- B) RAW: Item centers (embedding space) ---\n",
    "    if item_for_tsne[s] is not None and item_labels_final is not None:\n",
    "        emb_i_raw = item_for_tsne[s]\n",
    "        centers_i_raw = calculate_centers(emb_i_raw, item_labels_final, K_ITEMS)\n",
    "    else:\n",
    "        centers_i_raw = {k: None for k in range(K_ITEMS)}\n",
    "\n",
    "    # --- C) Stats (RAW) ---\n",
    "    # User-User\n",
    "    step_data[\"user_dist_raw_mean\"], step_data[\"user_dist_raw_min\"], step_data[\"user_dist_raw_max\"] = get_pairwise_stats(centers_u_raw)\n",
    "    if K_USERS == 2:\n",
    "        step_data[\"user_dist_raw_U0U1\"] = calculate_distance(centers_u_raw.get(0), centers_u_raw.get(1))\n",
    "\n",
    "    # Item-Item\n",
    "    step_data[\"item_dist_raw_mean\"], step_data[\"item_dist_raw_min\"], step_data[\"item_dist_raw_max\"] = get_pairwise_stats(centers_i_raw)\n",
    "    if K_ITEMS == 2:\n",
    "        step_data[\"item_dist_raw_I0I1\"] = calculate_distance(centers_i_raw.get(0), centers_i_raw.get(1))\n",
    "\n",
    "    rows.append(step_data)\n",
    "\n",
    "# --- Save CSV (only 2 rows) ---\n",
    "csv_filename = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_RAW_center_distances_user_item_first_last.csv\")\n",
    "if rows:\n",
    "    with open(csv_filename, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"  - saved: {csv_filename}\")\n",
    "\n",
    "# =========================\n",
    "# Visualization (First vs Last) - Separate plots\n",
    "# =========================\n",
    "\n",
    "steps_ax = [r[\"step\"] for r in rows]\n",
    "\n",
    "# 1) User-User only\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(\n",
    "    steps_ax,\n",
    "    [r[\"user_dist_raw_mean\"] for r in rows],\n",
    "    marker=\"o\",\n",
    "    label=\"Users (raw, mean)\"\n",
    ")\n",
    "if K_USERS == 2 and \"user_dist_raw_U0U1\" in rows[0]:\n",
    "    plt.plot(\n",
    "        steps_ax,\n",
    "        [r.get(\"user_dist_raw_U0U1\", np.nan) for r in rows],\n",
    "        marker=\"s\",\n",
    "        label=\"Users (raw, U0‚ÄìU1)\"\n",
    "    )\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Center distance\")\n",
    "plt.title(\"User‚ÄìUser Center Distance (RAW) - First vs Last\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "out_users = os.path.join(OUT_DIR, f\"step_by_step/step{CLUSTER_STEP}_RAW_user_user_first_last.png\")\n",
    "plt.savefig(out_users, dpi=220)\n",
    "plt.close()\n",
    "print(f\"  - saved: {out_users}\")\n",
    "\n",
    "# 2) Item-Item only\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(\n",
    "    steps_ax,\n",
    "    [r[\"item_dist_raw_mean\"] for r in rows],\n",
    "    marker=\"o\",\n",
    "    label=\"Items (raw, mean)\"\n",
    ")\n",
    "if K_ITEMS == 2 and \"item_dist_raw_I0I1\" in rows[0]:\n",
    "    plt.plot(\n",
    "        steps_ax,\n",
    "        [r.get(\"item_dist_raw_I0I1\", np.nan) for r in rows],\n",
    "        marker=\"s\",\n",
    "        label=\"Items (raw, I0‚ÄìI1)\"\n",
    "    )\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Center distance\")\n",
    "plt.title(\"Item‚ÄìItem Center Distance (RAW) - First vs Last\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "out_items = os.path.join(OUT_DIR, f\"step_by_step/step{CLUSTER_STEP}_RAW_item_item_first_last.png\")\n",
    "plt.savefig(out_items, dpi=220)\n",
    "plt.close()\n",
    "print(f\"  - saved: {out_items}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f44b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from itertools import combinations\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def get_user_label_map(user_ids: np.ndarray, user_labels: np.ndarray) -> Dict[int, int]:\n",
    "    return {int(u): int(l) for u, l in zip(user_ids.tolist(), user_labels.tolist())}\n",
    "\n",
    "def get_labels_for_ids(target_ids: np.ndarray, label_map: Dict[int, int]) -> np.ndarray:\n",
    "    if len(target_ids) == 0:\n",
    "        return np.empty((0,), dtype=int)\n",
    "    return np.array([label_map.get(int(u), -1) for u in target_ids.tolist()], dtype=int)\n",
    "\n",
    "def calculate_centers(points: np.ndarray, labels: np.ndarray, k: int) -> Dict[int, Optional[np.ndarray]]:\n",
    "    centers = {}\n",
    "    for cluster_id in range(k):\n",
    "        mask = (labels == cluster_id)\n",
    "        centers[cluster_id] = points[mask].mean(axis=0) if np.any(mask) else None\n",
    "    return centers\n",
    "\n",
    "def get_pairwise_stats(centers: Dict[int, Optional[np.ndarray]]) -> Tuple[float, float, float]:\n",
    "    valid = [v for v in centers.values() if v is not None]\n",
    "    if len(valid) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    dists = [np.linalg.norm(a - b) for a, b in combinations(valid, 2)]\n",
    "    return float(np.mean(dists)), float(np.min(dists)), float(np.max(dists))\n",
    "\n",
    "def calculate_distance(point_a: Optional[np.ndarray], point_b: Optional[np.ndarray]) -> float:\n",
    "    if point_a is None or point_b is None:\n",
    "        return np.nan\n",
    "    return float(np.linalg.norm(point_a - point_b))\n",
    "\n",
    "# =========================\n",
    "# Main (RAW ONLY) - All steps\n",
    "#   - Only User-User and Item-Item distances\n",
    "#   - CSV step: 0..4 (same as original file format)\n",
    "#   - Plot x-axis: 1..5 (mark only +1, remove 0.5 tick)\n",
    "# =========================\n",
    "\n",
    "# final step user clustering label map\n",
    "user_label_map_final = get_user_label_map(ids_final, user_labels_final)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for s in STEPS:\n",
    "    step_data = {\"step\": s}   # ‚úÖ Customize CSV format (0~4)\n",
    "\n",
    "    # --- A) RAW: User centers (embedding space) ---\n",
    "    ids_u_raw = aligned_users_by_step[s]\n",
    "    emb_u_raw = (\n",
    "        user_by_step[s][ids_u_raw, :]\n",
    "        if len(ids_u_raw) > 0\n",
    "        else np.empty((0, user_by_step[s].shape[1]))\n",
    "    )\n",
    "    labels_u_raw = get_labels_for_ids(ids_u_raw, user_label_map_final)\n",
    "    centers_u_raw = calculate_centers(emb_u_raw, labels_u_raw, K_USERS)\n",
    "\n",
    "    # --- B) RAW: Item centers (embedding space) ---\n",
    "    if item_for_tsne[s] is not None and item_labels_final is not None:\n",
    "        emb_i_raw = item_for_tsne[s]\n",
    "        centers_i_raw = calculate_centers(emb_i_raw, item_labels_final, K_ITEMS)\n",
    "    else:\n",
    "        centers_i_raw = {k: None for k in range(K_ITEMS)}\n",
    "\n",
    "    # --- C) Stats (RAW) ---\n",
    "    # User-User\n",
    "    step_data[\"user_dist_raw_mean\"], step_data[\"user_dist_raw_min\"], step_data[\"user_dist_raw_max\"] = get_pairwise_stats(centers_u_raw)\n",
    "    if K_USERS == 2:\n",
    "        step_data[\"user_dist_raw_U0U1\"] = calculate_distance(centers_u_raw.get(0), centers_u_raw.get(1))\n",
    "\n",
    "    # Item-Item\n",
    "    step_data[\"item_dist_raw_mean\"], step_data[\"item_dist_raw_min\"], step_data[\"item_dist_raw_max\"] = get_pairwise_stats(centers_i_raw)\n",
    "    if K_ITEMS == 2:\n",
    "        step_data[\"item_dist_raw_I0I1\"] = calculate_distance(centers_i_raw.get(0), centers_i_raw.get(1))\n",
    "\n",
    "    rows.append(step_data)\n",
    "\n",
    "# --- Save CSV (all steps) ---\n",
    "# ‚úÖ If you want to follow the file name format you showed, save it as below.\n",
    "csv_filename = os.path.join(OUT_DIR, f\"step{CLUSTER_STEP}_center_distances_over_steps_RAW_ONLY.csv\")\n",
    "if rows:\n",
    "    with open(csv_filename, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "    print(f\"  - saved: {csv_filename}\")\n",
    "\n",
    "# =========================\n",
    "# Visualization (All steps) - Separate plots\n",
    "#   - Display x-axis: step+1 (1..5)\n",
    "#   - Remove 0.5 ticks: plt.xticks (integer ticks)\n",
    "# =========================\n",
    "\n",
    "steps_raw = [r[\"step\"] for r in rows]           # 0..4 (same as CSV)\n",
    "steps_ax  = [s + 1 for s in steps_raw]          # 1..5 (for plot display)\n",
    "\n",
    "# 1) User-User only\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.plot(\n",
    "    steps_ax,\n",
    "    [r[\"user_dist_raw_mean\"] for r in rows],\n",
    "    marker=\"o\",\n",
    "    label=\"Users (raw, mean)\"\n",
    ")\n",
    "if K_USERS == 2 and \"user_dist_raw_U0U1\" in rows[0]:\n",
    "    plt.plot(\n",
    "        steps_ax,\n",
    "        [r.get(\"user_dist_raw_U0U1\", np.nan) for r in rows],\n",
    "        marker=\"s\",\n",
    "        label=\"Users (raw, U0‚ÄìU1)\"\n",
    "    )\n",
    "\n",
    "plt.xticks(steps_ax)     # ‚úÖ Remove 0.5 unit ticks\n",
    "plt.xlabel(\"Period\")\n",
    "plt.ylabel(\"Center distance\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_users = os.path.join(OUT_DIR, f\"step_by_step/step{CLUSTER_STEP}_RAW_user_user_all_steps.png\")\n",
    "os.makedirs(os.path.dirname(out_users), exist_ok=True)\n",
    "plt.savefig(out_users, dpi=220)\n",
    "plt.close()\n",
    "print(f\"  - saved: {out_users}\")\n",
    "\n",
    "# 2) Item-Item only\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "plt.plot(\n",
    "    steps_ax,\n",
    "    [r[\"item_dist_raw_mean\"] for r in rows],\n",
    "    marker=\"o\",\n",
    "    label=\"Items (raw, mean)\"\n",
    ")\n",
    "if K_ITEMS == 2 and \"item_dist_raw_I0I1\" in rows[0]:\n",
    "    plt.plot(\n",
    "        steps_ax,\n",
    "        [r.get(\"item_dist_raw_I0I1\", np.nan) for r in rows],\n",
    "        marker=\"s\",\n",
    "        label=\"Items (raw, I0‚ÄìI1)\"\n",
    "    )\n",
    "\n",
    "plt.xticks(steps_ax)     # ‚úÖ Remove 0.5 unit ticks\n",
    "plt.xlabel(\"Period\")\n",
    "plt.ylabel(\"Center distance\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_items = os.path.join(OUT_DIR, f\"step_by_step/step{CLUSTER_STEP}_RAW_item_item_all_steps.png\")\n",
    "os.makedirs(os.path.dirname(out_items), exist_ok=True)\n",
    "plt.savefig(out_items, dpi=220)\n",
    "plt.close()\n",
    "print(f\"  - saved: {out_items}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec431f3",
   "metadata": {},
   "source": [
    "## FilterBubble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = f\"{BASE_DIR}/figs_tsne_cluster_progress/step4_RAW_center_distances_over_steps.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Check whether step0 and step4 are exactly present (if not, use the smallest/largest step)\n",
    "step_min = int(df[\"step\"].min())\n",
    "step_max = int(df[\"step\"].max())\n",
    "\n",
    "# If what you want is fixing step0/step4, write the two lines below,\n",
    "# If it is not in CSV, a KeyError/empty result may occur.\n",
    "s0 = 0\n",
    "s4 = 4\n",
    "if not ((df[\"step\"] == s0).any() and (df[\"step\"] == s4).any()):\n",
    "    print(f\"[warn] step {s0}/{s4}Í∞Ä CSVÏóê ÏóÜÏñ¥ÏÑú {step_min}/{step_max}Î°ú ÎåÄÏ≤¥Ìï©ÎãàÎã§.\")\n",
    "    s0, s4 = step_min, step_max\n",
    "\n",
    "row0 = df.loc[df[\"step\"] == s0].iloc[0]\n",
    "row4 = df.loc[df[\"step\"] == s4].iloc[0]\n",
    "\n",
    "pairs = [\n",
    "    (\"U0\", \"I0\", \"ui_raw_U0_I0\"),\n",
    "    (\"U0\", \"I1\", \"ui_raw_U0_I1\"),\n",
    "    (\"U1\", \"I0\", \"ui_raw_U1_I0\"),\n",
    "    (\"U1\", \"I1\", \"ui_raw_U1_I1\"),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for u, i, col in pairs:\n",
    "    d0 = float(row0[col])\n",
    "    d4 = float(row4[col])\n",
    "    dec = d0 - d4   # + side gets closer, - side moves away\n",
    "    results.append({\"U\": u, \"I\": i, \"d_step0\": d0, \"d_step4\": d4, \"decrease\": dec})\n",
    "\n",
    "res = pd.DataFrame(results)\n",
    "\n",
    "# Summary by U: Compare I0/I1 reduction and determine if you are ‚Äúcloser to I0‚Äù\n",
    "summary_rows = []\n",
    "for u in [\"U0\", \"U1\"]:\n",
    "    dec_i0 = float(res[(res[\"U\"] == u) & (res[\"I\"] == \"I0\")][\"decrease\"].iloc[0])\n",
    "    dec_i1 = float(res[(res[\"U\"] == u) & (res[\"I\"] == \"I1\")][\"decrease\"].iloc[0])\n",
    "    # Difference in reduction (if positive, closer to I0)\n",
    "    diff = dec_i0 - dec_i1\n",
    "    summary_rows.append({\n",
    "        \"U\": u,\n",
    "        \"step_start\": s0,\n",
    "        \"step_end\": s4,\n",
    "        \"decrease_to_I0\": dec_i0,\n",
    "        \"decrease_to_I1\": dec_i1,\n",
    "        \"diff(I0_minus_I1)\": diff,\n",
    "        \"moved_more_toward\": \"I0\" if diff > 0 else (\"I1\" if diff < 0 else \"Same\")\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(summary_rows)\n",
    "\n",
    "print(\"\\n[raw distances + decreases]\")\n",
    "print(res.to_string(index=False))\n",
    "\n",
    "print(\"\\n[summary: compare decreases]\")\n",
    "print(summary.to_string(index=False))\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization\n",
    "# - Comparison of bars (I0 reduction amount, I1 reduction amount) in U0 and U1 respectively\n",
    "# ---------------------------\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "x = np.arange(2)  # U0, U1\n",
    "width = 0.35\n",
    "\n",
    "dec_i0s = summary[\"decrease_to_I0\"].values\n",
    "dec_i1s = summary[\"decrease_to_I1\"].values\n",
    "\n",
    "ax.bar(x - width/2, dec_i0s, width, label=\"Decrease to I0 (d_step0 - d_step4)\")\n",
    "ax.bar(x + width/2, dec_i1s, width, label=\"Decrease to I1 (d_step0 - d_step4)\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary[\"U\"].values)\n",
    "ax.set_ylabel(\"Distance decrease (+ means closer)\")\n",
    "ax.set_title(f\"U‚ÄìI distance decrease from step{s0} to step{s4}\")\n",
    "ax.axhline(0, linestyle=\"--\", linewidth=1, alpha=0.6)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "out_png = os.path.join(os.path.dirname(csv_path), f\"p{s0}_to_p{s4}_U_to_I_distance_decrease.png\")\n",
    "plt.savefig(out_png, dpi=220)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSaved plot ->\", out_png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmrec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
